#!/usr/bin/env ccp4-python

import os, sys, copy, glob

import numpy, pandas, json

import libtbx.phil, libtbx.easy_mp
import iotbx.pdb

from libtbx.utils import Sorry, Failure

from bamboo.common.logs import Log
from bamboo.common.path import easy_directory, rel_symlink
from bamboo.common.command import CommandManager

from giant.dataset import ModelAndData

from giant.structure.select import protein, backbone, sidechains
from giant.structure.formatting import PhenixSelection

import matplotlib
matplotlib.interactive(False)
from matplotlib import pyplot
pyplot.switch_backend('agg')
pyplot.interactive(0)

numpy.set_printoptions(threshold=numpy.nan)

from IPython import embed

############################################################################

PROGRAM = 'pandemic.adp_sweep'

DESCRIPTION = """
    Fit a series of B-factor models to a series of B-factor refinements
"""

############################################################################

blank_arg_prepend = {None:'refinement_dir=', '.pdb':'pdb='}

master_phil = libtbx.phil.parse("""
input {
    pdb = None
        .help = "input pdb files for refinement"
        .multiple = True
        .type = str
    cif = None
        .type = str
    refinement_dir = None
        .help = "input directory, containing different B-factor refinements"
        .optional = False
        .type = str
    analysis_dir = md_adp_analysis
        .help = "directory to dunp the analysis into"
        .type = str
    labelling = filename *foldername
        .type = choice
}
output {
    out_dir = adp-characterisation-sweep
        .help = "output directory"
        .type = str
    out_script = run_adp_sweeps.sh
        .help = "output script to run jobs"
        .type = str
}
options {
    refinements = isotropic tls *anisotropic
        .type = choice(multi=True)
    fitting_groups = *chain *auto_group *secondary_structure *residue *backbone *sidechain atom
        .type = choice(multi=True)
}
sweep_parameters {
    resolution_partitioning = *0.1 *0.5 *1.0 *10.0
        .type = choice(multi=True)
        .help = "Split datasets into groups for characterisation by resolution."
    datasets_per_characterisation = *2 *3 *4 *5 *6 *7 *8 *9 *10 *15 *20 *30 *40 *50 *100 *150 *200
        .type = choice(multi=True)
        .help = "Once datasets are partitioned, select random samples of datasets."
    optimisation_cutoffs = *1.5 *1.6 *1.7 *1.8 *1.9 *2.0 *2.5 *3.0 *3.5
        .type = choice(multi=True)
        .help = "Try different optimisation cutoffs for datasets in a partitioned group - can high resolution datasets parameterise low-resolution datasets?"
}
parameterisation {
    tbd = True
}
settings {
    max_cpus_per_job = 1
        .type = int
        .multiple = False
    concurrent_jobs = 1
        .type = int
        .multiple = False
}
""", process_includes=True)

############################################################################

def wrapper_run(arg):
    return arg.run()

def validate_parameters(params):
    assert os.path.exists(params.input.refinement_dir)

    params.sweep_parameters.resolution_partitioning         = map(float, params.sweep_parameters.resolution_partitioning        )
    print 'Resolution partitions: {}'.format(params.sweep_parameters.resolution_partitioning)
    params.sweep_parameters.datasets_per_characterisation   = map(int,   params.sweep_parameters.datasets_per_characterisation  )
    print 'Datasets per characterisation: {}'.format(params.sweep_parameters.datasets_per_characterisation)
    params.sweep_parameters.optimisation_cutoffs            = map(float, params.sweep_parameters.optimisation_cutoffs           )
    print 'Optimisation cutoffs: {}'.format(params.sweep_parameters.optimisation_cutoffs)

    pass

def run_refinements(params, log=None):

    if log is None: log = Log(verbose=True)
    log.heading('Creating B-factor refinement job')

    cmd = CommandManager('pandemic.adp')
    cmd.add_command_line_arguments(params.input.pdb)
    cmd.add_command_line_arguments(params.input.cif)
    cmd.add_command_line_arguments(r'tls_selections={}'.format(params.options.tls_selections))
    cmd.add_command_line_arguments(r'labelling={}'.format(params.input.labelling))
    cmd.add_command_line_arguments(r'b_factor_models={}'.format('+'.join(params.options.refinements)))
    cmd.add_command_line_arguments(r'out_dir={}'.format(params.input.refinement_dir))
    cmd.add_command_line_arguments(r'log_file={}.log'.format(params.input.dir))
    cmd.add_command_line_arguments(r'cpus={}'.format(params.settings.max_cpus_per_job*params.settings.concurrent_jobs))
    cmd.print_settings()
    ret = cmd.run()
    if ret != 0:
        raise Exception('Failed in refinement')

def actually_generate_sweep(sweep_dir,
                            input_pdbs,
                            #opt_cutoff,
                            params):

    sweep_dir = easy_directory(sweep_dir)

    # Write all pdb files to an input file to pass to program
    pdb_eff = os.path.join(sweep_dir, 'input_pdbs.eff')
    with open(pdb_eff, 'w') as fh:
        fh.write('\n'.join([r'pdb="{}"'.format(p) for p in input_pdbs]))

    cmd = CommandManager("pandemic.adp")
    cmd.add_command_line_arguments(r'cpus={}'.format(params.settings.max_cpus_per_job))
    cmd.add_command_line_arguments(r'labelling=foldername')
    cmd.add_command_line_arguments(pdb_eff)
    cmd.add_command_line_arguments(r'out_dir={}'.format(sweep_dir))
    cmd.print_settings()

    return cmd

def create_sweeps(params, log=None):

    if log is None: log = Log(verbose=True)

    # List of jobs
    jobs = []

    # -------------------------------------->
    # Create sweep by refinement type
    # -------------------------------------->
    for ref_type in params.options.refinements:
        log.heading('Creating jobs to analyse {}-refined structures'.format(ref_type))

        # Look for PDB files
        all_pdbs = sorted(glob.glob(os.path.join(params.input.refinement_dir, '*', '*-{}.pdb'.format(ref_type))))
        log('Found {} files.'.format(len(all_pdbs)))

        out_dir = easy_directory(os.path.join(params.output.out_dir, ref_type))
        log('Outputting to directory: {}'.format(out_dir))

        # Read in the models
        log('Loading datasets')
        datasets = [ModelAndData.from_file(model_filename=p, data_filename=p.replace('.pdb','.mtz')) for p in all_pdbs]

        # Extract the resolution limits
        resolution_limits = numpy.array([d.data.summary.high_res for d in datasets])
        log(str(resolution_limits))

        # -------------------------------------->
        # Iterate through the resolution range partitioning
        # -------------------------------------->
        for r_step in params.sweep_parameters.resolution_partitioning:
            log.heading('Partitioning datasets by {}A'.format(r_step))

            # Loop variables
            r_min = 0.0
            r_max = resolution_limits.max()

            # Iterate through all resolutions
            while r_min < r_max:
                print r_step, r_min, r_max

                # Resolution limits for this loop
                r_loop_min = r_min
                r_loop_max = r_min+r_step
                # Datasets between these resolutions
                d_sel = (resolution_limits <= r_loop_max) * (resolution_limits > r_loop_min)
                n_sel = sum(d_sel)
                log('{} new datasets at this resolution'.format(n_sel))

                # Update loop variable
                r_min = r_loop_max

                # No datasets selected: skip to next iteration
                if n_sel == 0:
                    continue

                # Select datasets at this resolution block
                r_loop_datasets = [d for i,d in zip(d_sel, datasets) if i]
                r_loop_d_reslns = resolution_limits[d_sel]

                # -------------------------------------->
                # Iterate through number of datasets for characterisation
                # -------------------------------------->
                for n_use in params.sweep_parameters.datasets_per_characterisation:
                    if n_use > n_sel: continue
                    log('> Sampling sets of {} datasets from {} datasets'.format(n_use, len(r_loop_datasets)))

                    # -------------------------------------->
                    # How many times to bootstrap this selection
                    # -------------------------------------->
                    n_bootstrap = 10
                    for i_sample in range(n_bootstrap):
                        # Sample datasets for this round
                        sel_dataset = numpy.random.choice(r_loop_datasets, size=n_use, replace=False)

#                        # -------------------------------------->
#                        # Iterate through optimisation cutoffs
#                        # -------------------------------------->
#                        for opt_cutoff in params.sweep_parameters.optimisation_cutoffs:
#                            if opt_cutoff > r_loop_max: continue
#
#                            # Number of datasets actually used for optimisation
#                            n_eff = sum(r_loop_d_reslns < opt_cutoff)
#                            if n_eff == 0: continue

                        #############################################
                        # Actually generate the sweep folders, etc.
                        #############################################
                        sweep_dir = easy_directory(os.path.join(out_dir,
                                                 'range_{}-{}'.format(r_loop_min,r_loop_max)+
                                                 '_ndatasets_{}'.format(n_use)+
                                                 '_runnumber_{}'.format(i_sample+1)
                                                 #'_optcutoff_{}'.format(opt_cutoff)+
                                                 #'_optndsets_{}'.format(n_eff)
                                                ))
                        job = actually_generate_sweep(sweep_dir = sweep_dir,
                                                      input_pdbs = [d.model.filename for d in sel_dataset],
                                                      #opt_cutoff = opt_cutoff,
                                                      params = params)

                        jobs.append(job)

    return jobs

def write_sweeps(jobs, params, log=None):

    if log is None: log = Log(verbose=True)

    log.heading('Writing commands for {} jobs'.format(len(jobs)))

    # Store all commands in a sub dir (there may be thousands)
    script_dir = easy_directory(os.path.join(params.output.out_dir, 'shell_scripts'))

    # Name of the master shell script
    out_script = os.path.join(script_dir, params.output.out_script)

    with open(out_script, 'w') as fh:
        fh.write('#!/usr/bin/env bash -l\n\n')
        for i, j in enumerate(jobs):
            log.subheading('Job {} of {}'.format(i+1, len(jobs)))
            log(str(j))
            job_file = os.path.splitext(out_script)[0]+'-job-{}.sh'.format(i+1)
            log('Writing command to {}'.format(job_file))
            with open(job_file, 'w') as jf:
                cmd = j.as_command()
                jf.write('#!/usr/bin/env bash -l\n\n')
                jf.write(cmd+'\n')
#            fh.write('qsub -pe orte {} {}\n'.format(j.n_cpus, os.path.abspath(job_file)))
            fh.write('qsub {}\n'.format(os.path.abspath(job_file)))

def run_sweeps(jobs, params, log=None):

    if log is None: log = Log(verbose=True)

    log.heading('Running {} jobs'.format(len(jobs)))

    if params.settings.concurrent_jobs > 1:
        for i, j in enumerate(jobs):
            log.subheading('Job {} of {}'.format(i+1, len(jobs)))
            log(str(j))
        log.subheading('Running {} jobs ({} at a time)'.format(len(jobs),params.settings.concurrent_jobs))
        ret = libtbx.easy_mp.pool_map(processes=params.settings.concurrent_jobs, func=wrapper_run, args=jobs, chunksize=1)
    else:
        for i, j in enumerate(jobs):
            log.subheading('Job {} of {}'.format(i+1, len(jobs)))
            log(str(j))
            ret = wrapper_run(j)

#    for i, j in enumerate(ret):
#        j.write_output('job-{}.log'.format(i+1))

############################################################################

def run(params):

    params.output.out_dir = os.path.abspath(params.output.out_dir)
    params.input.refinement_dir = os.path.abspath(params.input.refinement_dir)

    easy_directory(params.output.out_dir)

    validate_parameters(params)

    if params.input.pdb: run_refinements(params)

    jobs = create_sweeps(params)

    if params.output.out_script is not None:
        write_sweeps(jobs, params)
    else:
        run_sweeps(jobs, params)

    #embed()

############################################################################

if __name__=='__main__':
    from giant.jiffies import run_default
    run_default(
        run                 = run,
        master_phil         = master_phil,
        args                = sys.argv[1:],
        blank_arg_prepend   = blank_arg_prepend,
        program             = PROGRAM,
        description         = DESCRIPTION)


