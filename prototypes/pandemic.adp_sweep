#!/usr/bin/env ccp4-python

import os, sys, copy, glob

import numpy, pandas, json

import libtbx.phil, libtbx.easy_mp
import iotbx.pdb

from libtbx.utils import Sorry, Failure

from bamboo.common.logs import Log
from bamboo.common.path import easy_directory, rel_symlink
from bamboo.common.command import CommandManager

from giant.dataset import ModelAndData

from giant.structure.select import protein, backbone, sidechains
from giant.structure.formatting import PhenixSelection

import matplotlib
matplotlib.interactive(False)
from matplotlib import pyplot
pyplot.switch_backend('agg')
pyplot.interactive(0)

numpy.set_printoptions(threshold=numpy.nan)

from IPython import embed

############################################################################

PROGRAM = 'pandemic.adp_sweep'

DESCRIPTION = """
    Fit a series of B-factor models to a series of B-factor refinements
"""

############################################################################

blank_arg_prepend = {None:'refinement_dir=', '.pdb':'pdb='}

master_phil = libtbx.phil.parse("""
input {
    pdb = None
        .help = "input pdb files for refinement"
        .multiple = True
        .type = str
    cif = None
        .type = str
    refinement_dir = None
        .help = "input directory, containing different B-factor refinements"
        .optional = False
        .type = str
    labelling = filename *foldername
        .type = choice
}
output {
    out_dir = adp-characterisation-sweep
        .help = "output directory"
        .type = str
    out_script = run_adp_sweeps.sh
        .help = "output script to run jobs"
        .type = str
}
options {
    refinements = isotropic tls *anisotropic
        .type = choice(multi=True)
    fitting_groups = *chain *auto_group *secondary_structure *residue *backbone *sidechain atom
        .type = choice(multi=True)
}
sweep_parameters {
    resolution_partitioning = *0.1 *0.5 *1.0 *10.0
        .type = choice(multi=True)
        .help = "Split datasets into groups for characterisation by resolution."
    datasets_per_characterisation = *2 *3 *4 *5 *6 *7 *8 *9 *10 *15 *20 *30 *40 *50 *100 *150 *200
        .type = choice(multi=True)
        .help = "Once datasets are partitioned, select random samples of datasets."
    optimisation_cutoffs = *1.5 *1.6 *1.7 *1.8 *1.9 *2.0 *2.5 *3.0 *3.5
        .type = choice(multi=True)
        .help = "Try different optimisation cutoffs for datasets in a partitioned group - can high resolution datasets parameterise low-resolution datasets?"
}
process {
    out_csv = combined_data.csv
        .type = str
}
settings {
    max_cpus_per_job = 1
        .type = int
        .multiple = False
    concurrent_jobs = 1
        .type = int
        .multiple = False
}
""", process_includes=True)

############################################################################

def wrapper_run(arg):
    return arg.run()

def validate_parameters(params):
    assert os.path.exists(params.input.refinement_dir)
    params.input.refinement_dir = os.path.abspath(params.input.refinement_dir)

    params.sweep_parameters.resolution_partitioning         = map(float, params.sweep_parameters.resolution_partitioning        )
    print 'Resolution partitions: {}'.format(params.sweep_parameters.resolution_partitioning)
    params.sweep_parameters.datasets_per_characterisation   = map(int,   params.sweep_parameters.datasets_per_characterisation  )
    print 'Datasets per characterisation: {}'.format(params.sweep_parameters.datasets_per_characterisation)
    params.sweep_parameters.optimisation_cutoffs            = map(float, params.sweep_parameters.optimisation_cutoffs           )
    print 'Optimisation cutoffs: {}'.format(params.sweep_parameters.optimisation_cutoffs)

    pass


def actually_generate_sweep(sweep_dir,
                            input_pdbs,
                            params,
                            opt_cutoff=None):

    sweep_dir = easy_directory(sweep_dir)

    # Write all pdb files to an input file to pass to program
    pdb_eff = os.path.join(sweep_dir, 'input_pdbs.eff')
    with open(pdb_eff, 'w') as fh:
        fh.write('\n'.join([r'pdb="{}"'.format(p) for p in input_pdbs]))

    cmd = CommandManager("pandemic.adp")
    cmd.add_command_line_arguments(r'cpus={}'.format(params.settings.max_cpus_per_job))
    cmd.add_command_line_arguments(r'labelling=foldername')
    cmd.add_command_line_arguments(pdb_eff)
    cmd.add_command_line_arguments(r'out_dir={}'.format(sweep_dir))
    if opt_cutoff is not None:
        cmd.add_command_line_arguments(r'max_resolution_for_optimisation={}'.format(opt_cutoff))

    cmd.print_settings()

    return cmd

def create_sweeps(params, log=None):

    if log is None: log = Log(verbose=True)

    # List of jobs
    jobs = []

    ######################################################################
    # -------------------------------------->
    # Create sweep by refinement type
    # -------------------------------------->
    ######################################################################
    for ref_type in params.options.refinements:
        log.heading('Creating jobs to analyse {}-refined structures'.format(ref_type))

        # Look for PDB files
        all_pdbs = sorted(glob.glob(os.path.join(params.input.refinement_dir, '*', '*-{}.pdb'.format(ref_type))))
        log('Found {} files.'.format(len(all_pdbs)))

        out_dir = easy_directory(os.path.join(params.output.out_dir, ref_type))
        log('Outputting to directory: {}'.format(out_dir))

        # Read in the models
        log('Loading datasets')
        datasets = [ModelAndData.from_file(model_filename=p, data_filename=p.replace('.pdb','.mtz')) for p in all_pdbs]

        # Extract the resolution limits
        resolution_limits = numpy.array([d.data.summary.high_res for d in datasets])
        log(str(resolution_limits))

        ######################################################################
        # -------------------------------------->
        # Iterate through the resolution range partitioning
        # -------------------------------------->
        ######################################################################
        for r_step in params.sweep_parameters.resolution_partitioning:
            log.heading('Partitioning datasets by {}A'.format(r_step))

            # Loop variables
            r_min = 0.0
            r_max = resolution_limits.max()

            # Iterate through all resolutions
            while r_min < r_max:
                print r_step, r_min, r_max

                # Resolution limits for this loop
                r_loop_min = r_min
                r_loop_max = r_min+r_step
                # Datasets between these resolutions
                d_sel = (resolution_limits <= r_loop_max) * (resolution_limits > r_loop_min)
                n_sel = sum(d_sel)
                log('{} new datasets at this resolution'.format(n_sel))

                # Update loop variable
                r_min = r_loop_max

                # No datasets selected: skip to next iteration
                if n_sel == 0:
                    continue

                # Select datasets at this resolution block
                r_loop_datasets = [d for i,d in zip(d_sel, datasets) if i]
                r_loop_d_reslns = resolution_limits[d_sel]

                # -------------------------------------->
                # Iterate through number of datasets for characterisation
                # -------------------------------------->
                for n_use in params.sweep_parameters.datasets_per_characterisation:
                    if n_use > n_sel: continue
                    log('> Sampling sets of {} datasets from {} datasets'.format(n_use, len(r_loop_datasets)))

                    # -------------------------------------->
                    # How many times to bootstrap this selection
                    # -------------------------------------->
                    n_bootstrap = 10
                    for i_sample in range(n_bootstrap):
                        # Sample datasets for this round
                        sel_dataset = numpy.random.choice(r_loop_datasets, size=n_use, replace=False)

                        #############################################
                        # Actually generate the sweep folders, etc.
                        #############################################
                        sweep_dir = easy_directory(os.path.join(out_dir,
                                                 'range_{}-{}'.format(r_loop_min,r_loop_max)+
                                                 '_ndatasets_{}'.format(n_use)+
                                                 '_runnumber_{}'.format(i_sample+1)
                                                 #'_optcutoff_{}'.format(opt_cutoff)+
                                                 #'_optndsets_{}'.format(n_eff)
                                                ))
                        job = actually_generate_sweep(sweep_dir = sweep_dir,
                                                      input_pdbs = [d.model.filename for d in sel_dataset],
                                                      params = params)

                        jobs.append(job)

        ######################################################################
        # -------------------------------------->
        # Create one sweep through optimisation cutoffs for all datasets
        # -------------------------------------->
        ######################################################################
        out_dir = easy_directory(os.path.join(params.output.out_dir, ref_type+'-opt-cutoff'))
        for opt_cutoff in params.sweep_parameters.optimisation_cutoffs:

            # Number of datasets actually used for optimisation
            n_eff = sum(resolution_limits < opt_cutoff)
            # Skip if none above cutoff
            if n_eff == 0: continue

            # Create sweeps
            sweep_dir = easy_directory(os.path.join(out_dir,
                                     'all'+
                                     '_optcutoff_{}'.format(opt_cutoff)+
                                     '_optndsets_{}'.format(n_eff)
                                    ))
            job = actually_generate_sweep(sweep_dir = sweep_dir,
                                          input_pdbs = [d.model.filename for d in datasets],
                                          opt_cutoff = opt_cutoff,
                                          params = params)
            jobs.append(job)

            # Break if this is higher than the lowest resolution dataset
            if opt_cutoff > resolution_limits.max():
                break

    return jobs

def write_sweeps(jobs, params, log=None):

    if log is None: log = Log(verbose=True)

    log.heading('Writing commands for {} jobs'.format(len(jobs)))

    # Store all commands in a sub dir (there may be thousands)
    script_dir = easy_directory(os.path.join(params.output.out_dir, 'shell_scripts'))

    # Name of the master shell script
    out_script = os.path.join(script_dir, os.path.basename(params.output.out_script))

    with open(out_script, 'w') as fh:
        fh.write('#!/usr/bin/env bash -l\n\n')
        for i, j in enumerate(jobs):
            log.subheading('Job {} of {}'.format(i+1, len(jobs)))
            log(str(j))
            job_file = os.path.splitext(out_script)[0]+'-job-{}.sh'.format(i+1)
            log('Writing command to {}'.format(job_file))
            with open(job_file, 'w') as jf:
                cmd = j.as_command()
                jf.write('#!/usr/bin/env bash -l\n\n')
                jf.write(cmd+'\n')
            fh.write('qsub {}\n'.format(os.path.abspath(job_file)))

def process_output(params):

    log = Log(verbose=True)

    assert os.path.exists(params.output.out_dir)
    out_csv = os.path.join(params.output.out_dir, params.process.out_csv)
    assert not os.path.exists(out_csv), 'file already exists: {}'.format(out_csv)
    ref_csv = os.path.join(params.output.out_dir, 'reference_r_values.csv')
    assert os.path.exists(ref_csv), 'CSV containing reference R-free, R-work, etc, does not exist: {}'.format(ref_csv)
    ref_table = pandas.read_csv(ref_csv, index_col=0)

    COLUMN_NAMES = ['dataset', 'resolution', 'method', 'ref_type', 'n_dst', 'n_opt', 'n_run', 'res_opt', 'sel_res_range', 'sel_res_high', 'sel_res_low', 'r_free', 'r_work', 'r_gap', 'dr_free', 'dr_work', 'dr_gap']

    # Create table to contain output
    out_table = pandas.DataFrame(columns=COLUMN_NAMES)

    program_csvs = glob.glob(os.path.join(params.output.out_dir,'*/*/dataset_scores.csv'))
    assert program_csvs, 'no program csvs found'
    for data_csv in program_csvs:
        log.heading('Reading {}'.format(data_csv))

        # Read data?
        data_table = pandas.read_csv(data_csv, index_col=0)

        run_dir = os.path.dirname(data_csv)
        ref_dir = os.path.dirname(run_dir)

        run_info = os.path.basename(run_dir).split('_')
        log('> Run Info: {}'.format(run_info))

        ref_type = os.path.basename(ref_dir).split('-')[0]
        log('> Refinement type: {}'.format(ref_type))

        if 'range' in run_info:
            sel_res_range = run_info[run_info.index('range')+1]
            sel_res_high, sel_res_low = sel_res_range.split('-')
        else:
            sel_res_range = sel_res_low = sel_res_high = 'all'

        if 'ndatasets' in run_info:
            n_opt = run_info[run_info.index('ndatasets')+1]
            n_dst = n_opt
            res_opt = 'all'
        elif 'optcutoff' in run_info:
            n_opt = run_info[run_info.index('optndsets')+1]
            n_dst = 'all'
            res_opt = run_info[run_info.index('optcutoff')+1]
        else:
            n_opt = None
            n_dst = None
            res_opt = None

        if 'runnumber' in run_info:
            n_run = run_info[run_info.index('runnumber')+1]
        else:
            n_run = None

        # Iterate through all of the datasets for this run
        for dataset in data_table.index:
            log(dataset)

            dataset_row = data_table.loc[dataset]
            dataset_ref = ref_table.loc[dataset]

            for method in ['single-dataset', 'multi-dataset']:
                new_row_idx = len(out_table.index)
                log('> '+str(new_row_idx)+'\t'+method)

                if method == 'single-dataset':
                    prefix = 'old-'
                elif method == 'multi-dataset':
                    prefix = 'new-'
                else:
                    raise Exception()

                resolution = dataset_ref['resolution']
                r_free = dataset_row[prefix+'R-free']
                r_work = dataset_row[prefix+'R-work']
                r_gap  = dataset_row[prefix+'R-gap']

                dr_free = r_free - dataset_ref['R-free']
                dr_work = r_work - dataset_ref['R-work']
                dr_gap  = r_gap  - dataset_ref['R-gap']

                out_table.loc[new_row_idx] = map(eval, COLUMN_NAMES)
                #log(out_table.loc[new_row_idx])

    # Write output table
    log('Writing output table of {} rows: {}'.format(len(out_table.index), out_csv))
    out_table.to_csv(out_csv)



############################################################################

def run(params):

    params.output.out_dir = os.path.abspath(params.output.out_dir)

    # Perform refinements if required
#    if params.input.pdb and not os.path.exists(params.input.refinement_dir):
#        run_refinements(params)

    # Create parameterisation or analyse them
    if not os.path.exists(params.output.out_dir):
        validate_parameters(params)
        # create mode
        easy_directory(params.output.out_dir)
        jobs = create_sweeps(params)
        write_sweeps(jobs, params)
    else:
        # analysis mode
        process_output(params)

    #embed()

############################################################################

if __name__=='__main__':
    from giant.jiffies import run_default
    run_default(
        run                 = run,
        master_phil         = master_phil,
        args                = sys.argv[1:],
        blank_arg_prepend   = blank_arg_prepend,
        program             = PROGRAM,
        description         = DESCRIPTION)


