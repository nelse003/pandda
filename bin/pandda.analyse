#!/usr/bin/env pandda.python

import os, sys, glob, time, gc
import numpy

from scitbx.array_family import flex
from scitbx.math import basic_statistics

from Bamboo.Common import Meta, Info
from Giant.Utils import rel_symlink

from Giant.Grid.Masks import grid_mask
from Giant.Xray.Maps.Occupancy import calculate_occupancy_subtracted_map, estimate_event_occupancy

from PANDDAs import welcome, graphs
from PANDDAs.analyse import PanddaMultiDatasetAnalyser, PanddaMapAnalyser, PanddaZMapAnalyser, MapHolder
from PANDDAs.analyse_html import write_initial_html, write_analyse_html
from PANDDAs.events import PointCluster, Event, cluster_events 

def pandda_main(args):
    """Run the PANDDA algorithm, using supplied args object"""

    try:

        # ============================================================================>
        #####
        # MANUAL SETTINGS
        #####
        # ============================================================================>
        # None!
        # ============================================================================>
        #####
        # Initialise
        #####
        # ============================================================================>
        pandda = PanddaMultiDatasetAnalyser(args)
        # ============================================================================>
        #####
        # Initialise Settings
        #####
        # ============================================================================>
        pandda.set_low_resolution(pandda.params.analysis.high_res_lower_limit)
        pandda.set_high_resolution(pandda.params.analysis.high_res_upper_limit)
        pandda.run_pandda_init()
        # ============================================================================>
        #####
        # Build list of files in data directories
        #####
        # ============================================================================>
        input_files = pandda.build_input_list()
        # Check that some datasets have been found or already loaded
        if (not pandda.datasets.all()) and (not input_files):
            pandda.exit()
            raise SystemExit('NO DATASETS LOADED')
        # Check to see if we're reusing statistical maps
        if (not pandda.args.method.recalculate_statistical_maps) and pandda.stat_maps.get_resolutions():
            pandda.log('===================================>>>', True)
            pandda.log('REUSING STATISTICAL MAPS AT: {!s}'.format(pandda.stat_maps.get_resolutions()), True)
        # Check that enough datasets have been found
        elif pandda.datasets.size() + len(input_files) < pandda.params.analysis.min_build_datasets:
            pandda.log('===================================>>>', True)
            pandda.log('NOT ENOUGH DATASETS TO BUILD DISTRIBUTIONS!', True)
            pandda.log('Number loaded ({!s}) is less than the {!s} needed.'.format(pandda.datasets.size()+len(input_files), pandda.params.analysis.min_build_datasets), True)
            pandda.log('This value is defined by pandda.params.analysis.min_build_datasets', True)
            raise SystemExit('NOT ENOUGH DATASETS LOADED')
        # If dry_run, exit after initial search
        if pandda.args.settings.exit_flags.dry_run:
            return pandda, None
        # Load and process input files
        if input_files:
            # ============================================================================>
            #####
            # Add new files and load datasets
            #####
            # ============================================================================>
            pandda.add_new_files(input_files)
            pandda.load_new_datasets()
            pandda.initialise_analysis()
            # ============================================================================>
            #####
            # Set Reference Dataset
            #####
            # Select the reference dataset
            # ============================================================================>
            if not pandda.reference_dataset():
                # Select the reference dataset
                ref_pdb, ref_mtz = pandda.select_reference_dataset(method='resolution')
                # Load the reference dataset
                pandda.load_reference_dataset(ref_pdb=ref_pdb, ref_mtz=ref_mtz)
            # ============================================================================>
            #####
            # Scale, Align and Initial-Filter All Data
            #####
            # TODO CHANGE SCALING TO "LOAD_REFLECTION_DATA"
            # ============================================================================>
            # Filter out datasets with different protein structures
            pandda.filter_datasets_1()
            # Scale and align the datasets to the reference
            pandda.load_reflection_data(ampl_label=pandda.params.maps.ampl_label,
                                        phas_label=pandda.params.maps.phas_label)
#            pandda.scale_datasets()
            pandda.align_datasets(method=pandda.params.alignment.method)
        else:
            # ============================================================================>
            # Rebuild the masks of the rejected datasets (quick)
            # ============================================================================>
            pandda.initialise_analysis()
            pandda.filter_datasets_1()
        # ============================================================================>
        # Check that enough VALID datasets have been found
        # ============================================================================>
        if (not pandda.args.method.recalculate_statistical_maps) and pandda.stat_maps.get_resolutions():
            # Using existing maps - don't need to check
            pass
        elif pandda.datasets.size(mask_name='rejected - total', invert=True) < pandda.params.analysis.min_build_datasets:
            pandda.log('===================================>>>', True)
            pandda.log('NOT ENOUGH (NON-REJECTED) DATASETS TO BUILD DISTRIBUTIONS!', True)
            pandda.log('Number loaded ({!s}) is less than the {!s} needed.'.format(pandda.datasets.size(mask_name='rejected - total', invert=True), pandda.params.analysis.min_build_datasets), True)
            pandda.log('This value is defined by pandda.params.analysis.min_build_datasets', True)
            raise SystemExit('NOT ENOUGH DATASETS LOADED')

        # ============================================================================>
        #####
        # Filter and Analyse the Datasets
        #####
        # ============================================================================>

        pandda.log('============================================================================>>>', True)
        pandda.log('Preparing for Analysis', True)

        # Collate many variables across the datasets to be used for filtering
        pandda.analyse_dataset_variability_1()
        # Filter out the datasets that are not isomorphous and therefore incomparable
        pandda.filter_datasets_2()
        # Analyses the crystallographic and structural variability of the datasets
        pandda.calculate_mean_structure_and_protein_masks(deviation_cutoff=0.5)
        pandda.analyse_dataset_variability_2()
        # ============================================================================>
        #####
        # Update Settings
        #####
        # ============================================================================>
        # Update the resolution limits using the resolution limits from the datasets supplied
        if pandda.params.analysis.dynamic_res_limits:
            pandda.set_low_resolution(   min(   pandda.get_low_resolution(),
                                                max(pandda.tables.dataset_info['high_resolution']) ) )
            pandda.set_high_resolution(  max(   pandda.get_high_resolution(),
                                                min(pandda.tables.dataset_info['high_resolution']) ) )
            pandda.log('===================================>>>')
            pandda.log('UPDATED RESOLUTION LIMITS')
            pandda.log('LOW RESOLUTION:  {!s}'.format(pandda.get_low_resolution()))
            pandda.log('HIGH RESOLUTION: {!s}'.format(pandda.get_high_resolution()))
        else:
            pandda.log('===================================>>>')
            pandda.log('**NOT** UPDATING RESOLUTION LIMITS')
            pandda.log('LOW RESOLUTION:  {!s}'.format(pandda.get_low_resolution()))
            pandda.log('HIGH RESOLUTION: {!s}'.format(pandda.get_high_resolution()))
        # ============================================================================>
        #####
        # Create Sample Grid
        #####
        # Create reference grid based on the reference structure
        # ============================================================================>
        if pandda.reference_grid() is None:
            # Create parameter for setting grid spacing (multiple grids?)
            pandda.create_reference_grid(   grid_spacing     = pandda.params.maps.grid_spacing,
                                            expand_to_origin = False,
                                            buffer           = pandda.params.masks.outer_mask+pandda.params.maps.padding    )
            # Create various masks to define regions of the grid by distance to the protein and symmetry copies
            pandda.mask_reference_grid()

        # ============================================================================>
        # Print the summaries
        # ============================================================================>
        pandda.log('===================================>>>', True)
        pandda.log('Grid Summary: ', True)
        pandda.log(pandda.reference_grid().summary(), True)
        pandda.log(pandda.reference_grid().local_mask().summary(), True)
        pandda.log(pandda.reference_grid().global_mask().summary(), True)

        # ============================================================================>
        # ============================================================================>
        # ============================================================================>
        # ============================================================================>
        # ============================================================================>
        # TODO TODO TODO - MOVE?
        # Calculate centre of mass of the reference dataset
        pandda.log('===================================>>>', True)
        pandda.log('Calculating Reference Structure Centre of Mass', True)

        # Should be in the reference frame
        ref_structure = pandda.reference_dataset().hierarchy()
        backbone_atoms = ref_structure.select(ref_structure.atom_selection_cache().selection('pepnames and (name CA or name C or name O or name N)')).atoms()
        backbone_sites = backbone_atoms.extract_xyz()
        backbone_wghts = flex.double([1]*len(backbone_sites))

        centre_of_mass = backbone_sites.mean_weighted(weights=backbone_wghts)
        # TODO TODO TODO
        # ============================================================================>
        # ============================================================================>
        # ============================================================================>
        # ============================================================================>
        # ============================================================================>
        
        # ============================================================================>
        #####
        # STORING FOR REUSE
        #####
        # ============================================================================>
        # Pickle all of the large arrays so they can be reloaded
        # ============================================================================>
        pandda.pickle_the_pandda(all=True)

        # ============================================================================>
        #####
        # PRE-ANALYSIS ANALYSIS (DUMP OF DATASET PARAMETERS)
        #####
        # ============================================================================>
        pandda.write_output_csvs()
        pandda.write_dataset_summary_graphs()
        write_initial_html(pandda)
            
        # ============================================================================>
        #####
        # PREPARE VARIABLES TO LOOP OVER RESOLUTION SHELLS
        #####
        # ============================================================================>
        # Calculate cutoffs for resolution shells
        # ============================================================================>
        if (not pandda.args.method.recalculate_statistical_maps) and pandda.stat_maps.get_resolutions():
            # Use the resolution limits of previously run campaigns
            res_limits = pandda.stat_maps.get_resolutions()
            assert res_limits, 'No Resolution Limits found from statistical maps: {!s}'.format(res_limits)
            min_limit = min(res_limits)
            max_limit = max(res_limits)
        else:
            # Set pandda.args.method.recalculate_statistical_maps to True as no statistical maps have been found
            if not pandda.args.method.recalculate_statistical_maps:
                pandda.log('No Statistical Maps Found: Setting pandda.args.method.recalculate_statistical_maps to True', True)
                pandda.args.method.recalculate_statistical_maps = True
            # ============================================================================>
            # Select resolution limits based on dataset resolutions and given arguments
            # ============================================================================>
            if pandda.params.analysis.dynamic_res_limits:
                # Round the high limit DOWN to the nearest 0.01
                min_limit = round(pandda.get_high_resolution()- 0.005, 2)    # i.e. 1.344 -> 1.34
                # Round the low limit UP to the nearest 0.01
                max_limit = round(pandda.get_low_resolution() + 0.005, 2)    # i.e. 3.423 -> 3.43
            else:
                # Take the arguments as given by the user
                min_limit = pandda.get_high_resolution()
                max_limit = pandda.get_low_resolution()
            # ============================================================================>
            # Create resolution shells (or single limit)
            # ============================================================================>
            if not pandda.params.analysis.high_res_increment:
                # No variable cutoff - select all 
                res_limits = [max_limit]  # i.e. [2]
            else:
                # Calculate a range of resolution limits
                shell_width = pandda.params.analysis.high_res_increment
                res_limits = [round(x, 4) for x in numpy.arange(min_limit, max_limit, shell_width).tolist()]
                # Append the rounded max_limit to the end to ensure that the last dataset is processed            
                if res_limits[-1] != max_limit:
                    res_limits.append(max_limit)
        # ==================================================>
        # Initialise for iterations over shells
        # ============================================================================>
        # Analyse all datasets from high_shell_limit -> cut_resolution (initialise high_shell_limit to 0)
        high_shell_limit = 0
        # Record how many datasets are processed at each resolution
        resolution_count = {}
        # ============================================================================>
        # Report
        # ============================================================================>
        pandda.log('===================================>>>', True)
        if len(res_limits)==1:
            pandda.log('Analysing All Maps at {!s}A'.format(max_limit), True)
        else:
            pandda.log('Analysing Resolution Shells from {!s} -> {!s}A'.format(min_limit, max_limit), True)
            pandda.log('Limits: {!s}'.format(', '.join(map(str,res_limits))), True)

        # ============================================================================>
        #####
        # ANALYSE DATASETS - ITERATE THROUGH RESOLUTION SHELLS
        #####
        # ============================================================================>
        t_analysis_start = time.time()
        # ==================================================>
        pandda.log('===================================>>>', True)
        pandda.log('Dataset Analysis Started: {!s}'.format(time.strftime("%a, %d %b %Y %H:%M:%S", time.gmtime(t_analysis_start))), True)

        for cut_resolution in res_limits:

            # Which resolutions will be processed in this shell
            pandda.log('===================================>>>', True)
            pandda.log('Looking for Datasets to Process from {!s}A -> {!s}A'.format(high_shell_limit, cut_resolution), True)

            # ============================================================================>
            # Select datasets to build the distributions
            # ============================================================================>
            building_mask_name = 'selected for building @ {!s}A'.format(cut_resolution)
            if pandda.args.method.recalculate_statistical_maps:
                pandda.log('===================================>>>', True)
                pandda.log('Selecting Building Mask', True)
                building_mask = pandda.select_for_building_distributions(high_res_cutoff = cut_resolution,
                                                                         building_mask_name = building_mask_name)
                # Check that we have enough datasets to build distributions
                if sum(building_mask) < pandda.params.analysis.min_build_datasets:
                    # Don't have enough to construct robust distributions
                    pandda.log('NOT ENOUGH DATASETS TO CALCULATE DISTRIBUTIONS ({!s}<{!s})'.format(sum(building_mask),pandda.params.analysis.min_build_datasets), True)
                    continue
                else:
                    # Have enough to generate robust distributions
                    pandda.log('ENOUGH DATASETS -> PROCESSING THIS RESOLUTION', True)
                    pandda.log('Building Distributions using {!s} Datasets'.format(sum(building_mask)), True)
            else:
                pandda.log('===================================>>>', True)
                pandda.log('**NOT** Selecting Building Mask (Using Existing Statistical Maps)', True)
                # Create a dummy mask as we won't be using any datasets for building
                building_mask = [False]*pandda.datasets.size()
                pandda.datasets.all_masks().add_mask(mask_name=building_mask_name, mask=building_mask)

            # ============================================================================>
            # Select the datasets to analyse
            # ============================================================================>
            analysis_mask_name = 'selected for analysis @ {!s}A'.format(cut_resolution)
            pandda.log('===================================>>>', True)
            pandda.log('Selecting Analysis Mask', True)
            analysis_mask = pandda.select_for_analysis(high_res_large_cutoff = cut_resolution, 
                                                       high_res_small_cutoff = high_shell_limit, 
                                                       analysis_mask_name = analysis_mask_name)
            # Check that there're some datasets to analyse
            if sum(analysis_mask) == 0:
                pandda.log('NO DATASETS TO ANALYSE @ {!s}'.format(cut_resolution))
                continue
            else:
                pandda.log('Calculating Z-Maps for {!s} Datasets'.format(sum(analysis_mask)), True)

            # ============================================================================>
            # Combine the masks as we will need to load maps for all datasets
            # ============================================================================>
            pandda.log('===================================>>>', True)
            pandda.log('Combining (Analysis and Building) Masks', True)
            map_load_mask = pandda.datasets.all_masks().combine_masks([analysis_mask_name, building_mask_name])
            map_load_mask_name = 'selected for loading maps @ {!s}A'.format(cut_resolution)
            pandda.datasets.all_masks().add_mask(mask_name=map_load_mask_name, mask=map_load_mask)

            # ============================================================================>
            # Report
            # ============================================================================>
            pandda.log('===================================>>>')
            pandda.log('Mask Names for Building, Loading, and Analysis')
            pandda.log('Building ({!s} datasets): {!s}'.format(sum(building_mask), building_mask_name))
            pandda.log('Load Map ({!s} datasets): {!s}'.format(sum(map_load_mask), map_load_mask_name))
            pandda.log('Analysis ({!s} datasets): {!s}'.format(sum(analysis_mask), analysis_mask_name))
            pandda.log('===================================>>>', True)
            pandda.log('Loading Maps for {!s} Datasets at {!s}A'.format(pandda.datasets.size(mask_name=map_load_mask_name), cut_resolution), True)

            # ============================================================================>
            #####
            # LOAD AND ANALYSE MAPS
            #####
            # ============================================================================>
            t_loop_start = time.time()
            # ==================================================>
            pandda.log('{!s}A Analysis Started: {!s}'.format(cut_resolution, time.strftime("%a, %d %b %Y %H:%M:%S", time.gmtime(t_loop_start))), True)

            # ============================================================================>
            # Update limit (cut resolution becomes top limit in next shell)
            # ============================================================================>
            high_shell_limit = cut_resolution

            # ============================================================================>
            #####
            # Truncate data, load and scale maps to reference dataset
            #####
            # ============================================================================>
            # Truncate the data to a particular resolution
            # ============================================================================>
            pandda.truncate_scaled_data(dataset_handlers = pandda.datasets.mask(mask_name=map_load_mask_name))
            # ============================================================================>
            # Load the reference map so that we can scale the individual maps to this
            # ============================================================================>
            ref_map_holder = pandda.load_reference_map( map_resolution = cut_resolution )
            # ============================================================================>
            # Load the required maps
            # ============================================================================>
            map_holder_list = pandda.load_and_morph_maps( dataset_handlers = pandda.datasets.mask(mask_name=map_load_mask_name),
                                                          ref_map_holder   = ref_map_holder,
                                                          map_resolution   = cut_resolution     )
            # ============================================================================>
            # Extract the statistical maps at this resolution (if requested)
            # ============================================================================>
            if pandda.args.method.recalculate_statistical_maps:
                # No Statistical maps - set to None so that they are created
                statistical_maps = None
            else:
                # Try to find a statistical map at this resolution
                statistical_maps = pandda.stat_maps.get(cut_resolution)
            # ============================================================================>
            # Create an object to hold all of the maps, and can be used to calculate the mean maps, etc...
            # ============================================================================>
            map_analyser = PanddaMapAnalyser(   dataset_maps     = map_holder_list,
                                                meta             = Meta({'resolution'   : cut_resolution,
                                                                         'grid_size'    : pandda.reference_grid().grid_size(),
                                                                         'grid_size_1d' : pandda.reference_grid().grid_size_1d()}),
                                                statistical_maps = statistical_maps,
                                                parent           = pandda )
            # ============================================================================>
            # Add the analysis mask to the map analyser        
            # ============================================================================>
            map_analyser.dataset_maps.all_masks().add_mask(mask_name=analysis_mask_name, mask=[False]*map_analyser.dataset_maps.size())
            for dh in pandda.datasets.mask(mask_name=analysis_mask_name):
                map_analyser.dataset_maps.all_masks().set_mask_value(mask_name=analysis_mask_name, entry_id=dh.tag, value=True)
            # ============================================================================>
            # Add the building mask to the map analyser
            # ============================================================================>
            map_analyser.dataset_maps.all_masks().add_mask(mask_name=building_mask_name, mask=[False]*map_analyser.dataset_maps.size())
            for dh in pandda.datasets.mask(mask_name=building_mask_name):
                map_analyser.dataset_maps.all_masks().set_mask_value(mask_name=building_mask_name, entry_id=dh.tag, value=True)

            # ============================================================================>
            #####
            # Calculate Statistical Maps (if required) and Calculate Dataset Uncertainties
            #####
            # ============================================================================>
            if pandda.args.method.recalculate_statistical_maps:
                pandda.log('===================================>>>', True)
                pandda.log('Building Map Distributions for {!s} Datasets at {!s}A'.format(pandda.datasets.size(mask_name=building_mask_name), cut_resolution), True)
            else:
                pandda.log('===================================>>>', True)
                pandda.log('Using Existing Map Distributions at {!s}A'.format(cut_resolution), True)
                assert pandda.datasets.size(mask_name=building_mask_name) == 0, 'BUILDING MASKS HAVE BEEN SELECTED WHEN MAPS ALREADY EXIST'
                assert map_analyser.dataset_maps.size(mask_name=building_mask_name) == 0, 'BUILDING MASKS HAVE BEEN SELECTED WHEN MAPS ALREADY EXIST'
            # ============================================================================>
            # Calculate the mean map
            # ============================================================================>
            if (not map_analyser.statistical_maps.mean_map) or pandda.args.method.recalculate_statistical_maps:
                mean_map = map_analyser.calculate_mean_map(masked_idxs = pandda.reference_grid().global_mask().outer_mask_indices(), 
                                                           mask_name   = building_mask_name)
            # ============================================================================>
            # If only Mean Map Requested -- return
            # ============================================================================>
            if pandda.args.settings.exit_flags.calculate_first_mean_map_only:
                pandda.write_array_to_map(output_file = pandda.output_handler.get_file('mean_map').format(cut_resolution),
                                                        map_data    = map_analyser.statistical_maps.mean_map     )
                return pandda, None
            # ============================================================================>
            # Plot Mean Map against Reference Map - should be fairly similar...
            # ============================================================================>
            # Plot the mean map against the reference map (unsorted)
            graphs.mean_obs_scatter(        f_name    = os.path.join(pandda.output_handler.get_dir('reference'), 'reference_against_mean_unsorted.png'),
                                            mean_vals = mean_map.select(pandda.reference_grid().global_mask().outer_mask_indices()),
                                            obs_vals  = ref_map_holder.map.select(pandda.reference_grid().global_mask().outer_mask_indices())           )
            # Plot the mean map against the reference map (sorted)
            graphs.sorted_mean_obs_scatter( f_name    = os.path.join(pandda.output_handler.get_dir('reference'), 'reference_against_mean_sorted.png'),
                                            mean_vals = sorted(mean_map.select(pandda.reference_grid().global_mask().outer_mask_indices())),
                                            obs_vals  = sorted(ref_map_holder.map.select(pandda.reference_grid().global_mask().outer_mask_indices()))   )
            # Plot the reference map distribution
            pandda.write_map_value_distribution( map_vals    = ref_map_holder.map.select(pandda.reference_grid().global_mask().outer_mask_indices()),
                                                 output_file = os.path.join(pandda.output_handler.get_dir('reference'), 'reference_map_distribution.png')     )
            # ============================================================================>
            # Calculate the uncertainty of all loaded maps (needs the mean map to have been calculated)
            # ============================================================================>
            # If not pandda.args.method.recalculate_statistical_maps, then no building_mask datasets will have been added, so don't need to be selective in this function
            map_uncties = map_analyser.calculate_map_uncertainties(masked_idxs=pandda.reference_grid().global_mask().inner_mask_indices())
            # ============================================================================>
            # Plot uncertainties of maps
            # ============================================================================>
            try:
                from ascii_graph import Pyasciigraph
                g=Pyasciigraph()
                graph_data = [(mh.tag, round(mh.meta.map_uncertainty,3)) for mh in map_analyser.dataset_maps.all()]
                for l in g.graph(label='Sorted Map Uncertainties (Ascending Order)', data=graph_data, sort=1):
                    print(l)
            except ImportError:
                print('IMPORT ERROR (ascii_graph) - CANNOT GENERATE UNCERTAINTY GRAPH')
                pass
            # ============================================================================>
            # Calculate the statistics of the maps
            # ============================================================================>
            if (not map_analyser.statistical_maps.sadj_map) or pandda.args.method.recalculate_statistical_maps:
                map_analyser.calculate_statistical_maps(masked_idxs = pandda.reference_grid().global_mask().outer_mask_indices(),
                                                        mask_name   = building_mask_name,
                                                        cpus        = pandda.args.settings.cpus)

            # ============================================================================>
            # PICKLE THE DATASETS THAT HAVE JUST BEEN PROCESSED
            # ============================================================================>
            pandda.pickle_the_pandda(components=['datasets'], datasets=pandda.datasets.mask(mask_name=analysis_mask_name))
            # ============================================================================>
            # Extract and store the statistical map objects
            # ============================================================================>
            # Only need to add if we're calculating new statistical maps
            if pandda.args.method.recalculate_statistical_maps:
                # Check to see if already in there
                if cut_resolution in pandda.stat_maps.get_resolutions():
                    pandda.log('Overwriting existing statistical maps @ {!s}A'.format(cut_resolution))
                pandda.stat_maps.add(stat_map_list=map_analyser.statistical_maps, resolution=cut_resolution, overwrite=True)
                pandda.pickle_the_pandda(components=['stat_maps'])
            # ============================================================================>
            # Write out Grid Point Distributions for interesting grid points (high modality, etc...)
            # ============================================================================>
            # TODO TODO TODO
            # TODO TODO TODO
            # ============================================================================>
            # Write Grid Point Distributions - Standard Function, just to provide an output
            # ============================================================================>
            try:
                from libtbx.math_utils import iceil
                grid_size = pandda.reference_grid().grid_size()
                num_points = min(10, min(grid_size))
                grid_points = zip(*[range(0, s, iceil(s/num_points)) for s in grid_size])
                pandda.write_grid_point_distributions(  grid_points     = grid_points,
                                                        map_analyser    = map_analyser,
                                                        output_filename = None          )
            except:
                print('UNIMPORTANT: FAILED TO WRITE AUTOMATIC DISTRIBUTION OF GRID POINTS')
                raise

            # ============================================================================>
            #####
            # DATA PROCESSING
            #####
            # Calculate the moments of the distributions at the grid points
            # Use the means and the stds to convert the maps to z-maps
            # Use the local mask to look for groups of significant z-values
            # ============================================================================>
            # Count the number of datasets processed at each resolution
            assert cut_resolution not in resolution_count
            resolution_count[cut_resolution] = []
            # ============================================================================>
            # Blob Search Object
            # ============================================================================>
            blob_finder = PanddaZMapAnalyser( params = pandda.params.blob_search, 
                                              grid_spacing = pandda.reference_grid().grid_spacing(),
                                              log = pandda._log )
            blob_finder.print_settings()
            # ============================================================================>
            #####
            # Calculate Z-Maps
            #####
            # ============================================================================>
            t_anal_start = time.time()
            # ==================================================>
            pandda.log('============================================================================>>>', True)
            pandda.log('Calculating Z-Maps for {!s} Datasets at {!s}A'.format(pandda.datasets.size(mask_name=analysis_mask_name), cut_resolution), True)
                
            # Iterate through and calculate z-maps
            for i_dh, d_handler in enumerate(pandda.datasets.mask(mask_name=analysis_mask_name)):
                
                pandda.log('============================================================================>>>', True)
                pandda.log('Analysing Dataset {!s} ({!s}/{!s})'.format(d_handler.tag, i_dh+1, pandda.datasets.size(mask_name=analysis_mask_name)), True)

                # ============================================================================>
                # Update tables, masks etc.
                # ============================================================================>
                # Record the which resolution this dataset was analysed at
                resolution_count[cut_resolution].append(d_handler.tag)
                # Update datasets masks flag
                pandda.datasets.all_masks().set_mask_value(mask_name='analysed', entry_id=d_handler.tag, value=True)
                # Create a file that says what resolution this was processed at
                link_dir = os.path.join(pandda.output_handler.get_dir('resolutions'), '{!s}-{!s}'.format(cut_resolution, d_handler.name))
                if not os.path.exists(link_dir): rel_symlink(orig=d_handler.output_handler.get_dir('root'), link=link_dir)
                
                # ============================================================================>
                # Validate/Check/Zero the dataset
                # ============================================================================>
                # Check that the dataset is new or being reprocessed
                if pandda.datasets.all_masks().get_mask_value(mask_name='old datasets', entry_id=d_handler.tag):
                    # Delete events from before
                    d_handler.events = []
                else:
                    # Dataset should not have any events
                    assert d_handler.events == []

                # ============================================================================>
                # Generate masks for this dataset
                # ============================================================================>
                # Generate symmetry contacts for this dataset
                dataset_sym_copies = d_handler.generate_symmetry_copies(rt_method      = pandda.params.alignment.method, 
                                                                        save_operators = False, 
                                                                        buffer         = pandda.params.masks.outer_mask+5   )
                cache = dataset_sym_copies.atom_selection_cache()
                dataset_sym_sites_cart = dataset_sym_copies.select(cache.selection('pepnames and not element H')).atoms().extract_xyz()
                # Generate custom grid mask for this dataset
                dataset_contact_mask = grid_mask(   cart_sites = dataset_sym_sites_cart,
                                                    grid_size  = pandda.reference_grid().grid_size(),
                                                    unit_cell  = pandda.reference_grid().unit_cell(),
                                                    max_dist   = pandda.params.masks.outer_mask,
                                                    min_dist   = pandda.params.masks.inner_mask )
                # Combine the standard mask with the custom mask
                grid_idxr = pandda.reference_grid().grid_indexer()
                dataset_total_mask = [gp for gp in pandda.reference_grid().global_mask().total_mask() if dataset_contact_mask.inner_mask_binary()[grid_idxr(gp)] < 0.5]
                dataset_outer_mask = [gp for gp in pandda.reference_grid().global_mask().outer_mask() if dataset_contact_mask.inner_mask_binary()[grid_idxr(gp)] < 0.5]

                # Write coordinates of symmetry copies 
                if not os.path.exists(d_handler.output_handler.get_file('symmetry_copies')):    
                    dataset_sym_copies.write_pdb_file(d_handler.output_handler.get_file('symmetry_copies'))

                ################################################
                # Extract the map for this dataset    
                ################################################
                m_handler = map_analyser.dataset_maps.get(tag=d_handler.tag)

                ################################################
                # AUTOGENERATE SCRIPTS FOR VIEWING THE DATASET #
                ################################################
                pandda.write_pymol_scripts(d_handler=d_handler)
                
                #############################################
                # CALCULATE Z-MAPS AND LOOK FOR LARGE BLOBS #
                #############################################

                pandda.log('===================================>>>', True)
                pandda.log('Calculating Z-MAPs for Dataset {!s}'.format(d_handler.tag), True)

                ##################################
                # EXTRACT MAP VALUES             #
                ##################################
                assert m_handler.map is not None, 'NO MAP FOUND'

                ##################################
                # CALCULATE MEAN-DIFF MAPS       #
                ##################################
                d_map = m_handler.map - map_analyser.statistical_maps.mean_map

                # ============================================================================>
                # NAIVE Z-MAP - NOT USING UNCERTAINTY ESTIMATION OR ADJUSTED STDS
                # ============================================================================>
                z_map_naive = map_analyser.calculate_z_map(tag=d_handler.tag, method='naive')
                # Normalise this map to N(0,1)
                z_map_naive_masked     = [z_map_naive[i] for i in pandda.reference_grid().global_mask().outer_mask_indices()]
                z_map_naive_normalised = (z_map_naive - numpy.mean(z_map_naive_masked)) / numpy.std(z_map_naive_masked)

                # ============================================================================>
                # UNCERTAINTY Z-MAP - NOT USING ADJUSTED STDS
                # ============================================================================>
                z_map_uncty = map_analyser.calculate_z_map(tag=d_handler.tag, method='uncertainty')
                # Normalise this map to N(0,1)
                z_map_uncty_masked     = [z_map_uncty[i] for i in pandda.reference_grid().global_mask().outer_mask_indices()]
                z_map_uncty_normalised = (z_map_uncty - numpy.mean(z_map_uncty_masked)) / numpy.std(z_map_uncty_masked)

                # ============================================================================>
                # ADJUSTED+UNCERTAINTY Z-MAP
                # ============================================================================>
                z_map_compl = map_analyser.calculate_z_map(tag=d_handler.tag, method='adjusted+uncertainty')
                # Normalise this map to N(0,1)
                z_map_compl_masked     = [z_map_compl[i] for i in pandda.reference_grid().global_mask().outer_mask_indices()]
                z_map_compl_normalised = (z_map_compl - numpy.mean(z_map_compl_masked)) / numpy.std(z_map_compl_masked)

                # ============================================================================>
                # ANALYSE Z-MAP FOR STATISTICAL VALIDITY
                # ============================================================================>
                # Calculate statistics of z-maps
                z_map_stats = basic_statistics(flex.double(z_map_compl_masked))
                
                # ============================================================================>
                # STORE ANALYSIS DATA IN DATASET MAP TABLE
                # ============================================================================>
                # Add to the dataset map summary table
                pandda.tables.dataset_map_info.set_value(d_handler.tag, 'analysed_resolution', m_handler.meta.resolution)
                pandda.tables.dataset_map_info.set_value(d_handler.tag, 'map_uncertainty',     m_handler.meta.map_uncertainty)
                pandda.tables.dataset_map_info.set_value(d_handler.tag, 'obs_map_mean',        m_handler.meta.obs_map_mean)
                pandda.tables.dataset_map_info.set_value(d_handler.tag, 'obs_map_rms',         m_handler.meta.obs_map_rms)
                pandda.tables.dataset_map_info.set_value(d_handler.tag, 'z_map_mean',          z_map_stats.mean)
                pandda.tables.dataset_map_info.set_value(d_handler.tag, 'z_map_std',           z_map_stats.bias_corrected_standard_deviation)
                pandda.tables.dataset_map_info.set_value(d_handler.tag, 'z_map_skew',          z_map_stats.skew)
                pandda.tables.dataset_map_info.set_value(d_handler.tag, 'z_map_kurt',          z_map_stats.kurtosis)
                
                # ============================================================================>
                # WRITE OUT DATASET INFORMATION TO CSV FILE
                # ============================================================================>
                out_list = pandda.tables.dataset_info.loc[d_handler.tag].append(pandda.tables.dataset_map_info.loc[d_handler.tag])
                out_list.to_csv(   path = d_handler.output_handler.get_file('dataset_info'),
                                   header      = True,
                                   index_label = 'dtag'    )

                # ============================================================================>
                # XXX  WHICH MAP TO DO THE BLOB SEARCHING ON  XXX
                # ============================================================================>
                if pandda.params.z_map.map_type == 'naive':
                    z_map = z_map_naive_normalised
                elif pandda.params.z_map.map_type == 'adjusted':
                    z_map = z_map_adjst_normalised
                elif pandda.params.z_map.map_type == 'uncertainty':
                    z_map = z_map_uncty_normalised
                elif pandda.params.z_map.map_type == 'adjusted+uncertainty':
                    z_map = z_map_compl_normalised

                # ============================================================================>
                # STORE THE Z MAP        
                # ============================================================================>
                z_map_holder = MapHolder(   num         = m_handler.num,
                                            tag         = m_handler.tag,
                                            map         = z_map,
                                            # Change these for the 'fake' grid unit_cell and a P1 space_group
                                            unit_cell   = pandda.reference_grid().unit_cell(),
                                            space_group = pandda.reference_grid().space_group(),
                                            meta        = Meta({'type'          : 'z-map',
                                                                'resolution'    : m_handler.meta.resolution}),
                                            parent      = m_handler  )
                # Link the sampled map handler to the z map holder
                m_handler.child = z_map_holder
                # Link the dataset handler to the child map holder
                d_handler.child = m_handler

                # ============================================================================>
                #####
                # WRITE ALL MAP DISTRIBUTIONS (THESE DON'T USE MUCH SPACE)
                #####
                # ============================================================================>
                
                # Sampled Map
                if not os.path.exists(d_handler.output_handler.get_file('s_map_png')):
                    pandda.write_map_value_distribution(map_vals     = m_handler.map.select(pandda.reference_grid().global_mask().outer_mask_indices()),
                                                        output_file  = d_handler.output_handler.get_file('s_map_png'))
                # Mean-Difference
                if not os.path.exists(d_handler.output_handler.get_file('d_mean_map_png')):
                    pandda.write_map_value_distribution(map_vals     = d_map.select(pandda.reference_grid().global_mask().outer_mask_indices()),
                                                        output_file  = d_handler.output_handler.get_file('d_mean_map_png'))
                # Naive Z-Map
                if not os.path.exists(d_handler.output_handler.get_file('z_map_naive_png')):
                    pandda.write_map_value_distribution(map_vals     = z_map_naive.select(pandda.reference_grid().global_mask().outer_mask_indices()),
                                                        output_file  = d_handler.output_handler.get_file('z_map_naive_png'),
                                                        plot_normal  = True)
                # Normalised Naive Z-Map
                if not os.path.exists(d_handler.output_handler.get_file('z_map_naive_normalised_png')):
                    pandda.write_map_value_distribution(map_vals     = z_map_naive_normalised.select(pandda.reference_grid().global_mask().outer_mask_indices()),
                                                        output_file  = d_handler.output_handler.get_file('z_map_naive_normalised_png'),
                                                        plot_normal  = True)
                # Uncertainty Z-Map
                if not os.path.exists(d_handler.output_handler.get_file('z_map_uncertainty_png')):
                    pandda.write_map_value_distribution(map_vals     = z_map_uncty.select(pandda.reference_grid().global_mask().outer_mask_indices()),
                                                        output_file  = d_handler.output_handler.get_file('z_map_uncertainty_png'),
                                                        plot_normal  = True)
                # Normalised Uncertainty Z-Map
                if not os.path.exists(d_handler.output_handler.get_file('z_map_uncertainty_normalised_png')):
                    pandda.write_map_value_distribution(map_vals     = z_map_uncty_normalised.select(pandda.reference_grid().global_mask().outer_mask_indices()),
                                                        output_file  = d_handler.output_handler.get_file('z_map_uncertainty_normalised_png'),
                                                        plot_normal  = True)
                # Corrected Z-Map
                if not os.path.exists(d_handler.output_handler.get_file('z_map_corrected_png')):
                    pandda.write_map_value_distribution(map_vals     = z_map_compl.select(pandda.reference_grid().global_mask().outer_mask_indices()),
                                                        output_file  = d_handler.output_handler.get_file('z_map_corrected_png'),
                                                        plot_normal  = True)
                # Normalised Corrected Z-Map
                if not os.path.exists(d_handler.output_handler.get_file('z_map_corrected_normalised_png')):
                    pandda.write_map_value_distribution(map_vals     = z_map_compl_normalised.select(pandda.reference_grid().global_mask().outer_mask_indices()),
                                                        output_file  = d_handler.output_handler.get_file('z_map_corrected_normalised_png'),
                                                        plot_normal  = True)
                # Plot Q-Q Plot of Corrected Z-Map to see how normal it is
                if not os.path.exists(d_handler.output_handler.get_file('z_map_qq_plot_png')):
                    pandda.write_qq_plot_against_normal(map_vals     = z_map_compl_normalised.select(pandda.reference_grid().global_mask().outer_mask_indices()),
                                                        output_file  = d_handler.output_handler.get_file('z_map_qq_plot_png'))
        
                # ============================================================================>
                #####
                # LOOK FOR CLUSTERS OF LARGE Z-SCORES  
                #####
                # ============================================================================>
                # Contour the grid at a particular Z-Value
                # ============================================================================>
                num_clusters, z_clusters = blob_finder.cluster_high_z_values(z_map=z_map, point_mask=dataset_total_mask)
                # ============================================================================>
                # Too many points to cluster -- probably a bad dataset
                # ============================================================================>
                if num_clusters == -1:
                    # This dataset is too noisy to analyse - flag!
                    pandda.datasets.all_masks().set_mask_value(mask_name='noisy zmap', entry_id=d_handler.tag, value=True)
                    # Link datasets to the noisy results directory
                    noisy_dir = os.path.join(pandda.output_handler.get_dir('noisy_datasets'), d_handler.name)
                    if not os.path.exists(noisy_dir): rel_symlink(orig=d_handler.output_handler.get_dir('root'), link=noisy_dir)

                # ============================================================================>
                #####
                # FILTER/SELECT CLUSTERS OF Z-SCORES  
                #####
                # ============================================================================>
                # Filter the clusters by size and peak height
                # ============================================================================>
                if num_clusters > 0:
                    num_clusters, z_clusters = blob_finder.filter_z_clusters_1(z_clusters=z_clusters)
                    if num_clusters == 0: pandda.log('===> Minimum cluster peak/size not reached.', True)
                # ============================================================================>
                # Filter the clusters by distance from protein
                # ============================================================================>
                if num_clusters > 0: 
                    num_clusters, z_clusters = blob_finder.filter_z_clusters_2(z_clusters       = z_clusters,
                                                                               grid_origin_cart = pandda.reference_dataset().origin_shift(),
                                                                               ref_structure    = pandda.reference_dataset().new_structure().hierarchy)
                    if num_clusters == 0: pandda.log('===> Clusters too far from protein.', True)
                # ============================================================================>
                # Group Nearby Clusters Together
                # ============================================================================>
                if num_clusters > 0:
                    num_clusters, z_clusters = blob_finder.group_clusters(z_clusters=z_clusters)
                # ============================================================================>
                # Filter the clusters by symmetry equivalence
                # ============================================================================>
                if num_clusters > 0:
                    num_clusters, z_clusters = blob_finder.filter_z_clusters_3(z_clusters       = z_clusters,
                                                                               grid_origin_cart = pandda.reference_dataset().origin_shift(),
                                                                               ref_unit_cell    = pandda.reference_dataset().unit_cell,
                                                                               ref_sym_ops      = pandda.reference_dataset().crystal_contact_generators,
                                                                               ref_structure    = pandda.reference_dataset().new_structure().hierarchy     )
                
                # ============================================================================>
                #####
                # WRITE MAPS
                #####
                # ============================================================================>
                if (num_clusters != 0) or pandda.args.settings.developer.write_maps_for_empty_datasets:
                    # ============================================================================>
                    # NATIVE MAPS (ROTATED)
                    # ============================================================================>
                    # Observed map
                    if not os.path.exists(d_handler.output_handler.get_file('native_obs_map')):
                        pandda.write_array_to_map(  output_file = d_handler.output_handler.get_file('native_obs_map'),
                                                    map_data    = pandda.rotate_map(d_handler=d_handler, map_data=m_handler.map))
                    # Z-map
                    if not os.path.exists(d_handler.output_handler.get_file('native_z_map')):
                        pandda.write_array_to_map(  output_file = d_handler.output_handler.get_file('native_z_map'),
                                                    map_data    = pandda.rotate_map(d_handler=d_handler, map_data=z_map))
                    # ============================================================================>
                    # REFERENCE FRAME MAPS (NOT ROTATED)
                    # ============================================================================>
                    # Write the sampled map
                    if not os.path.exists(d_handler.output_handler.get_file('sampled_map')):
                        pandda.write_array_to_map(  output_file = d_handler.output_handler.get_file('sampled_map'),
                                                    map_data    = m_handler.map)
                    # Write the mean-difference map
                    if not os.path.exists(d_handler.output_handler.get_file('mean_diff_map')):
                        pandda.write_array_to_map(  output_file = d_handler.output_handler.get_file('mean_diff_map'),
                                                    map_data    = d_map)
                    # Write Chosen Z-Map (ALWAYS WRITE THIS MAP)
                    if not os.path.exists(d_handler.output_handler.get_file('z_map')):
                        pandda.write_array_to_map(  output_file = d_handler.output_handler.get_file('z_map'),
                                                    map_data    = z_map)
                    # Write map of where the blobs are
                    if not os.path.exists(d_handler.output_handler.get_file('high_z_mask')):
                        highz_points = z_clusters[0][0]; [highz_points.extend(x[0]) for x in z_clusters[1:]]
                        highz_points = [map(int, v) for v in highz_points]
                        highz_map_array = numpy.zeros(pandda.reference_grid().grid_size_1d(), dtype=int)
                        highz_map_array.put(map(pandda.reference_grid().grid_indexer(), list(highz_points)), 1)
                        map_mask = flex.double(highz_map_array.tolist()); map_mask.reshape(flex.grid(pandda.reference_grid().grid_size()))
                        pandda.write_array_to_map(output_file=d_handler.output_handler.get_file('high_z_mask'), map_data=map_mask)
                    # ============================================================================>
                    # MASKS/BLOBS/GENERIC MAPS
                    # ============================================================================>
                    if pandda.args.settings.developer.write_grid_masks:
                        # Write map of grid + symmetry mask
                        if not os.path.exists(d_handler.output_handler.get_file('grid_mask')):
                            map_mask = numpy.zeros(pandda.reference_grid().grid_size_1d(), dtype=int)
                            map_mask.put(map(pandda.reference_grid().grid_indexer(), dataset_outer_mask), 1)
                            map_mask = flex.double(map_mask.tolist()); map_mask.reshape(flex.grid(pandda.reference_grid().grid_size()))
                            pandda.write_array_to_map(output_file=d_handler.output_handler.get_file('grid_mask'), map_data=map_mask)
                    # ============================================================================>
                    # Write different Z-Maps? (Probably only needed for testing)
                    # ============================================================================>
                    if pandda.args.settings.developer.write_all_z_map_types:
                        # Write Naive Z-Map
                        if not os.path.exists(d_handler.output_handler.get_file('z_map_naive')):
                            pandda.write_array_to_map(  output_file = d_handler.output_handler.get_file('z_map_naive'),
                                                        map_data    = z_map_naive)
                        # Write Normalised Naive Z-Map
                        if not os.path.exists(d_handler.output_handler.get_file('z_map_naive_normalised')):
                            pandda.write_array_to_map(  output_file = d_handler.output_handler.get_file('z_map_naive_normalised'),
                                                        map_data    = z_map_naive_normalised)
                        # Write Uncertainty Z-Map
                        if not os.path.exists(d_handler.output_handler.get_file('z_map_uncertainty')):
                            pandda.write_array_to_map(  output_file = d_handler.output_handler.get_file('z_map_uncertainty'),
                                                        map_data    = z_map_uncertainty)
                        # Write Normalised Uncertainty Z-Map
                        if not os.path.exists(d_handler.output_handler.get_file('z_map_uncertainty_normalised')):
                            pandda.write_array_to_map(  output_file = d_handler.output_handler.get_file('z_map_uncertainty_normalised'),
                                                        map_data    = z_map_uncertainty_normalised)
                        # Write Corrected Z-Map
                        if not os.path.exists(d_handler.output_handler.get_file('z_map_corrected')):
                            pandda.write_array_to_map(  output_file = d_handler.output_handler.get_file('z_map_corrected'),
                                                        map_data    = z_map_compl)
                        # Write Normalised Corrected Z-Map (ALWAYS WRITE THIS MAP)
                        if not os.path.exists(d_handler.output_handler.get_file('z_map_corrected_normalised')):
                            pandda.write_array_to_map(  output_file = d_handler.output_handler.get_file('z_map_corrected_normalised'),
                                                        map_data    = z_map_compl_normalised)
                
                # ============================================================================>
                # Skip to next dataset if no clusters found
                # ============================================================================>
                if num_clusters > 0:
                    pandda.log('===> {!s} Cluster(s) found.'.format(num_clusters), True)
                else:
                    pandda.log('===> No Clusters found.', True)
                    continue
                assert num_clusters > 0, 'NUMBER OF CLUSTERS AFTER FILTERING == 0!'

                # ============================================================================>
                # Link outputs for datasets with hits
                # ============================================================================>
                # Create a link to the interesting directories in the initial results directory
                hit_dir = os.path.join(pandda.output_handler.get_dir('interesting_datasets'), d_handler.name)
                if not os.path.exists(hit_dir): rel_symlink(orig=d_handler.output_handler.get_dir('root'), link=hit_dir)
                # Flag the dataset as interesting 
                pandda.datasets.all_masks().set_mask_value(mask_name='interesting', entry_id=d_handler.tag, value=True)
 
                # ============================================================================>
                # Process the identified features
                # ============================================================================>
                for event_idx, (event_points, event_values) in enumerate(z_clusters):
                    # Number events from 1
                    event_num = event_idx + 1
                    # Create a unique identifier for this event
                    event_key = (d_handler.tag, event_num)
                    # ============================================================================>
                    # Create a point cluster object
                    # ============================================================================>
                    point_cluster = PointCluster(id=event_key, points=event_points, values=event_values)
                    # ============================================================================>
                    # Estimate the occupancy of the detected feature 
                    # ============================================================================>
                    # Extract sites for this cluster and estimate the occupancy of the event
                    pandda.log('===================================>>>', True)
                    pandda.log('Estimating Event {!s} Occupancy'.format(event_num), True)
                    # Generate custom grid mask for this dataset
                    event_mask = grid_mask( cart_sites = flex.vec3_double(point_cluster.points)*pandda.reference_grid().grid_spacing(),
                                            grid_size  = pandda.reference_grid().grid_size(),
                                            unit_cell  = pandda.reference_grid().unit_cell(),
                                            max_dist   = 5.0,
                                            min_dist   = 0.0 )
                    expanded_points = list(event_mask.outer_mask())
                    pandda.log('=> Event sites ({!s} points) expanded to {!s} points'.format(len(point_cluster.points), len(expanded_points)))
                    # Use the outer mask to provide a reference point for the correlation to the mean map
                    reference_points = list(pandda.reference_grid().global_mask().inner_mask())
                    # Calculate the correlation with the mean map as different amounts of the mean map are subtracted
                    event_occ_est = estimate_event_occupancy( 
                        ref_map          = map_analyser.statistical_maps.mean_map,
                        query_map        = m_handler.map,
                        feature_region   = expanded_points,
                        reference_region = reference_points,
                        min_occ          = pandda.params.occupancy_estimation.min_occ,
                        max_occ          = pandda.params.occupancy_estimation.max_occ,
                        occ_increment    = pandda.params.occupancy_estimation.occ_increment,
                        method           = 'value',
                        verbose          = pandda.args.settings.verbose)
                    pandda.log('=> Event occupancy estimated as {!s}'.format(event_occ_est), True)
                    # ============================================================================>
                    # Calculate occupancy map at this occupancy
                    # ============================================================================>
                    event_occ_map = calculate_occupancy_subtracted_map(
                        ref_map      = map_analyser.statistical_maps.mean_map, 
                        query_map    = m_handler.map,
                        subtract_occ = 1.0 - event_occ_est)
                    # Normalise the event map
                    #event_occ_map = event_occ_map / event_occ_est
                    # Write out this array
                    pandda.write_array_to_map(  output_file = d_handler.output_handler.get_file('occupancy_map').format(event_num, event_occ_est), 
                                                map_data    = event_occ_map )    
                    pandda.write_array_to_map(  output_file = d_handler.output_handler.get_file('native_occupancy_map').format(event_num, event_occ_est),
                                                map_data    = pandda.rotate_map(d_handler=d_handler, map_data=event_occ_map))
                    # ============================================================================>
                    # Create an event object
                    # ============================================================================>
                    event_obj = Event(id=point_cluster.id, cluster=point_cluster)
                    event_obj.info.estimated_occupancy = event_occ_est
                    # ============================================================================>
                    # Append to dataset handler
                    # ============================================================================>
                    d_handler.events.append(event_obj)
                    # ============================================================================>
                    # Add event to the event table
                    # ============================================================================>
                    pandda.add_event_to_event_table(d_handler=d_handler, event=event_obj)

                    # ============================================================================>
                    # Write out the graph of the occupancy correlations
                    # ============================================================================>
#                    if pandda.args.output.plot_graphs:
#                        pandda.something()
                        
# XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX
# TODO REIMPLEMENT THIS IF IT CAN BE DONE SO THAT IT LOOKS NICE
#
#                # Image Blobs in the datasets using ccp4mg
#                if d_handler.hit_clusters:
#
#                    pandda.log('===================================>>>', True)
#                    pandda.log('Imaging blobs in Dataset {!s}'.format(d_handler.tag), True)
#
#                    sorted_blob_indices = d_handler.hit_clusters.sort(sorting_function=max)
#                    blob_num = len(sorted_blob_indices)
#
#                    for blob_rank, blob_grid_peak in enumerate(d_handler.hit_clusters.get_centroids(indices=sorted_blob_indices)):
#
#                        # Only produce a certain number of images
#                        if blob_rank == pandda.params.blob_search.blobs_to_image:
#                            break
#
#                        status_bar(n=blob_rank, n_max=blob_num)
#
#                        blob_cart_peak = [g*pandda.reference_grid().grid_spacing() for g in blob_grid_peak]
#
#                        # Make images of the blob
#                        pandda.image_blob(  script    = d_handler.output_handler.get_file('ccp4mg_script'),
#                                            image     = d_handler.output_handler.get_file('ccp4mg_png'),
#                                            d_handler = d_handler,
#                                            point_no  = blob_rank+1,
#                                            point     = blob_cart_peak,
#                                            towards   = centre_of_mass
#                                        )
#
#                    status_bar(n=blob_num, n_max=blob_num)
# XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX

            # XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX
            # TODO REIMPLEMENT THIS IF WE CAN MAKE IT RUN EVEN VAGUELY QUICKLY
            #
            #    ##################################
            #    # POST-PROCESS Z-MAPS            #
            #    ##################################
            #
            #    mod_z_map, resamp_mod_z_map = pandda.process_z_map(z_map=z_map)
            #
            #    # Write map
            #    pandda.write_array_to_map(  output_file  = d_handler.get_mtz_filename().replace('.mtz','.processed.zvalues.ccp4'),
            #                                map_data     = flex.double(mod_z_map))
            #    # Write down-sampled map
            #    pandda.write_array_to_map(  output_file  = d_handler.get_mtz_filename().replace('.mtz','.resamp.processed.zvalues.ccp4'),
            #                                map_data     = flex.double(resamp_mod_z_map),
            #                                grid_size    = pandda.reference_grid().resampled_grid_size(),
            #                                grid_spacing = pandda.reference_grid().resampled_grid_spacing())
            # XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX

            # ============================================================================>
            #####
            # Write the summaries
            #####
            # ============================================================================>
            pandda.write_map_analyser_summary(map_analyser=map_analyser, analysis_mask_name=analysis_mask_name)
        
            # ============================================================================>
            #####
            # Print summaries
            #####
            # ============================================================================>
            t_loop_end = time.time()
            # ==================================================>
            pandda.log('============================================================================>>>', True)
            pandda.log('{!s}A Z-Map Processing Time: {!s}'.format(cut_resolution, time.strftime("%H hours:%M minutes:%S seconds", time.gmtime(t_loop_end - t_anal_start))), True)
            pandda.log('{!s}A Total Processing Time: {!s}'.format(cut_resolution, time.strftime("%H hours:%M minutes:%S seconds", time.gmtime(t_loop_end - t_loop_start))), True)
            pandda.log('============================================================================>>>', True)
            pandda.log('@{!s}A:\t {!s}/{!s} Datasets Analysed'.format(cut_resolution, pandda.datasets.size(mask_name=analysis_mask_name), pandda.datasets.size(mask_name='rejected - total', invert=True)))
            pandda.log('Total:\t {!s}/{!s} Datasets Analysed'.format(pandda.datasets.size(mask_name='analysed'), pandda.datasets.size(mask_name='rejected - total', invert=True)))
            
            # ============================================================================>
            #####
            # PICKLE THE DATASETS THAT HAVE JUST BEEN PROCESSED
            #####
            # ============================================================================>
            # Clear the linked maps if requested
            if not pandda.args.settings.pickling.pickle_maps:
                for d_handler in pandda.datasets.mask(mask_name=analysis_mask_name):
                    d_handler.child = None           
            # Pickle the datasets 
            pandda.pickle_the_pandda(components=['datasets'], datasets=pandda.datasets.mask(mask_name=analysis_mask_name))

            # ============================================================================>
            # DELETE THE MAP ANALYSER TO FREE UP MEMORY
            # ============================================================================>            
            map_analyser.parent = None
            del map_analyser
            map_analyser = None
            # ============================================================================>
            # DELETE THE PROCESSED MAPS + Z-MAPS TO SAVE MEMORY (JUST BE CAREFUL NOT TO OVERWRITE THEIR PICKLED FILES!)
            # ============================================================================>            
            for d_handler in pandda.datasets.mask(mask_name=analysis_mask_name):
                d_handler.child = None            
            # ============================================================================>
            # Launch Garbage-Collection Manually just to be sure
            # ============================================================================>            
            gc.collect() 
            gc.collect() 
            gc.collect() 
        
            # ============================================================================>
            #####
            # LIVE ANALYSIS - RUN AT THE END OF EACH LOOP
            #####
            # Analyse the processed data
            # TODO Create a `hit processor` class
            # ============================================================================>
            # Extract all events from datasets
            all_events=[]; [all_events.extend(d.events) for d in pandda.datasets.all()]
            # Process the events if some have been found
            if all_events:
                # ==================================================>
                t_event_cluster_start = time.time()
                # ==================================================>
                # Cluster events to sites 
                site_list = cluster_events(events=all_events, cutoff=10.0/pandda.reference_grid().grid_spacing(), linkage='single')
                # Sort sites by largest Z-Value
                site_list.sort(key=lambda s: (s.info.num_events, max([e.cluster.max for e in s.children])), reverse=True).renumber()
                # Add meta to the site list
                [s.find_protein_context(hierarchy=pandda.reference_dataset().hierarchy()) for s in site_list.children]
                # Add the site information to the site table
                pandda.update_site_table(site_list=site_list, clear_table=True)
                # Update the site labels in the event table
                pandda.update_event_table_site_info(events=all_events)
                # Plot output graph of site list 
                graphs.multiple_bar_plot(   f_name    = pandda.output_handler.get_file('site_bar_graph'), 
                                            plot_vals = [sorted([e.cluster.max for e in s.children],reverse=True) for s in site_list.children]   ) 
                # ==================================================>
                t_event_cluster_end = time.time()
                # ==================================================>
                pandda.log('============================================================================>>>', True)
                pandda.log('Event Clustering: {!s}'.format(time.strftime("%H hours:%M minutes:%S seconds", time.gmtime(t_event_cluster_end - t_event_cluster_start))), True)
                # ==================================================>
                # Update the output file
                # ==================================================>
                pandda.write_output_csvs()
                write_analyse_html(pandda)
                # ==================================================>
            else:
                print 'No Events Found'

        # ============================================================================>
        #####
        # END OF MAIN LOOP
        #####
        # ============================================================================>
        t_analysis_end = time.time()
        # ==================================================>
        pandda.log('============================================================================>>>', True)
        pandda.log('Total Analysis Time: {!s}'.format(time.strftime("%H hours:%M minutes:%S seconds", time.gmtime(t_analysis_end - t_analysis_start))), True)

        # Pickle statistical maps again just to be sure
        pandda.pickle_the_pandda(components=['stat_maps'])

        # ============================================================================>
        #####
        # FINAL ANALYSIS - RUN ONCE AT THE END OF THE PROGRAM
        #####
        # Analyse all of the processed data
        # ============================================================================>
        
        pandda.log('============================================================================>>>', True)
        pandda.log('Analysis Summary', True)
            
        # Collate events
        event_total, event_num, all_dataset_events = pandda.collate_event_counts()
        # Print some slightly less important information
        pandda.log('===================================>>>', False)
        for d_tag, event_count in event_num:
            pandda.log('Dataset {!s}: {!s} Events'.format(d_tag, event_count), False)
        # Print a summary of the number of identified events
        pandda.log('===================================>>>', True)
        pandda.log('Total Datasets with Events: {!s}'.format(len(event_num)), True)
        pandda.log('Total Events: {!s}'.format(event_total), True)
        
        pandda.log('===================================>>>', True)
        pandda.log('Potentially Useful Shortcuts for Future Runs')
        if event_num:
            pandda.log('no_build={!s}'.format(','.join(zip(*event_num)[0])))

        # ============================================================================>
        #####
        # SUMMARIES ------------------------------>>>
        #####
        # ============================================================================>

        pandda.write_output_csvs()
        write_analyse_html(pandda)

        try:
            from ascii_graph import Pyasciigraph
            g=Pyasciigraph()
            graph_data = [(res, len(resolution_count[res])) for res in sorted(resolution_count)]
            for l in g.graph(label='Datasets Processed at Each Resolution', data=graph_data, sort=0):
                print(l)
        except ImportError:
            print('IMPORT ERROR (ascii_graph) - CANNOT GENERATE MAP ANALYSIS GRAPH')
            pass

        pandda.log('============================================================================>>>', True)
        pandda.log('Datasets Processed: {!s}'.format(sum(map(len,resolution_count.values()))))
        pandda.log('Datasets Loaded {!s}'.format(pandda.datasets.size(mask_name='rejected - total', invert=True)))

    except KeyboardInterrupt:
        raise
    except AssertionError:
        raise
#    except Exception as err:
#        return (pandda, err)
    finally:
        try:
            pandda.update_status('done')
            pandda.exit()
        except:
            print 'PANDDA EXITED WITHOUT CLOSING'

    return pandda, None

def run_custom_analyses(pandda):
    """If a list of hits is available, test to see whether the pandda identified them"""

    try:

        # ============================================================================>
        # ======================================>
        # Manual Analyses - These only need processing once
        # ======================================>
        # ============================================================================>
        analyses_dir = pandda.output_handler.get_dir('analyses')
        # ============================================================================>

        if 0:
            pandda.log('===================================>>>')
            pandda.log('Calculating Deviations of C-alphas between structures')

            rms = lambda vals: numpy.sqrt(numpy.mean(numpy.abs(vals)**2))
            norm = lambda vals: numpy.sqrt(numpy.sum(numpy.abs(vals)**2))

            # Pull all c-alpha sites for each structure
            all_sites = numpy.array([d.transform_points_to_reference(d.get_calpha_sites()) for d in pandda.datasets.all()])
            # Calculate the mean x,y,z for each c-alpha
            mean_sites = numpy.mean(all_sites, axis=0)
            # Differences from the mean
            diff_sites = all_sites - mean_sites
            # Euclidean norms of the distances moved
            diff_norms = numpy.apply_along_axis(norm, axis=2, arr=diff_sites)

            with open(os.path.join(analyses_dir,'calpha_variation.csv'), 'w') as fh:
                for row in diff_norms:
                    out_list = row.round(3).tolist()
                    out_line = ', '.join(map(str,out_list)) + '\n'
                    fh.write(out_line)

            pandda.log('Largest deviation from the mean site: {!s}'.format(diff_norms.max()))
            pandda.log('Average deviation from the mean site: {!s}'.format(diff_norms.mean()))

        # ============================================================================>

        if 0:
            pandda.log('===================================>>>')
            pandda.log('Clustering the Refined Structures')

            distance_matrix = []
            for d1 in pandda.datasets.all():
               distance_matrix.append([d1.transform_points_to_reference(d1.get_calpha_sites()).rms_difference(d2.transform_points_to_reference(d2.get_calpha_sites())) for d2 in pandda.datasets.all()])

            distance_matrix = numpy.array(distance_matrix)

            with open(os.path.join(analyses_dir,'calpha_distance_matrix.csv'), 'w') as fh:
                for row in distance_matrix:
                    out_list = row.round(3).tolist()
                    out_line = ', '.join(map(str,out_list)) + '\n'
                    fh.write(out_line)

        # ============================================================================>

    except:
        pandda.log('FAILURE DURING CUSTOM ANALYSES', True)
        raise

    return 0

# ============================================================================>
#
#   COMMAND LINE RUN
#
# ============================================================================>

if __name__ == '__main__':

    welcome(os.getlogin())

    pandda, err = pandda_main(args=None)

    if err:
        raise err
