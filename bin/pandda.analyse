#!/usr/bin/env pandda.python

import os, sys, glob, time, gc
import numpy, pandas

from scitbx.array_family import flex
from scitbx.math import basic_statistics

from Giant.Xray.Maps.Occupancy import calculate_occupancy_subtracted_map, estimate_event_occupancy

from Giant.Grid.Masks import spherical_mask, atomic_mask, non_symmetrical_atomic_mask
from Giant.Grid.Utils import get_grid_points_within_index_cutoff_of_grid_sites

from Giant.Stats.Cluster import cluster_data
from Giant.Utils import status_bar, rel_symlink

from PANDDAs.Main import PanddaMultiDatasetAnalyser, PanddaMapAnalyser, MapHolder, Meta
from PANDDAs.Events import PointCluster, Event, Site, SiteList, cluster_events 
from PANDDAs import welcome

def pandda_main(args):
    """Run the PANDDA algorithm, using supplied args object"""

    try:

        # ============================================================================>
        #####
        # MANUAL SETTINGS
        #####
        # ============================================================================>
        # None!
        # ============================================================================>
        #####
        # Initialise
        #####
        # ============================================================================>
        pandda = PanddaMultiDatasetAnalyser(args)
        # ============================================================================>
        #####
        # Initialise Settings
        #####
        # ============================================================================>
        pandda.set_low_resolution(pandda.params.analysis.high_res_lower_limit)
        pandda.set_high_resolution(pandda.params.analysis.high_res_upper_limit)
        pandda.run_pandda_init()
        # ============================================================================>
        #####
        # Build list of files in data directories
        #####
        # ============================================================================>
        input_files = pandda.build_input_list()
        # Check that some datasets have been found or already loaded
        if (not pandda.datasets.all()) and (not input_files):
            pandda.exit()
            raise SystemExit('NO DATASETS LOADED')
        # Check to see if we're reusing statistical maps
        if (not pandda.args.method.recalculate_statistical_maps) and pandda.stat_maps.get_resolutions():
            pandda.log('===================================>>>', True)
            pandda.log('REUSING STATISTICAL MAPS AT: {!s}'.format(pandda.stat_maps.get_resolutions()), True)
        # Check that enough datasets have been found
        elif pandda.datasets.size() + len(input_files) < pandda.params.analysis.min_build_datasets:
            pandda.log('===================================>>>', True)
            pandda.log('NOT ENOUGH DATASETS TO BUILD DISTRIBUTIONS!', True)
            pandda.log('Number loaded ({!s}) is less than the {!s} needed.'.format(pandda.datasets.size()+len(input_files), pandda.params.analysis.min_build_datasets), True)
            pandda.log('This value is defined by pandda.params.analysis.min_build_datasets', True)
            raise SystemExit('NOT ENOUGH DATASETS LOADED')
        # If dry_run, exit after initial search
        if pandda.args.settings.exit_flags.dry_run:
            return pandda, None
        # Load and process input files
        if input_files:
            # ============================================================================>
            #####
            # Add new files and load datasets
            #####
            # ============================================================================>
            pandda.add_new_files(input_files)
            pandda.load_new_datasets()
            pandda.initialise_analysis()
            # ============================================================================>
            #####
            # Set Reference Dataset
            #####
            # Select the reference dataset
            # ============================================================================>
            if not pandda.reference_dataset():
                # Select the reference dataset
                ref_pdb, ref_mtz = pandda.select_reference_dataset(method='resolution')
                # Load the reference dataset
                pandda.load_reference_dataset(ref_pdb=ref_pdb, ref_mtz=ref_mtz)
            # ============================================================================>
            #####
            # Scale, Align and Initial-Filter All Data
            #####
            # TODO Revisit Scaling
            # ============================================================================>
            # Filter out datasets with different protein structures
            pandda.filter_datasets_1()
            # Scale and align the datasets to the reference
            pandda.scale_datasets(  ampl_label=pandda.params.maps.ampl_label,
                                    phas_label=pandda.params.maps.phas_label    )
            pandda.align_datasets(  method=pandda.params.alignment.method   )
        else:
            # ============================================================================>
            # Rebuild the masks of the rejected datasets (quick)
            # ============================================================================>
            pandda.initialise_analysis()
            pandda.filter_datasets_1()
        # ============================================================================>
        # Check that enough VALID datasets have been found
        # ============================================================================>
        if (not pandda.args.method.recalculate_statistical_maps) and pandda.stat_maps.get_resolutions():
            # Using existing maps - don't need to check
            pass
        elif pandda.datasets.size(mask_name='rejected - total', invert=True) < pandda.params.analysis.min_build_datasets:
            pandda.log('===================================>>>', True)
            pandda.log('NOT ENOUGH (NON-REJECTED) DATASETS TO BUILD DISTRIBUTIONS!', True)
            pandda.log('Number loaded ({!s}) is less than the {!s} needed.'.format(pandda.datasets.size(mask_name='rejected - total', invert=True), pandda.params.analysis.min_build_datasets), True)
            pandda.log('This value is defined by pandda.params.analysis.min_build_datasets', True)
            raise SystemExit('NOT ENOUGH DATASETS LOADED')
        # ============================================================================>
        # Create neighbouring symmetry copies of the reference structures
        # ============================================================================>
        ref_sym_copies = pandda.generate_reference_symmetry_copies()
        # ============================================================================>
        #####
        # Filter and Analyse the Datasets
        #####
        # ============================================================================>
        # Collate many variables across the datasets to be used for filtering
        pandda.analyse_dataset_variability_1()
        # Filter out the datasets that are not isomorphous and therefore incomparable
        pandda.filter_datasets_2()
        # Analyses the crystallographic and structural variability of the datasets
        pandda.calculate_mean_structure_and_protein_masks(deviation_cutoff=0.5)
        pandda.analyse_dataset_variability_2()
        # Analyse the structural variation in the datasets
#        pandda.analyse_structure_variability_1()
#        pandda.analyse_structure_variability_2()
        # ============================================================================>
        #####
        # Update Settings
        #####
        # ============================================================================>
        # Update the resolution limits using the resolution limits from the datasets supplied
        if pandda.params.analysis.dynamic_res_limits:
            pandda.set_low_resolution(   min(   pandda.get_low_resolution(),
                                                max(pandda.tables.dataset_info['high_resolution']) ) )
            pandda.set_high_resolution(  max(   pandda.get_high_resolution(),
                                                min(pandda.tables.dataset_info['high_resolution']) ) )
            pandda.log('===================================>>>')
            pandda.log('UPDATED RESOLUTION LIMITS')
            pandda.log('LOW RESOLUTION:  {!s}'.format(pandda.get_low_resolution()))
            pandda.log('HIGH RESOLUTION: {!s}'.format(pandda.get_high_resolution()))
        else:
            pandda.log('===================================>>>')
            pandda.log('**NOT** UPDATING RESOLUTION LIMITS')
            pandda.log('LOW RESOLUTION:  {!s}'.format(pandda.get_low_resolution()))
            pandda.log('HIGH RESOLUTION: {!s}'.format(pandda.get_high_resolution()))
        # ============================================================================>
        #####
        # Create Sample Grid
        #####
        # Create reference grid based on the reference structure
        # ============================================================================>
        if pandda.reference_grid() is None:
            # Create parameter for setting grid spacing (multiple grids?)
#            pandda.create_reference_grid(   grid_spacing     = pandda.params.maps.resolution_factor*pandda.get_high_resolution(),
#                                            expand_to_origin = False,
#                                            buffer           = pandda.params.masks.outer_mask+pandda.params.maps.padding    )
            pandda.create_reference_grid(   grid_spacing     = pandda.params.maps.grid_spacing,
                                            expand_to_origin = False,
                                            buffer           = pandda.params.masks.outer_mask+pandda.params.maps.padding    )
        # ============================================================================>
        #####
        # Create Local and Global Masks
        #####
        # ============================================================================>
        # Local mask used for forming groups of points around a grid point
        # ============================================================================>
        if pandda.reference_grid().local_mask() is None:
            pandda.log('===================================>>>')
            pandda.log('Generating Local Mask')
            local_mask = spherical_mask(grid_spacing    = pandda.reference_grid().grid_spacing(),
                                        distance_cutoff = 1.2,
                                        grid_jump       = 1 )
            pandda.reference_grid().set_local_mask(local_mask)
        # ============================================================================>
        # Global mask used for removing points in the bulk solvent regions
        # ============================================================================>
        if pandda.reference_grid().global_mask() is None:
            pandda.log('===================================>>>')
            pandda.log('Generating Protein Mask')
            # Select the masking atoms from the reference structure
            cache = pandda.reference_dataset().hierarchy().atom_selection_cache()
            pro_sites_cart = pandda.reference_dataset().hierarchy().select(cache.selection('pepnames and not element H')).atoms().extract_xyz()
            # Generate the main protein mask
            global_mask = atomic_mask(  cart_sites   = pro_sites_cart,
                                        grid_size    = pandda.reference_grid().grid_size(),
                                        unit_cell    = pandda.reference_grid().unit_cell(),
                                        max_dist     = pandda.params.masks.outer_mask,
                                        min_dist     = pandda.params.masks.inner_mask )
            pandda.reference_grid().set_global_mask(global_mask)
        # ============================================================================>
        # Global mask used for removing points close to symmetry copies of the protein
        # ============================================================================>
        if pandda.reference_grid().symmetry_mask() is None:
            pandda.log('===================================>>>')
            pandda.log('Generating Symmetry Mask')
            # Pull out the cartesian sites of the symmetry mates
            cache = ref_sym_copies.atom_selection_cache()
            sym_sites_cart = ref_sym_copies.select(cache.selection('pepnames and not element H')).atoms().extract_xyz()
            # Generate the symmetry mask
            symmetry_mask = non_symmetrical_atomic_mask(  cart_sites = sym_sites_cart,
                                                          grid_spacing = pandda.reference_grid().grid_spacing(),
                                                          grid_size  = pandda.reference_grid().grid_size(),
                                                          unit_cell  = pandda.reference_grid().unit_cell(),
                                                          max_dist   = pandda.params.masks.outer_mask,
                                                          min_dist   = pandda.params.masks.inner_mask )
            pandda.reference_grid().set_symmetry_mask(symmetry_mask)
        # ============================================================================>
        # Print the summaries
        # ============================================================================>
        pandda.log('===================================>>>', True)
        pandda.log('Grid Summary: ', True)
        pandda.log(pandda.reference_grid().summary(), True)
        pandda.log(pandda.reference_grid().local_mask().summary(), True)
        pandda.log(pandda.reference_grid().global_mask().summary(), True)
        # TODO TODO TODO INCORPORATE THE COMBINATION OF MASKS INTO THIS FUNCTION
        # Create various masks to define regions of the grid by distance to the protein and symmetry copies
        pandda.mask_reference_grid(d_handler=pandda.reference_dataset())
    #    if pandda.reference_grid().masked_grid_points() is None:
    #        pandda.mask_resampled_reference_grid()
    #    # TODO TODO TODO

        # ============================================================================>
        # ============================================================================>
        # ============================================================================>
        # ============================================================================>
        # ============================================================================>
        # TODO TODO TODO - MOVE?
        # Combine the symmetry mask and the atomic mask to create the mask for clustering
        pandda.log('===================================>>>', True)
        pandda.log('Combining Atomic Mask and Symmetry Mask', True)
        grid_idxr = pandda.reference_grid().grid_indexer()
        # Mask EXCLUDING the inner mask (excluding symmetry inner mask)
        combined_total_mask = [gp for gp in pandda.reference_grid().global_mask().total_mask() if pandda.reference_grid().symmetry_mask().inner_mask_binary()[grid_idxr(gp)] == 0]
        # Mask INCLUDING the inner mask (excluding symmetry inner mask)
        combined_outer_mask = [gp for gp in pandda.reference_grid().global_mask().outer_mask() if pandda.reference_grid().symmetry_mask().inner_mask_binary()[grid_idxr(gp)] == 0]

        pandda.log('GLOBAL TOTAL MASK: {!s}'.format(len(pandda.reference_grid().global_mask().total_mask())))
        pandda.log('SYMMETRY INNER MASK: {!s}'.format(len(pandda.reference_grid().symmetry_mask().inner_mask())))
        pandda.log('COMBINED TOTAL MASK: {!s}'.format(len(combined_total_mask)))
        pandda.log('COMBINED OUTER MASK: {!s}'.format(len(combined_outer_mask)))
        # TODO TODO TODO

        # TODO TODO TODO - MOVE?
        # Calculate centre of mass of the reference dataset
        pandda.log('===================================>>>', True)
        pandda.log('Calculating Reference Structure Centre of Mass', True)

        # Should be in the reference frame
        ref_structure = pandda.reference_dataset().hierarchy()
        backbone_atoms = ref_structure.select(ref_structure.atom_selection_cache().selection('pepnames and (name CA or name C or name O or name N)')).atoms()
        backbone_sites = backbone_atoms.extract_xyz()
        backbone_wghts = flex.double([1]*len(backbone_sites))

        centre_of_mass = backbone_sites.mean_weighted(weights=backbone_wghts)
        # TODO TODO TODO
        # ============================================================================>
        # ============================================================================>
        # ============================================================================>
        # ============================================================================>
        # ============================================================================>
        
        # ============================================================================>
        #####
        # STORING FOR REUSE
        #####
        # ============================================================================>
        # Pickle all of the large arrays so they can be reloaded
        # ============================================================================>
        pandda.pickle_the_pandda(all=True)
        pandda.update_pandda_size(tag='After Pre-processing')

        # ============================================================================>
        #####
        # PRE-ANALYSIS ANALYSIS (DUMP OF DATASET PARAMETERS)
        #####
        # ============================================================================>
        pandda.write_output_csvs()
        pandda.write_dataset_summary_graphs()
            
        # ============================================================================>
        #####
        # PREPARE VARIABLES TO LOOP OVER RESOLUTION SHELLS
        #####
        # ============================================================================>
        # Calculate cutoffs for resolution shells
        # ============================================================================>
        if (not pandda.args.method.recalculate_statistical_maps) and pandda.stat_maps.get_resolutions():
            # Use the resolution limits of previously run campaigns
            res_limits = pandda.stat_maps.get_resolutions()
            assert res_limits, 'No Resolution Limits found from statistical maps: {!s}'.format(res_limits)
            min_limit = min(res_limits)
            max_limit = max(res_limits)
        else:
            # Set pandda.args.method.recalculate_statistical_maps to True as no statistical maps have been found
            if not pandda.args.method.recalculate_statistical_maps:
                pandda.log('No Statistical Maps Found: Setting pandda.args.method.recalculate_statistical_maps to True', True)
                pandda.args.method.recalculate_statistical_maps = True
            # ============================================================================>
            # Select resolution limits based on dataset resolutions and given arguments
            # ============================================================================>
            if pandda.params.analysis.dynamic_res_limits:
                # Round the high limit DOWN to the nearest 0.01
                min_limit = round(pandda.get_high_resolution()- 0.005, 2)    # i.e. 1.344 -> 1.34
                # Round the low limit UP to the nearest 0.01
                max_limit = round(pandda.get_low_resolution() + 0.005, 2)    # i.e. 3.423 -> 3.43
            else:
                # Take the arguments as given by the user
                min_limit = pandda.get_high_resolution()
                max_limit = pandda.get_low_resolution()
            # ============================================================================>
            # Create resolution shells (or single limit)
            # ============================================================================>
            if not pandda.params.analysis.high_res_increment:
                # No variable cutoff - select all 
                res_limits = [max_limit]  # i.e. [2]
            else:
                # Calculate a range of resolution limits
                shell_width = pandda.params.analysis.high_res_increment
                res_limits = [round(x, 4) for x in numpy.arange(min_limit, max_limit, shell_width).tolist()]
                # Append the rounded max_limit to the end to ensure that the last dataset is processed            
                if res_limits[-1] != max_limit:
                    res_limits.append(max_limit)
        # ============================================================================>
        # Report
        # ============================================================================>
        pandda.log('===================================>>>', True)
        if len(res_limits)==1:
            pandda.log('Analysing All Maps at {!s}A'.format(max_limit), True)
        else:
            pandda.log('Analysing Resolution Shells from {!s} -> {!s}A'.format(min_limit, max_limit), True)
            pandda.log('Limits: {!s}'.format(', '.join(map(str,res_limits))), True)
        # ============================================================================>
        # Initialise for iterations over shells
        # ============================================================================>
        # Analyse all datasets from high_shell_limit -> cut_resolution (initialise high_shell_limit to 0)
        high_shell_limit=0
        # Record how many datasets are processed at each resolution
        resolution_count={}
        # Record the amount of time spent analysing the maps
        t_analysis_start = time.time()
        pandda.log('===================================>>>', True)
        pandda.log('Map Analysis Started: {!s}'.format(time.strftime("%a, %d %b %Y %H:%M:%S", time.gmtime(t_analysis_start))), True)

        # ============================================================================>
        #####
        # ANALYSE DATASETS - ITERATE THROUGH RESOLUTION SHELLS
        #####
        # ============================================================================>
        for cut_resolution in res_limits:

            # Which resolutions will be processed in this shell
            pandda.log('===================================>>>', True)
            pandda.log('Looking for Datasets to Process from {!s}A -> {!s}A'.format(high_shell_limit, cut_resolution), True)

            # ============================================================================>
            # Select datasets to build the distributions
            # ============================================================================>
            building_mask_name = 'selected for building @ {!s}A'.format(cut_resolution)
            if pandda.args.method.recalculate_statistical_maps:
                pandda.log('===================================>>>', True)
                pandda.log('Selecting Building Mask', True)
                building_mask = pandda.select_for_building_distributions(high_res_cutoff = cut_resolution,
                                                                         building_mask_name = building_mask_name)
                # Check that we have enough datasets to build distributions
                if sum(building_mask) < pandda.params.analysis.min_build_datasets:
                    # Don't have enough to construct robust distributions
                    pandda.log('NOT ENOUGH DATASETS TO CALCULATE DISTRIBUTIONS ({!s}<{!s})'.format(sum(building_mask),pandda.params.analysis.min_build_datasets), True)
                    continue
                else:
                    # Have enough to generate robust distributions
                    pandda.log('ENOUGH DATASETS -> PROCESSING THIS RESOLUTION', True)
                    pandda.log('Building Distributions using {!s} Datasets'.format(sum(building_mask)), True)
            else:
                pandda.log('===================================>>>', True)
                pandda.log('**NOT** Selecting Building Mask (Using Existing Statistical Maps)', True)
                # Create a dummy mask as we won't be using any datasets for building
                building_mask = [False]*pandda.datasets.size()
                pandda.datasets.all_masks().add_mask(mask_name=building_mask_name, mask=building_mask)

            # ============================================================================>
            # Select the datasets to analyse
            # ============================================================================>
            analysis_mask_name = 'selected for analysis @ {!s}A'.format(cut_resolution)
            pandda.log('===================================>>>', True)
            pandda.log('Selecting Analysis Mask', True)
            analysis_mask = pandda.select_for_analysis(high_res_large_cutoff = cut_resolution, 
                                                       high_res_small_cutoff = high_shell_limit, 
                                                       analysis_mask_name = analysis_mask_name)
            # Check that there're some datasets to analyse
            if sum(analysis_mask) == 0:
                pandda.log('NO DATASETS TO ANALYSE @ {!s}'.format(cut_resolution))
                continue
            else:
                pandda.log('Calculating Z-Maps for {!s} Datasets'.format(sum(analysis_mask)), True)

            # ============================================================================>
            # Combine the masks as we will need to load maps for all datasets
            # ============================================================================>
            pandda.log('===================================>>>', True)
            pandda.log('Combining (Analysis and Building) Masks', True)
            map_load_mask = pandda.datasets.all_masks().combine_masks([analysis_mask_name, building_mask_name])
            map_load_mask_name = 'selected for loading maps @ {!s}A'.format(cut_resolution)
            pandda.datasets.all_masks().add_mask(mask_name=map_load_mask_name, mask=map_load_mask)

            # ============================================================================>
            # Report
            # ============================================================================>
            pandda.log('===================================>>>')
            pandda.log('Mask Names for Building, Loading, and Analysis')
            pandda.log('Building ({!s} datasets): {!s}'.format(sum(building_mask), building_mask_name))
            pandda.log('Load Map ({!s} datasets): {!s}'.format(sum(map_load_mask), map_load_mask_name))
            pandda.log('Analysis ({!s} datasets): {!s}'.format(sum(analysis_mask), analysis_mask_name))
            pandda.log('===================================>>>', True)
            pandda.log('Loading Maps for {!s} Datasets at {!s}A'.format(pandda.datasets.size(mask_name=map_load_mask_name), cut_resolution), True)

            # ============================================================================>
            # Update limit (cut resolution becomes top limit in next shell)
            # ============================================================================>
            high_shell_limit = cut_resolution

            # ============================================================================>
            #####
            # Truncate data, load and scale maps to reference dataset
            #####
            # ============================================================================>
            # Truncate the data to a particular resolution
            # ============================================================================>
            pandda.truncate_scaled_data(dataset_handlers = pandda.datasets.mask(mask_name=map_load_mask_name))
            # ============================================================================>
            # Load the reference map so that we can scale the individual maps to this
            # ============================================================================>
            ref_map_holder = pandda.load_reference_map( map_resolution = cut_resolution )
            # ============================================================================>
            # Load the required maps
            # ============================================================================>
            map_holder_list = pandda.load_and_morph_maps( dataset_handlers = pandda.datasets.mask(mask_name=map_load_mask_name),
                                                          ref_map_holder   = ref_map_holder,
                                                          map_resolution   = cut_resolution     )
            # ============================================================================>
            # Extract the statistical maps at this resolution (if requested)
            # ============================================================================>
            if pandda.args.method.recalculate_statistical_maps:
                # No Statistical maps - set to None so that they are created
                statistical_maps = None
            else:
                # Try to find a statistical map at this resolution
                statistical_maps = pandda.stat_maps.get(cut_resolution)
            # ============================================================================>
            # Create an object to hold all of the maps, and can be used to calculate the mean maps, etc...
            # ============================================================================>
            map_analyser = PanddaMapAnalyser(   dataset_maps     = map_holder_list,
                                                meta             = Meta({'resolution'   : cut_resolution,
                                                                         'grid_size'    : pandda.reference_grid().grid_size(),
                                                                         'grid_size_1d' : pandda.reference_grid().grid_size_1d()}),
                                                statistical_maps = statistical_maps,
                                                parent           = pandda )
            # ============================================================================>
            # Add the analysis mask to the map analyser        
            # ============================================================================>
            map_analyser.dataset_maps.all_masks().add_mask(mask_name=analysis_mask_name, mask=[False]*map_analyser.dataset_maps.size())
            for dh in pandda.datasets.mask(mask_name=analysis_mask_name):
                map_analyser.dataset_maps.all_masks().set_mask_value(mask_name=analysis_mask_name, entry_id=dh.tag, value=True)
            # ============================================================================>
            # Add the building mask to the map analyser
            # ============================================================================>
            map_analyser.dataset_maps.all_masks().add_mask(mask_name=building_mask_name, mask=[False]*map_analyser.dataset_maps.size())
            for dh in pandda.datasets.mask(mask_name=building_mask_name):
                map_analyser.dataset_maps.all_masks().set_mask_value(mask_name=building_mask_name, entry_id=dh.tag, value=True)

            # ============================================================================>
            #####
            # Calculate Statistical Maps (if required) and Calculate Dataset Uncertainties
            #####
            # ============================================================================>
            if pandda.args.method.recalculate_statistical_maps:
                pandda.log('===================================>>>', True)
                pandda.log('Building Map Distributions for {!s} Datasets at {!s}A'.format(pandda.datasets.size(mask_name=building_mask_name), cut_resolution), True)
            else:
                pandda.log('===================================>>>', True)
                pandda.log('Using Existing Map Distributions at {!s}A'.format(cut_resolution), True)
                assert pandda.datasets.size(mask_name=building_mask_name) == 0, 'BUILDING MASKS HAVE BEEN SELECTED WHEN MAPS ALREAD EXIST'
                assert map_analyser.dataset_maps.size(mask_name=building_mask_name) == 0, 'BUILDING MASKS HAVE BEEN SELECTED WHEN MAPS ALREAD EXIST'
            # ============================================================================>
            # Calculate the mean map
            # ============================================================================>
            if (not map_analyser.statistical_maps.mean_map) or pandda.args.method.recalculate_statistical_maps:
                mean_map = map_analyser.calculate_mean_map(masked_idxs=pandda.reference_grid().global_mask().outer_mask_indices(), mask_name=building_mask_name)
            # ============================================================================>
            # If only Mean Map Requested -- return
            # ============================================================================>
            if pandda.args.settings.exit_flags.calculate_first_mean_map_only:
                pandda.write_array_to_map(output_file = pandda.output_handler.get_file('mean_map').format(cut_resolution),
                                                        map_data    = map_analyser.statistical_maps.mean_map     )
                return pandda, None
            # ============================================================================>
            # Calculate the uncertainty of all loaded maps (needs the mean map to have been calculated)
            # ============================================================================>
            # If not pandda.args.method.recalculate_statistical_maps, then no building_mask datasets will have been added, so don't need to be selective in this function
            map_uncties = map_analyser.calculate_map_uncertainties(masked_idxs=pandda.reference_grid().global_mask().inner_mask_indices())
            # ============================================================================>
            # Plot uncertainties of maps
            # ============================================================================>
            try:
                from ascii_graph import Pyasciigraph
                g=Pyasciigraph()
                graph_data = [(mh.tag, round(mh.meta.map_uncertainty,3)) for mh in map_analyser.dataset_maps.all()]
                for l in g.graph(label='Sorted Map Uncertainties (Ascending Order)', data=graph_data, sort=1):
                    print(l)
            except ImportError:
                print('IMPORT ERROR (ascii_graph) - CANNOT GENERATE UNCERTAINTY GRAPH')
                pass
            # ============================================================================>
            # Calculate the statistics of the maps
            # ============================================================================>
            if (not map_analyser.statistical_maps.sadj_map) or pandda.args.method.recalculate_statistical_maps:
                map_analyser.calculate_statistical_maps(    masked_idxs=pandda.reference_grid().global_mask().outer_mask_indices(),
                                                            mask_name=building_mask_name,
                                                            cpus=pandda.args.settings.cpus)

            # ============================================================================>
            # PICKLE THE DATASETS THAT HAVE JUST BEEN PROCESSED
            # ============================================================================>
            pandda.pickle_the_pandda(components=['datasets'], datasets=pandda.datasets.mask(mask_name=analysis_mask_name))
            pandda.update_pandda_size(tag='After Analysing Maps @ {!s}'.format(cut_resolution))
            # ============================================================================>
            # Extract and store the statistical map objects
            # ============================================================================>
            # Only need to add if we're calculating new statistical maps
            if pandda.args.method.recalculate_statistical_maps:
                # Check to see if already in there
                if cut_resolution in pandda.stat_maps.get_resolutions():
                    pandda.log('Overwriting existing statistical maps @ {!s}A'.format(cut_resolution))
                pandda.stat_maps.add(stat_map_list=map_analyser.statistical_maps, resolution=cut_resolution)
                pandda.pickle_the_pandda(components=['stat_maps'])
            # ============================================================================>
            # Write out Grid Point Distributions for interesting grid points (high modality, etc...)
            # ============================================================================>
            # TODO TODO TODO
            # TODO TODO TODO
            # ============================================================================>
            # Write Grid Point Distributions - Standard Function, just to provide an output
            # ============================================================================>
            try:
                from libtbx.math_utils import iceil
                grid_size = pandda.reference_grid().grid_size()
                num_points = min(10, min(grid_size))
                grid_points = zip(*[range(0, s, iceil(s/num_points)) for s in grid_size])
                pandda.write_grid_point_distributions(  grid_points     = grid_points,
                                                        map_analyser    = map_analyser,
                                                        output_filename = None          )
            except:
                print('UNIMPORTANT: FAILED TO WRITE AUTOMATIC DISTRIBUTION OF GRID POINTS')
                raise
            # ============================================================================>
            #####
            # Blob Search - Manual Settings
            #####
            # ============================================================================>
            # Minimum size of cluster
            min_cluster_volume   = pandda.params.blob_search.min_blob_volume
            min_cluster_z_peak   = pandda.params.blob_search.min_blob_z_peak
            # Z cutoff for maps
            z_cutoff             = pandda.params.blob_search.contour_level
            # Clustering Methods
            clustering_criterion = pandda.params.blob_search.clustering.criterion
            clustering_metric    = pandda.params.blob_search.clustering.metric
            clustering_method    = pandda.params.blob_search.clustering.linkage
            # Cutoff for separation of clusters (sqrt((2x)**2 + (2y)**2 + (2z)**2)) -- allows diagonal grid points to connect
            clustering_cutoff = 1.1 * numpy.math.sqrt(3) * pandda.reference_grid().grid_spacing()
            # ============================================================================>
            #####
            # DATA PROCESSING
            #####
            # Calculate the moments of the distributions at the grid points
            # Use the means and the stds to convert the maps to z-maps
            # Use the local mask to look for groups of significant z-values
            # ============================================================================>
            # XXX Change this to a separate object XXX
            pandda.print_clustering_settings(   z_cutoff             = z_cutoff,
                                                min_cluster_volume   = min_cluster_volume,
                                                min_cluster_z_peak   = min_cluster_z_peak,
                                                clustering_cutoff    = clustering_cutoff,
                                                clustering_criterion = clustering_criterion,
                                                clustering_metric    = clustering_metric,
                                                clustering_method    = clustering_method  )

            assert cut_resolution not in resolution_count
            resolution_count[cut_resolution] = []

            # ============================================================================>
            #####
            # Calculate Z-Maps
            #####
            # ============================================================================>
            # Time the processing of the dataset maps
            t_start = time.time()
            pandda.log('===================================>>>', True)
            pandda.log('Calculating Z-Maps for {!s} Datasets at {!s}A'.format(pandda.datasets.size(mask_name=analysis_mask_name), cut_resolution), True)

            # Iterate through and calculate z-maps
            for i_dh, d_handler in enumerate(pandda.datasets.mask(mask_name=analysis_mask_name)):

                # Record the which resolution this dataset was analysed at
                resolution_count[cut_resolution].append(d_handler.tag)
                    
                # Create a file that says what resolution this was processed at
                link_dir = os.path.join(pandda.output_handler.get_dir('resolutions'), '{!s}-{!s}'.format(cut_resolution, d_handler.name))
                if not os.path.exists(link_dir):
                    rel_symlink(orig=d_handler.output_handler.get_dir('root'), link=link_dir)

                # Flag that this dataset is being analysed 
                pandda.datasets.all_masks().set_mask_value(mask_name='analysed', entry_id=d_handler.tag, value=True)
                
                # Extract the map for this dataset    
                m_handler = map_analyser.dataset_maps.get(tag=d_handler.tag)
                
                ################################################
                # AUTOGENERATE SCRIPTS FOR VIEWING THE DATASET #
                ################################################

                pandda.write_pymol_scripts(d_handler=d_handler)
                
                #############################################
                # CALCULATE Z-MAPS AND LOOK FOR LARGE BLOBS #
                #############################################

                pandda.log('===================================>>>', True)
                pandda.log('Calculating Z-MAPs for Dataset {!s} ({!s}/{!s})'.format(d_handler.tag, i_dh+1, pandda.datasets.size(mask_name=analysis_mask_name)), True)

                ##################################
                # EXTRACT MAP VALUES             #
                ##################################
                assert m_handler.map is not None, 'NO MAP FOUND'

                ##################################
                # CALCULATE MEAN-DIFF MAPS       #
                ##################################
                d_map = m_handler.map - map_analyser.statistical_maps.mean_map

                ###################################################################
                # NAIVE Z-MAP - NOT USING UNCERTAINTY ESTIMATION OR ADJUSTED STDS #
                ###################################################################
                z_map_naive = map_analyser.calculate_z_map(tag=d_handler.tag, method='naive')
                # Normalise this map to N(0,1)
                z_map_naive_masked = [z_map_naive[i] for i in pandda.reference_grid().global_mask().outer_mask_indices()]
                z_map_naive_normalised = (z_map_naive - numpy.mean(z_map_naive_masked)) / numpy.std(z_map_naive_masked)

                ##################################
                # ADJUSTED+UNCERTAINTY Z-MAP     #
                ##################################
                z_map_corrected = map_analyser.calculate_z_map(tag=d_handler.tag, method='adjusted+uncertainty')
                # Normalise this map to N(0,1)
                z_map_corrected_masked = [z_map_corrected[i] for i in pandda.reference_grid().global_mask().outer_mask_indices()]
                z_map_corrected_normalised = (z_map_corrected - numpy.mean(z_map_corrected_masked)) / numpy.std(z_map_corrected_masked)

                # ============================================================================>
                # ANALYSE Z-MAP FOR STATISTICAL VALIDITY
                # ============================================================================>
                # Calculate statistics of z-maps
                z_map_stats = basic_statistics(flex.double([z_map_corrected[i] for i in pandda.reference_grid().global_mask().outer_mask_indices()]))
                
                # ============================================================================>
                # STORE ANALYSIS DATA IN DATASET MAP TABLE
                # ============================================================================>
                # Add to the dataset map summary table
                pandda.tables.dataset_map_info.set_value(d_handler.tag, 'analysed_resolution', m_handler.meta.resolution)
                pandda.tables.dataset_map_info.set_value(d_handler.tag, 'map_uncertainty',     m_handler.meta.map_uncertainty)
                pandda.tables.dataset_map_info.set_value(d_handler.tag, 'obs_map_mean',        m_handler.meta.obs_map_mean)
                pandda.tables.dataset_map_info.set_value(d_handler.tag, 'obs_map_rms',         m_handler.meta.obs_map_rms)
                pandda.tables.dataset_map_info.set_value(d_handler.tag, 'z_map_mean',          z_map_stats.mean)
                pandda.tables.dataset_map_info.set_value(d_handler.tag, 'z_map_std',           z_map_stats.bias_corrected_standard_deviation)
                pandda.tables.dataset_map_info.set_value(d_handler.tag, 'z_map_skew',          z_map_stats.skew)
                pandda.tables.dataset_map_info.set_value(d_handler.tag, 'z_map_kurt',          z_map_stats.kurtosis)
                
                # ============================================================================>
                # WRITE OUT DATASET INFORMATION TO CSV FILE
                # ============================================================================>
                out_list = pandda.tables.dataset_info.loc[d_handler.tag].append(pandda.tables.dataset_map_info.loc[d_handler.tag])
                out_list.to_csv(   path = d_handler.output_handler.get_file('dataset_info'),
                                   header      = True,
                                   index_label = 'dtag'    )

                # ============================================================================>
                # XXX  WHICH MAP TO DO THE BLOB SEARCHING ON  XXX
                # ============================================================================>
                z_map = z_map_corrected_normalised

                # ============================================================================>
                # STORE THE Z MAP        
                # ============================================================================>
                z_map_holder = MapHolder(   num         = m_handler.num,
                                            tag         = m_handler.tag,
                                            map         = z_map,
                                            # Change these for the 'fake' grid unit_cell and a P1 space_group
                                            unit_cell   = pandda.reference_grid().unit_cell(),
                                            space_group = pandda.reference_grid().space_group(),
                                            meta        = Meta({'type'          : 'z-map',
                                                                'resolution'    : m_handler.meta.resolution}),
                                            parent      = m_handler  )
                # Link the sampled map handler to the z map holder
                m_handler.child = z_map_holder
                # Link the dataset handler to the child map holder
                d_handler.child = m_handler

                # ============================================================================>
                #####
                # LOOK FOR CLUSTERS OF Z-SCORES  
                #####
                # ============================================================================>

                num_clusters, raw_hit_clusters = pandda.cluster_high_z_values(
                                                            d_handler            = d_handler,
                                                            z_map                = z_map,
                                                            z_cutoff             = z_cutoff,
#                                                                point_mask           = combined_outer_mask,
                                                            point_mask           = combined_total_mask,
                                                            clustering_cutoff    = clustering_cutoff,
                                                            clustering_criterion = clustering_criterion,
                                                            clustering_metric    = clustering_metric,
                                                            clustering_method    = clustering_method
                                                         )
                
                # ============================================================================>
                #####
                # WRITE ALL MAP DISTRIBUTIONS (THESE DON'T USE MUCH SPACE)
                #####
                # ============================================================================>
                
                # Sampled Map
                if not os.path.exists(d_handler.output_handler.get_file('s_map_png')):
                    pandda.write_map_value_distribution(map_vals     = m_handler.map.select(pandda.reference_grid().global_mask().outer_mask_indices()),
                                                        output_file  = d_handler.output_handler.get_file('s_map_png'))
                # Mean-Difference
                if not os.path.exists(d_handler.output_handler.get_file('d_mean_map_png')):
                    pandda.write_map_value_distribution(map_vals     = d_map.select(pandda.reference_grid().global_mask().outer_mask_indices()),
                                                        output_file  = d_handler.output_handler.get_file('d_mean_map_png'))
                # Naive Z-Map
                if not os.path.exists(d_handler.output_handler.get_file('z_map_naive_png')):
                    pandda.write_map_value_distribution(map_vals     = z_map_naive.select(pandda.reference_grid().global_mask().outer_mask_indices()),
                                                        output_file  = d_handler.output_handler.get_file('z_map_naive_png'),
                                                        plot_normal  = True)
                # Normalised Naive Z-Map
                if not os.path.exists(d_handler.output_handler.get_file('z_map_naive_normalised_png')):
                    pandda.write_map_value_distribution(map_vals     = z_map_naive_normalised.select(pandda.reference_grid().global_mask().outer_mask_indices()),
                                                        output_file  = d_handler.output_handler.get_file('z_map_naive_normalised_png'),
                                                        plot_normal  = True)
                # Correct Z-Map
                if not os.path.exists(d_handler.output_handler.get_file('z_map_corrected_png')):
                    pandda.write_map_value_distribution(map_vals     = z_map_corrected.select(pandda.reference_grid().global_mask().outer_mask_indices()),
                                                        output_file  = d_handler.output_handler.get_file('z_map_corrected_png'),
                                                        plot_normal  = True)
                # Normalised Corrected Z-Map
                if not os.path.exists(d_handler.output_handler.get_file('z_map_corrected_normalised_png')):
                    pandda.write_map_value_distribution(map_vals     = z_map_corrected_normalised.select(pandda.reference_grid().global_mask().outer_mask_indices()),
                                                        output_file  = d_handler.output_handler.get_file('z_map_corrected_normalised_png'),
                                                        plot_normal  = True)
                # Plot Q-Q Plot of Corrected Z-Map to see how normal it is
                if not os.path.exists(d_handler.output_handler.get_file('z_map_qq_plot_png')):
                    pandda.write_qq_plot_against_normal(map_vals     = z_map_corrected_normalised.select(pandda.reference_grid().global_mask().outer_mask_indices()),
                                                        output_file  = d_handler.output_handler.get_file('z_map_qq_plot_png'))
        
                
                # ============================================================================>
                #####
                # WRITE MAPS (ONLY IF CLUSTERS FOUND)
                #####
                # ============================================================================>
                
                if (num_clusters != 0) or pandda.args.settings.developer.write_maps_for_empty_datasets:
                    
                    # Write the NATIVE map
                    if not os.path.exists(d_handler.output_handler.get_file('native_map')):
                        pandda.write_array_to_map(  output_file = d_handler.output_handler.get_file('native_map'),
                                                    map_data    = pandda.rotate_map(d_handler=d_handler, map_data=m_handler.map))
                
                    # Write the sampled map
                    if not os.path.exists(d_handler.output_handler.get_file('sampled_map')):
                        pandda.write_array_to_map(  output_file = d_handler.output_handler.get_file('sampled_map'),
                                                    map_data    = m_handler.map)
                    # Write the mean-difference map
                    if not os.path.exists(d_handler.output_handler.get_file('mean_diff_map')):
                        pandda.write_array_to_map(  output_file = d_handler.output_handler.get_file('mean_diff_map'),
                                                    map_data    = d_map)
                    # Write Normalised Corrected Z-Map (ALWAYS WRITE THIS MAP)
                    if not os.path.exists(d_handler.output_handler.get_file('z_map_corrected_normalised')):
                        pandda.write_array_to_map(  output_file = d_handler.output_handler.get_file('z_map_corrected_normalised'),
                                                    map_data    = z_map_corrected_normalised)
                    
                    # Write different Z-Maps? (Probably only needed for testing)
                    if pandda.args.settings.developer.write_all_z_map_types:
                
                        # Write Naive Z-Map
                        if not os.path.exists(d_handler.output_handler.get_file('z_map_naive')):
                            pandda.write_array_to_map(  output_file = d_handler.output_handler.get_file('z_map_naive'),
                                                        map_data    = z_map_naive)
                        # Write Normalised Naive Z-Map
                        if not os.path.exists(d_handler.output_handler.get_file('z_map_naive_normalised')):
                            pandda.write_array_to_map(  output_file = d_handler.output_handler.get_file('z_map_naive_normalised'),
                                                        map_data    = z_map_naive_normalised)
                        # Write Corrected Z-Map
                        if not os.path.exists(d_handler.output_handler.get_file('z_map_corrected')):
                            pandda.write_array_to_map(  output_file = d_handler.output_handler.get_file('z_map_corrected'),
                                                        map_data    = z_map_corrected)

                # ============================================================================>
                #####
                # SKIP TO NEXT DATASET IF NO CLUSTERS FOUND
                #####
                # ============================================================================>
                
                if num_clusters == 0:
                    # Nothing more to do with this dataset at the moment
                    continue
                elif num_clusters == -1:
                    # This dataset is too noisy to analyse - flag!
                    pandda.datasets.all_masks().set_mask_value(mask_name='noisy zmap', entry_id=d_handler.tag, value=True)
                    # Link datasets to the noisy results directory
                    noisy_dir = os.path.join(pandda.output_handler.get_dir('noisy_datasets'), d_handler.name)
                    if not os.path.exists(noisy_dir):
                        rel_symlink(orig=d_handler.output_handler.get_dir('root'), link=noisy_dir)
                    # Nothing more to do with this dataset at the moment
                    continue
                
                # ============================================================================>
                #####
                # FILTER CLUSTERS OF Z-SCORES  
                #####
                # ============================================================================>

                # Filter the clusters by size and peak height
                num_clusters, filtered_hit_clusters = pandda.filter_z_clusters_1(
                                                            hit_clusters       = raw_hit_clusters,
                                                            min_cluster_volume = min_cluster_volume,
                                                            min_cluster_z_peak = min_cluster_z_peak     )

                if num_clusters == 0:
                    pandda.log('===> No Clusters found - Minimum cluster peak/size not reached.', True)
                    # Nothing more to do with this dataset at the moment
                    continue

                # ============================================================================>
                # Filter the clusters by distance from protein
                # ============================================================================>
                num_clusters, filtered_hit_clusters = pandda.filter_z_clusters_2(
                                                            hit_clusters     = filtered_hit_clusters,
                                                            grid_spacing     = pandda.reference_grid().grid_spacing(), 
                                                            grid_origin_cart = pandda.reference_dataset().origin_shift(),
                                                            ref_structure    = pandda.reference_dataset().new_structure().hierarchy     )

                if num_clusters == 0:
                    pandda.log('===> No Clusters found - Clusters too far from protein.', True)
                    # Nothing more to do with this dataset at the moment
                    continue
                
                # ============================================================================>
                # GROUP NEARBY CLUSTERS TOGETHER
                # ============================================================================>
                num_clusters, filtered_hit_clusters = pandda.group_clusters( hit_clusters = filtered_hit_clusters,
                                                                             grid_spacing = pandda.reference_grid().grid_spacing()   )

                # ============================================================================>
                # Filter the clusters by symmetry equivalence
                # ============================================================================>
                num_clusters, filtered_hit_clusters = pandda.filter_z_clusters_3(
                                                            hit_clusters     = filtered_hit_clusters,
                                                            grid_spacing     = pandda.reference_grid().grid_spacing(), 
                                                            grid_origin_cart = pandda.reference_dataset().origin_shift(),
                                                            ref_unit_cell    = pandda.reference_dataset().unit_cell,
                                                            ref_sym_ops      = pandda.crystal_contact_generators,
                                                            ref_structure    = pandda.reference_dataset().new_structure().hierarchy     )
                
                pandda.log('===> {!s} Cluster(s) found.'.format(num_clusters), True)
                assert num_clusters > 0, 'NUMBER OF CLUSTERS AFTER FILTERING == 0!'
                
                # Create a link to the interesting directories in the initial results directory
                hit_dir = os.path.join(pandda.output_handler.get_dir('interesting_datasets'), d_handler.name)
                if not os.path.exists(hit_dir):
                    rel_symlink(orig=d_handler.output_handler.get_dir('root'), link=hit_dir)
                   
                # Flag the dataset as interesting 
                pandda.datasets.all_masks().set_mask_value(mask_name='interesting', entry_id=d_handler.tag, value=True)

                # ============================================================================>
                # PRINT MAP OF THE CLUSTERS  #
                # ============================================================================>
                highz_points = []; [highz_points.extend(zip(*v)[0]) for v in filtered_hit_clusters.values()]
                highz_map_array = numpy.zeros(pandda.reference_grid().grid_size_1d(), dtype=int)
                highz_map_array.put(map(pandda.reference_grid().grid_indexer(), highz_points), [1]*len(highz_points))
                map_mask = flex.double(highz_map_array.tolist()); map_mask.reshape(flex.grid(pandda.reference_grid().grid_size()))
                pandda.write_array_to_map(output_file=d_handler.output_handler.get_file('high_z_mask'), map_data=map_mask)
                
                # ============================================================================>
                # Process the identified features
                # ============================================================================>
                for event_idx, event_point_value_list in filtered_hit_clusters.items():
                    # Create a unique identifier for this event
                    event_key = (d_handler.tag, event_idx)
                    # Split the tuples into lists
                    points, values = zip(*event_point_value_list)
                    # ============================================================================>
                    # Create a point cluster object
                    # ============================================================================>
                    point_cluster = PointCluster(id=event_key, points=points, values=values)
                    # ============================================================================>
                    # Estimate the occupancy of the detected feature 
                    # ============================================================================>
                    # Extract sites for this cluster and estimate the occupancy of the event
                    pandda.log('===================================>>>', True)
                    # Expand the sites to include the surrounding region (Only take 2nd return arguement of 'max' sites)
                    expanded_points = get_grid_points_within_index_cutoff_of_grid_sites(point_cluster.points, max_grid_dist=3)[1]
                    pandda.log('=> Event sites ({!s} points) expanded to {!s} points'.format(len(points), len(expanded_points)))
                    # Use the outer mask to provide a reference point for the correlation to the mean map
                    reference_points = pandda.reference_grid().global_mask().outer_mask()
                    # Calculate the correlation with the mean map as different amounts of the mean map are subtracted
                    event_occ_est = estimate_event_occupancy( 
                        ref_map          = map_analyser.statistical_maps.mean_map,
                        query_map        = m_handler.map,
                        feature_region   = expanded_points,
                        reference_region = reference_points,
                        min_occ          = pandda.params.occupancy_estimation.min_occ,
                        max_occ          = pandda.params.occupancy_estimation.max_occ,
                        occ_increment    = pandda.params.occupancy_estimation.occ_increment,
                        method           = 'value',
                        verbose          = pandda.args.settings.verbose)
                    pandda.log('=> Event occupancy estimated as {!s}'.format(event_occ_est), True)
                    # ============================================================================>
                    # Calculate occupancy map at this occupancy
                    # ============================================================================>
                    event_occ_map = calculate_occupancy_subtracted_map(
                        ref_map      = map_analyser.statistical_maps.mean_map, 
                        query_map    = m_handler.map,
                        subtract_occ = 1.0 - event_occ_est)
                    # Normalise the event map
                    event_occ_map = event_occ_map / event_occ_est
                    # Write out this array
                    pandda.write_array_to_map(  output_file = d_handler.output_handler.get_file('occupancy_map').format(event_idx, event_occ_est), 
                                                map_data    = event_occ_map )
                    # ============================================================================>
                    # Create an event object
                    # ============================================================================>
                    event_obj = Event(id=point_cluster.id, cluster=point_cluster)
                    event_obj.info.estimated_occupancy = event_occ_est
                    # ============================================================================>
                    # Append to dataset handler
                    # ============================================================================>
                    d_handler.events.append(event_obj)
                    # ============================================================================>
                    # Add event to the event table
                    # ============================================================================>
                    pandda.add_event_to_event_table(d_handler=d_handler, event=event_obj)

                    # ============================================================================>
                    # Write out the graph of the occupancy correlations
                    # ============================================================================>
#                    if pandda.args.output.plot_graphs:
#                        import matplotlib
#                        matplotlib.interactive(0)
#                        from matplotlib import pyplot
#                        # 2 subplots sharing x-axis
#                        fig, (axis_1_1, axis_2_1) = pyplot.subplots(2, sharex=True)
#                        # 1st Plot - 1st Y-Axis
#                        line_1_1, = axis_1_1.plot(occ_est_data.occ_values, occ_est_data.global_corr_vals, 'g--', label='GLOBAL')
#                        line_1_2, = axis_1_1.plot(occ_est_data.occ_values, occ_est_data.local_corr_vals, 'k--', label='EVENT')
#                        axis_1_1.set_xlabel('Event Occupancy')
#                        axis_1_1.set_ylabel('Correlation to Mean Map', color='k')
#                        axis_1_1.set_ylim((-1, 1))
#                        # 1st Plot - 2nd Y-Axis
#                        axis_1_2 = axis_1_1.twinx()
#                        line_1_3, = axis_1_2.plot(occ_est_data.occ_values, occ_est_data.corr_vals_diffs, 'b-', label='DISCREP')
#                        axis_1_2.set_ylabel('Correlation Differences', color='b')
#                        # Plot line at the maximum
#                        line_1_4, = axis_1_2.plot([occ_est_data.feature_occ_est,occ_est_data.feature_occ_est],[-1,1], 'k-')
#                        text_1_1 = axis_1_2.text(0.02+occ_est_data.feature_occ_est, 0.0, str(occ_est_data.feature_occ_est))
#                        # Joint legend
#                        axis_1_1.legend(handles=[line_1_1, line_1_2, line_1_3], loc=4)
#                        # Title
#                        axis_1_1.set_title('Estimating Occupancy by Correlation Value Differences')
#                        # 2nd Plot - 1st Y-Axis
#                        line_2_1, = axis_2_1.plot(occ_est_data.occ_values, occ_est_data.global_corr_grads, 'g--', label='GLOBAL')
#                        line_2_2, = axis_2_1.plot(occ_est_data.occ_values, occ_est_data.local_corr_grads, 'k--', label='EVENT')
#                        axis_2_1.set_xlabel('Event Occupancy')
#                        axis_2_1.set_ylabel('Correlation to Mean Map', color='k')
#                        axis_2_1.set_ylim((-1, 1))
#                        # 1st Plot - 2nd Y-Axis
#                        axis_2_2 = axis_2_1.twinx()
#                        line_2_3, = axis_2_2.plot(occ_est_data.occ_values, occ_est_data.corr_vals_diffs, 'b-', label='DISCREP')
#                        axis_2_2.set_ylabel('Correlation Differences', color='b')
#                        # Plot line at the maximum
#                        line_2_4, = axis_2_2.plot([occ_est_data.feature_occ_est,occ_est_data.feature_occ_est],[-1,1], 'k-')
#                        text_2_1 = axis_2_2.text(0.02+occ_est_data.feature_occ_est, 0.0, str(occ_est_data.feature_occ_est))
#                        # Joint legend
#                        axis_2_1.legend(handles=[line_2_1, line_2_2, line_2_3], loc=4)
#                        # Title
#                        axis_2_1.set_title('Estimating Occupancy by Correlation Gradient Differences')
#                        # Remove spacing between subplots
##                        fig.subplots_adjust(hspace=0)
#                        pyplot.setp([a.get_xticklabels() for a in fig.axes[:-1]], visible=False)
#                        pyplot.setp([axis_2_1.get_xticklabels()], visible=True)
#                        # Apply tight layout to prevent overlaps
#                        #pyplot.tight_layout()
#                        # Save
#                        pyplot.savefig(d_handler.output_handler.get_file('occ_corr_png').format(event_key))
#                        pyplot.close(fig)

# XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX
# TODO REIMPLEMENT THIS IF IT CAN BE DONE SO THAT IT LOOKS NICE
#
#                # Image Blobs in the datasets using ccp4mg
#                if d_handler.hit_clusters:
#
#                    pandda.log('===================================>>>', True)
#                    pandda.log('Imaging blobs in Dataset {!s}'.format(d_handler.tag), True)
#
#                    sorted_blob_indices = d_handler.hit_clusters.sort(sorting_function=max)
#                    blob_num = len(sorted_blob_indices)
#
#                    for blob_rank, blob_grid_peak in enumerate(d_handler.hit_clusters.get_centroids(indices=sorted_blob_indices)):
#
#                        # Only produce a certain number of images
#                        if blob_rank == pandda.params.blob_search.blobs_to_image:
#                            break
#
#                        status_bar(n=blob_rank, n_max=blob_num)
#
#                        blob_cart_peak = [g*pandda.reference_grid().grid_spacing() for g in blob_grid_peak]
#
#                        # Make images of the blob
#                        pandda.image_blob(  script    = d_handler.output_handler.get_file('ccp4mg_script'),
#                                            image     = d_handler.output_handler.get_file('ccp4mg_png'),
#                                            d_handler = d_handler,
#                                            point_no  = blob_rank+1,
#                                            point     = blob_cart_peak,
#                                            towards   = centre_of_mass
#                                        )
#
#                    status_bar(n=blob_num, n_max=blob_num)
# XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX

            # XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX
            # TODO REIMPLEMENT THIS IF WE CAN MAKE IT RUN EVEN VAGUELY QUICKLY
            #
            #    ##################################
            #    # POST-PROCESS Z-MAPS            #
            #    ##################################
            #
            #    mod_z_map, resamp_mod_z_map = pandda.process_z_map(z_map=z_map)
            #
            #    # Write map
            #    pandda.write_array_to_map(  output_file  = d_handler.get_mtz_filename().replace('.mtz','.processed.zvalues.ccp4'),
            #                                map_data     = flex.double(mod_z_map))
            #    # Write down-sampled map
            #    pandda.write_array_to_map(  output_file  = d_handler.get_mtz_filename().replace('.mtz','.resamp.processed.zvalues.ccp4'),
            #                                map_data     = flex.double(resamp_mod_z_map),
            #                                grid_size    = pandda.reference_grid().resampled_grid_size(),
            #                                grid_spacing = pandda.reference_grid().resampled_grid_spacing())
            # XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX

            # ============================================================================>
            #####
            # Write the summaries
            #####
            # ============================================================================>
            pandda.write_map_analyser_summary(map_analyser=map_analyser, analysis_mask_name=analysis_mask_name)
        
            # ============================================================================>
            #####
            # Print summaries
            #####
            # ============================================================================>
            t_end = time.time()
            pandda.log('===================================>>>', True)
            pandda.log('{!s}A Map Processing Time: {!s}'.format(cut_resolution, time.strftime("%H hours:%M minutes:%S seconds", time.gmtime(t_end - t_start))), True)
            pandda.log('@{!s}A:\t {!s}/{!s} Datasets Analysed'.format(cut_resolution, pandda.datasets.size(mask_name=analysis_mask_name), pandda.datasets.size(mask_name='rejected - total', invert=True)))
            pandda.log('Total:\t {!s}/{!s} Datasets Analysed'.format(pandda.datasets.size(mask_name='analysed'), pandda.datasets.size(mask_name='rejected - total', invert=True)))
            
            # ============================================================================>
            #####
            # PICKLE THE DATASETS THAT HAVE JUST BEEN PROCESSED
            #####
            # ============================================================================>
            # Clear the linked maps if requested
            if not pandda.args.settings.pickling.pickle_maps:
                for d_handler in pandda.datasets.mask(mask_name=analysis_mask_name):
                    d_handler.child = None           
            # Pickle the datasets 
            pandda.pickle_the_pandda(components=['datasets'], datasets=pandda.datasets.mask(mask_name=analysis_mask_name))
            pandda.update_pandda_size(tag='After Finding Clusters @ {!s}A'.format(cut_resolution))

            # ============================================================================>
            # DELETE THE MAP ANALYSER TO FREE UP MEMORY
            # ============================================================================>            
            map_analyser.parent = None
            del map_analyser
            map_analyser = None
            # ============================================================================>
            # DELETE THE PROCESSED MAPS + Z-MAPS TO SAVE MEMORY (JUST BE CAREFUL NOT TO OVERWRITE THEIR PICKLED FILES!)
            # ============================================================================>            
            for d_handler in pandda.datasets.mask(mask_name=analysis_mask_name):
                d_handler.child = None            
            # ============================================================================>
            # Launch Garbage-Collection Manually just to be sure
            # ============================================================================>            
            gc.collect() 
            gc.collect() 
            gc.collect() 
        
            # ============================================================================>
            #####
            # LIVE ANALYSIS - RUN AT THE END OF EACH LOOP
            #####
            # Analyse the processed data
            # TODO Create a `hit processor` class
            # ============================================================================>

            # Extract all events from datasets
            all_events=[]; [all_events.extend(d.events) for d in pandda.datasets.all()]
            # Cluster events to sites 
            site_list = cluster_events(all_events)
            # Sort sites by largest Z-Value
            site_list.sort(key=lambda s: (s.num_events, max([e.cluster.max for e in s.children])), reverse=True).renumber()
            # Add the site information to the site table (at the moment overwriting old information -- TODO)
            pandda.add_sites_to_site_table(new_sites=site_list, overwrite=True)
            # Update the site labels in the event table
            pandda.update_event_table(events=all_events)
            
            # XXX OLD METHOD XXX TO DELETE XXX
            # Write a combined file of all of the result hits
#            combined_cluster = pandda.process_z_value_clusters()
#            pandda.write_ranked_cluster_csv(    cluster        = combined_cluster,
#                                                sorted_indices = combined_cluster.sort(sorting_data='values', sorting_function=max, decreasing=True),
#                                                outfile        = pandda.output_handler.get_file('identified_sites')     )


            # ============================================================================>
            # Update the output file
            # ============================================================================>
            pandda.write_output_csvs()

        # ============================================================================>
        #####
        # END OF MAIN LOOP
        #####
        # ============================================================================>

        t_analysis_end = time.time()
        pandda.log('===================================>>>', True)
        pandda.log('Total Analysis Time: {!s}'.format(time.strftime("%H hours:%M minutes:%S seconds", time.gmtime(t_analysis_end - t_analysis_start))), True)
        # Pickle statistical maps again just to be sure
        pandda.pickle_the_pandda(components=['stat_maps'])
        pandda.update_pandda_size(tag='After Processing')

        # ============================================================================>
        #####
        # FINAL ANALYSIS - RUN ONCE AT THE END OF THE PROGRAM
        #####
        # Analyse all of the processed data
        # ============================================================================>
            
        # Collate events
        event_total, event_num, all_dataset_events = pandda.collate_event_counts()
        # Print some slightly less important information
        pandda.log('===================================>>>', False)
        for d_tag, event_count in event_num:
            pandda.log('Dataset {!s}: {!s} Events'.format(d_tag, event_count), False)
        # Print a summary of the number of identified events
        pandda.log('===================================>>>', True)
        pandda.log('Total Datasets with Events: {!s}'.format(len(event_num)), True)
        pandda.log('Total Events: {!s}'.format(event_total), True)

        # ============================================================================>
        #####
        # SUMMARIES ------------------------------>>>
        #####
        # ============================================================================>

        # Ugly output for the last time
        pandda.write_output_csvs()
        # Pretty output
        pandda.write_html_analysis_summary()

        try:
            from ascii_graph import Pyasciigraph
            g=Pyasciigraph()
            graph_data = [(res, len(resolution_count[res])) for res in sorted(resolution_count)]
            for l in g.graph(label='Datasets Processed at Each Resolution', data=graph_data, sort=0):
                print(l)
        except ImportError:
            print('IMPORT ERROR (ascii_graph) - CANNOT GENERATE MAP ANALYSIS GRAPH')
            pass

        pandda.log('Datasets Processed: {!s}'.format(sum(map(len,resolution_count.values()))))
        pandda.log('Datasets Loaded {!s}'.format(pandda.datasets.size(mask_name='rejected - total', invert=True)))

        pandda.exit()

    except KeyboardInterrupt:
        raise
    except AssertionError:
        raise
#    except Exception as err:
#        return (pandda, err)

    return pandda, None

def run_custom_analyses(pandda):
    """If a list of hits is available, test to see whether the pandda identified them"""

    try:

        # ============================================================================>
        # ======================================>
        # Manual Analyses - These only need processing once
        # ======================================>
        # ============================================================================>
        analyses_dir = pandda.output_handler.get_dir('analyses')
        # ============================================================================>

        if 0:
            pandda.log('===================================>>>')
            pandda.log('Calculating Deviations of C-alphas between structures')

            rms = lambda vals: numpy.sqrt(numpy.mean(numpy.abs(vals)**2))
            norm = lambda vals: numpy.sqrt(numpy.sum(numpy.abs(vals)**2))

            # Pull all c-alpha sites for each structure
            all_sites = numpy.array([d.transform_points_to_reference(d.get_calpha_sites()) for d in pandda.datasets.all()])
            # Calculate the mean x,y,z for each c-alpha
            mean_sites = numpy.mean(all_sites, axis=0)
            # Differences from the mean
            diff_sites = all_sites - mean_sites
            # Euclidean norms of the distances moved
            diff_norms = numpy.apply_along_axis(norm, axis=2, arr=diff_sites)

            with open(os.path.join(analyses_dir,'calpha_variation.csv'), 'w') as fh:
                for row in diff_norms:
                    out_list = row.round(3).tolist()
                    out_line = ', '.join(map(str,out_list)) + '\n'
                    fh.write(out_line)

            pandda.log('Largest deviation from the mean site: {!s}'.format(diff_norms.max()))
            pandda.log('Average deviation from the mean site: {!s}'.format(diff_norms.mean()))

        # ============================================================================>

        if 0:
            pandda.log('===================================>>>')
            pandda.log('Clustering the Refined Structures')

            distance_matrix = []
            for d1 in pandda.datasets.all():
               distance_matrix.append([d1.transform_points_to_reference(d1.get_calpha_sites()).rms_difference(d2.transform_points_to_reference(d2.get_calpha_sites())) for d2 in pandda.datasets.all()])

            distance_matrix = numpy.array(distance_matrix)

            with open(os.path.join(analyses_dir,'calpha_distance_matrix.csv'), 'w') as fh:
                for row in distance_matrix:
                    out_list = row.round(3).tolist()
                    out_line = ', '.join(map(str,out_list)) + '\n'
                    fh.write(out_line)

        # ============================================================================>

    except:
        pandda.log('FAILURE DURING CUSTOM ANALYSES', True)
        raise

    return 0

# ============================================================================>
#
#   COMMAND LINE RUN
#
# ============================================================================>

if __name__ == '__main__':

    welcome(os.getlogin())

    pandda, err = pandda_main(args=None)

    if err:
        raise err
