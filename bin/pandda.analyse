#!/usr/bin/env pandda.python

import os, sys, glob, time, gc
import numpy

from scitbx.array_family import flex
from scitbx.math import basic_statistics

from Giant.Xray.Maps.Occupancy import calculate_occupancy_subtracted_map, calculate_occupancy_correlations
from Giant.Xray.Symmetry import combine_hierarchies, generate_adjacent_symmetry_copies

from Giant.Grid.Masks import spherical_mask, atomic_mask, non_symmetrical_atomic_mask
from Giant.Grid.Utils import get_grid_points_within_index_cutoff_of_grid_sites

from Giant.Stats.Cluster import cluster_data
from Giant.Utils import status_bar, rel_symlink

from PANDDAs.Main import PanddaMultiDatasetAnalyser, PanddaMapAnalyser, MapHolder, Meta
from PANDDAs import welcome

def pandda_main(args):
    """Run the PANDDA algorithm, using supplied args object"""

    try:

        # ============================================================================>
        #####
        # MANUAL SETTINGS
        #####
        # ============================================================================>
        # None!
        # ============================================================================>
        #####
        # Initialise
        #####
        # ============================================================================>
        pandda = PanddaMultiDatasetAnalyser(args)
        # ============================================================================>
        #####
        # Initialise Settings
        #####
        # ============================================================================>
        pandda.set_low_resolution(pandda.params.analysis.high_res_lower_limit)
        pandda.set_high_resolution(pandda.params.analysis.high_res_upper_limit)
        pandda.run_pandda_init()
        # ============================================================================>
        #####
        # Build list of files in data directories
        #####
        # ============================================================================>
        input_files = pandda.build_input_list()
        # Check that some datasets have been found or already loaded
        if (not pandda.datasets.all()) and (not input_files):
            pandda.exit()
            raise SystemExit('NO DATASETS LOADED')
        # Check to see if we're reusing statistical maps
        if (not pandda.args.method.recalculate_statistical_maps) and pandda.stat_maps.get_resolutions():
            pandda.log('===================================>>>', True)
            pandda.log('REUSING STATISTICAL MAPS AT: {!s}'.format(pandda.stat_maps.get_resolutions()), True)
        # Check that enough datasets have been found
        elif pandda.datasets.size() + len(input_files) < pandda.params.analysis.min_build_datasets:
            pandda.log('===================================>>>', True)
            pandda.log('NOT ENOUGH DATASETS TO BUILD DISTRIBUTIONS!', True)
            pandda.log('Number loaded ({!s}) is less than the {!s} needed.'.format(pandda.datasets.size()+len(input_files), pandda.params.analysis.min_build_datasets), True)
            pandda.log('This value is defined by pandda.params.analysis.min_build_datasets', True)
            raise SystemExit('NOT ENOUGH DATASETS LOADED')
        # If dry_run, exit after initial search
        if pandda.args.settings.dry_run:
            return pandda, None
        # Load and process input files
        if input_files:
            # ============================================================================>
            #####
            # Add new files and load datasets
            #####
            # ============================================================================>
            pandda.add_new_files(input_files)
            pandda.load_new_datasets()
            pandda.initialise_analysis()
            # ============================================================================>
            #####
            # Set Reference Dataset
            #####
            # Select the reference dataset
            # ============================================================================>
            if not pandda.reference_dataset():
                # Select the reference dataset
                ref_pdb, ref_mtz = pandda.select_reference_dataset(method='resolution')
                # Load the reference dataset
                pandda.load_reference_dataset(ref_pdb=ref_pdb, ref_mtz=ref_mtz)
            # ============================================================================>
            #####
            # Scale, Align and Initial-Filter All Data
            #####
            # TODO Revisit Scaling
            # ============================================================================>
            # Filter out datasets with different protein structures
            pandda.filter_datasets_1()
            # Scale and align the datasets to the reference
            pandda.scale_datasets(  ampl_label=pandda.params.maps.ampl_label,
                                    phas_label=pandda.params.maps.phas_label    )
            pandda.align_datasets(  method=pandda.params.alignment.method   )
    #        symmetry_root = pandda.generate_crystal_contacts()
        else:
            # ============================================================================>
            # Rebuild the masks of the rejected datasets (quick)
            # ============================================================================>
            pandda.initialise_analysis()
            pandda.filter_datasets_1()
        # ============================================================================>
        # Check that enough VALID datasets have been found
        # ============================================================================>
        if (not pandda.args.method.recalculate_statistical_maps) and pandda.stat_maps.get_resolutions():
            # Using existing maps - don't need to check
            pass
        elif pandda.datasets.size(mask_name='rejected - total', invert=True) < pandda.params.analysis.min_build_datasets:
            pandda.log('===================================>>>', True)
            pandda.log('NOT ENOUGH (NON-REJECTED) DATASETS TO BUILD DISTRIBUTIONS!', True)
            pandda.log('Number loaded ({!s}) is less than the {!s} needed.'.format(pandda.datasets.size(mask_name='rejected - total', invert=True), pandda.params.analysis.min_build_datasets), True)
            pandda.log('This value is defined by pandda.params.analysis.min_build_datasets', True)
            raise SystemExit('NOT ENOUGH DATASETS LOADED')
        # ============================================================================>
        # Create neighbouring symmetry copies of the reference structures
        # ============================================================================>
        # XXX XXX XXX XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX XXX XXX XXX
        # Use symmetry operations to create the symmetry mates of the reference structure
        sym_ops, sym_hierarchies, chain_mappings = generate_adjacent_symmetry_copies(   ref_hierarchy    = pandda.reference_dataset().new_structure().hierarchy,
                                                                                        crystal_symmetry = pandda.reference_dataset().input().crystal_symmetry(),
                                                                                        buffer_thickness = pandda.params.masks.outer_mask+5    )
        # Record the symmetry operations that generate the crystal contacts
        pandda.crystal_contact_generators = sym_ops

        # Create a combined hierarchy of the crystal contacts
        symmetry_root = combine_hierarchies(sym_hierarchies)
        symmetry_root.atoms().set_xyz(symmetry_root.atoms().extract_xyz() + pandda.reference_dataset().origin_shift())

        # Write out the symmetry sites
        if not os.path.exists(pandda.output_handler.get_file('reference_symmetry')):
            symmetry_root.write_pdb_file(pandda.output_handler.get_file('reference_symmetry'))
        # XXX XXX XXX XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX XXX XXX XXX
        # ============================================================================>
        #####
        # Filter and Analyse the Datasets
        #####
        # ============================================================================>
        # Collate many variables across the datasets to be used for filtering
        pandda.analyse_dataset_variability_1()
        # Filter out the datasets that are not isomorphous and therefore incomparable
        pandda.filter_datasets_2()
        # Analyses the crystallographic and structural variability of the datasets
        pandda.calculate_mean_structure_and_protein_masks(deviation_cutoff=0.5)
        pandda.analyse_dataset_variability_2()
        # Analyse the structural variation in the datasets
#        pandda.analyse_structure_variability_1()
#        pandda.analyse_structure_variability_2()
        # ============================================================================>
        #####
        # Update Settings
        #####
        # ============================================================================>
        # Update the resolution limits using the resolution limits from the datasets supplied
        if pandda.params.analysis.dynamic_res_limits:
            pandda.set_low_resolution(   min(   pandda.get_low_resolution(),
                                                max(pandda.datasets_summary.get_data('high_res_limit'))   ))
            pandda.set_high_resolution(  max(   pandda.get_high_resolution(),
                                                min(pandda.datasets_summary.get_data('high_res_limit'))   ))
            pandda.log('===================================>>>')
            pandda.log('UPDATED RESOLUTION LIMITS')
            pandda.log('LOW RESOLUTION:  {!s}'.format(pandda.get_low_resolution()))
            pandda.log('HIGH RESOLUTION: {!s}'.format(pandda.get_high_resolution()))
        else:
            pandda.log('===================================>>>')
            pandda.log('**NOT** UPDATING RESOLUTION LIMITS')
            pandda.log('LOW RESOLUTION:  {!s}'.format(pandda.get_low_resolution()))
            pandda.log('HIGH RESOLUTION: {!s}'.format(pandda.get_high_resolution()))
        # ============================================================================>
        #####
        # Create Sample Grid
        #####
        # Create reference grid based on the reference structure
        # ============================================================================>
        if pandda.reference_grid() is None:
            # Create parameter for setting grid spacing (multiple grids?)
#            pandda.create_reference_grid(   grid_spacing     = pandda.params.maps.resolution_factor*pandda.get_high_resolution(),
#                                            expand_to_origin = False,
#                                            buffer           = pandda.params.masks.outer_mask+pandda.params.maps.padding    )
            pandda.create_reference_grid(   grid_spacing     = pandda.params.maps.grid_spacing,
                                            expand_to_origin = False,
                                            buffer           = pandda.params.masks.outer_mask+pandda.params.maps.padding    )
        # ============================================================================>
        #####
        # Create Local and Global Masks
        #####
        # ============================================================================>
        # Local mask used for forming groups of points around a grid point
        # ============================================================================>
        if pandda.reference_grid().local_mask() is None:
            pandda.log('===================================>>>')
            pandda.log('Generating Local Mask')
            local_mask = spherical_mask(grid_spacing    = pandda.reference_grid().grid_spacing(),
                                        distance_cutoff = 1.2,
                                        grid_jump       = 1 )
            pandda.reference_grid().set_local_mask(local_mask)
        # ============================================================================>
        # Global mask used for removing points in the bulk solvent regions
        # ============================================================================>
        if pandda.reference_grid().global_mask() is None:
            pandda.log('===================================>>>')
            pandda.log('Generating Protein Mask')
            # Select the masking atoms from the reference structure
            cache = pandda.reference_dataset().hierarchy().atom_selection_cache()
            pro_sites_cart = pandda.reference_dataset().hierarchy().select(cache.selection('pepnames and not element H')).atoms().extract_xyz()
            # Generate the main protein mask
            global_mask = atomic_mask(  cart_sites   = pro_sites_cart,
                                        grid_size    = pandda.reference_grid().grid_size(),
                                        unit_cell    = pandda.reference_grid().fake_unit_cell(),
                                        max_dist     = pandda.params.masks.outer_mask,
                                        min_dist     = pandda.params.masks.inner_mask )
            pandda.reference_grid().set_global_mask(global_mask)
        # ============================================================================>
        # Global mask used for removing points close to symmetry copies of the protein
        # ============================================================================>
        if pandda.reference_grid().symmetry_mask() is None:
            pandda.log('===================================>>>')
            pandda.log('Generating Symmetry Mask')
            # Pull out the cartesian sites of the symmetry mates
            cache = symmetry_root.atom_selection_cache()
            sym_sites_cart = symmetry_root.select(cache.selection('pepnames and not element H')).atoms().extract_xyz()
            # Generate the symmetry mask
            symmetry_mask = non_symmetrical_atomic_mask(  cart_sites = sym_sites_cart,
                                                          grid_spacing = pandda.reference_grid().grid_spacing(),
                                                          grid_size  = pandda.reference_grid().grid_size(),
                                                          unit_cell  = pandda.reference_grid().fake_unit_cell(),
                                                          max_dist   = pandda.params.masks.outer_mask,
                                                          min_dist   = pandda.params.masks.inner_mask )
            pandda.reference_grid().set_symmetry_mask(symmetry_mask)
        # ============================================================================>
        # Print the summaries
        # ============================================================================>
        pandda.log('===================================>>>', True)
        pandda.log('Grid Summary: ', True)
        pandda.log(pandda.reference_grid().summary(), True)
        pandda.log(pandda.reference_grid().local_mask().summary(), True)
        pandda.log(pandda.reference_grid().global_mask().summary(), True)
        # TODO TODO TODO INCORPORATE THE COMBINATION OF MASKS INTO THIS FUNCTION
        # Create various masks to define regions of the grid by distance to the protein and symmetry copies
        pandda.mask_reference_grid(d_handler=pandda.reference_dataset())
    #    if pandda.reference_grid().masked_grid_points() is None:
    #        pandda.mask_resampled_reference_grid()
    #    # TODO TODO TODO

        # ============================================================================>
        # ============================================================================>
        # ============================================================================>
        # ============================================================================>
        # ============================================================================>
        # TODO TODO TODO - MOVE?
        # Combine the symmetry mask and the atomic mask to create the mask for clustering
        pandda.log('===================================>>>', True)
        pandda.log('Combining Atomic Mask and Symmetry Mask', True)
        grid_idxr = pandda.reference_grid().grid_indexer()
        # Mask EXCLUDING the inner mask (excluding symmetry inner mask)
        combined_total_mask = [gp for gp in pandda.reference_grid().global_mask().total_mask() if pandda.reference_grid().symmetry_mask().inner_mask_binary()[grid_idxr(gp)] == 0]
        # Mask INCLUDING the inner mask (excluding symmetry inner mask)
        combined_outer_mask = [gp for gp in pandda.reference_grid().global_mask().outer_mask() if pandda.reference_grid().symmetry_mask().inner_mask_binary()[grid_idxr(gp)] == 0]

        pandda.log('GLOBAL TOTAL MASK: {!s}'.format(len(pandda.reference_grid().global_mask().total_mask())))
        pandda.log('SYMMETRY INNER MASK: {!s}'.format(len(pandda.reference_grid().symmetry_mask().inner_mask())))
        pandda.log('COMBINED TOTAL MASK: {!s}'.format(len(combined_total_mask)))
        pandda.log('COMBINED OUTER MASK: {!s}'.format(len(combined_outer_mask)))
        # TODO TODO TODO

        # TODO TODO TODO - MOVE?
        # Calculate centre of mass of the reference dataset
        pandda.log('===================================>>>', True)
        pandda.log('Calculating Reference Structure Centre of Mass', True)

        # Should be in the reference frame
        ref_structure = pandda.reference_dataset().hierarchy()
        backbone_atoms = ref_structure.select(ref_structure.atom_selection_cache().selection('pepnames and (name CA or name C or name O or name N)')).atoms()
        backbone_sites = backbone_atoms.extract_xyz()
        backbone_wghts = flex.double([1]*len(backbone_sites))

        centre_of_mass = backbone_sites.mean_weighted(weights=backbone_wghts)
        # TODO TODO TODO
        # ============================================================================>
        # ============================================================================>
        # ============================================================================>
        # ============================================================================>
        # ============================================================================>
            
        # ============================================================================>
        #####
        # STORING FOR REUSE
        #####
        # ============================================================================>
        # Pickle all of the large arrays so they can be reloaded
        # ============================================================================>
        pandda.pickle_the_pandda(all=True)
        pandda.update_pandda_size(tag='After Pre-processing')

        # ============================================================================>
        #####
        # PREPARE VARIABLES TO LOOP OVER RESOLUTION SHELLS
        #####
        # ============================================================================>
        # Calculate cutoffs for resolution shells
        # ============================================================================>
        if (not pandda.args.method.recalculate_statistical_maps) and pandda.stat_maps.get_resolutions():
            # Use the resolution limits of previously run campaigns
            res_limits = pandda.stat_maps.get_resolutions()
            assert res_limits, 'No Resolution Limits found from statistical maps: {!s}'.format(res_limits)
            min_limit = min(res_limits)
            max_limit = max(res_limits)
        else:
            # Set pandda.args.method.recalculate_statistical_maps to True as no statistical maps have been found
            if not pandda.args.method.recalculate_statistical_maps:
                pandda.log('No Statistical Maps Found: Setting pandda.args.method.recalculate_statistical_maps to True', True)
                pandda.args.method.recalculate_statistical_maps = True
            # ============================================================================>
            # Select resolution limits based on dataset resolutions and given arguments
            # ============================================================================>
            if pandda.params.analysis.dynamic_res_limits:
                # Round the high limit DOWN to the nearest 0.01
                min_limit = round(pandda.get_high_resolution()- 0.005, 2)    # i.e. 1.344 -> 1.34
                # Round the low limit UP to the nearest 0.01
                max_limit = round(pandda.get_low_resolution() + 0.005, 2)    # i.e. 3.423 -> 3.43
            else:
                # Take the arguments as given by the user
                min_limit = pandda.get_high_resolution()
                max_limit = pandda.get_low_resolution()
            # ============================================================================>
            # Create resolution shells (or single limit)
            # ============================================================================>
            if not pandda.params.analysis.high_res_increment:
                # No variable cutoff - select all 
                res_limits = [max_limit]  # i.e. [2]
            else:
                # Calculate a range of resolution limits
                shell_width = pandda.params.analysis.high_res_increment
                res_limits = [round(x, 4) for x in numpy.arange(min_limit, max_limit, shell_width).tolist()]
                # Append the rounded max_limit to the end to ensure that the last dataset is processed            
                if res_limits[-1] != max_limit:
                    res_limits.append(max_limit)
        # ============================================================================>
        # Report
        # ============================================================================>
        pandda.log('===================================>>>', True)
        if len(res_limits)==1:
            pandda.log('Analysing All Maps at {!s}A'.format(max_limit), True)
        else:
            pandda.log('Analysing Resolution Shells from {!s} -> {!s}A'.format(min_limit, max_limit), True)
            pandda.log('Limits: {!s}'.format(', '.join(map(str,res_limits))), True)
        # ============================================================================>
        # Initialise for iterations over shells
        # ============================================================================>
        # Analyse all datasets from high_shell_limit -> cut_resolution (initialise high_shell_limit to 0)
        high_shell_limit=0
        # Record how many datasets are processed at each resolution
        resolution_count={}
        # Record the amount of time spent analysing the maps
        t_analysis_start = time.time()
        pandda.log('===================================>>>', True)
        pandda.log('Map Analysis Started: {!s}'.format(time.strftime("%a, %d %b %Y %H:%M:%S", time.gmtime(t_analysis_start))), True)

        # ============================================================================>
        #####
        # ANALYSE DATASETS - ITERATE THROUGH RESOLUTION SHELLS
        #####
        # ============================================================================>
        for cut_resolution in res_limits:

            # Which resolutions will be processed in this shell
            pandda.log('===================================>>>', True)
            pandda.log('Looking for Datasets to Process from {!s}A -> {!s}A'.format(high_shell_limit, cut_resolution), True)

            # ============================================================================>
            # Select datasets to build the distributions
            # ============================================================================>
            building_mask_name = 'selected for building @ {!s}A'.format(cut_resolution)
            if pandda.args.method.recalculate_statistical_maps:
                pandda.log('===================================>>>', True)
                pandda.log('Selecting Building Mask', True)
                building_mask = pandda.select_for_building_distributions(high_res_cutoff = cut_resolution,
                                                                         building_mask_name = building_mask_name)
                # Check that we have enough datasets to build distributions
                if sum(building_mask) < pandda.params.analysis.min_build_datasets:
                    # Don't have enough to construct robust distributions
                    pandda.log('NOT ENOUGH DATASETS TO CALCULATE DISTRIBUTIONS ({!s}<{!s})'.format(sum(building_mask),pandda.params.analysis.min_build_datasets), True)
                    continue
                else:
                    # Have enough to generate robust distributions
                    pandda.log('ENOUGH DATASETS -> PROCESSING THIS RESOLUTION', True)
                    pandda.log('Building Distributions using {!s} Datasets'.format(sum(building_mask)), True)
            else:
                pandda.log('===================================>>>', True)
                pandda.log('**NOT** Selecting Building Mask (Using Existing Statistical Maps)', True)
                # Create a dummy mask as we won't be using any datasets for building
                building_mask = [False]*pandda.datasets.size()
                pandda.datasets.all_masks().add_mask(mask_name=building_mask_name, mask=building_mask)

            # ============================================================================>
            # Select the datasets to analyse
            # ============================================================================>
            analysis_mask_name = 'selected for analysis @ {!s}A'.format(cut_resolution)
            pandda.log('===================================>>>', True)
            pandda.log('Selecting Analysis Mask', True)
            analysis_mask = pandda.select_for_analysis(high_res_large_cutoff = cut_resolution, 
                                                       high_res_small_cutoff = high_shell_limit, 
                                                       analysis_mask_name = analysis_mask_name)
            # Check that there're some datasets to analyse
            if sum(analysis_mask) == 0:
                pandda.log('NO DATASETS TO ANALYSE @ {!s}'.format(cut_resolution))
                continue
            else:
                pandda.log('Calculating Z-Maps for {!s} Datasets'.format(sum(analysis_mask)), True)

            # ============================================================================>
            # Combine the masks as we will need to load maps for all datasets
            # ============================================================================>
            pandda.log('===================================>>>', True)
            pandda.log('Combining (Analysis and Building) Masks', True)
            map_load_mask = pandda.datasets.all_masks().combine_masks([analysis_mask_name, building_mask_name])
            map_load_mask_name = 'selected for loading maps @ {!s}A'.format(cut_resolution)
            pandda.datasets.all_masks().add_mask(mask_name=map_load_mask_name, mask=map_load_mask)

            # ============================================================================>
            # Report
            # ============================================================================>
            pandda.log('===================================>>>')
            pandda.log('Mask Names for Building, Loading, and Analysis')
            pandda.log('Building ({!s} datasets): {!s}'.format(sum(building_mask), building_mask_name))
            pandda.log('Load Map ({!s} datasets): {!s}'.format(sum(map_load_mask), map_load_mask_name))
            pandda.log('Analysis ({!s} datasets): {!s}'.format(sum(analysis_mask), analysis_mask_name))
            pandda.log('===================================>>>', True)
            pandda.log('Loading Maps for {!s} Datasets at {!s}A'.format(pandda.datasets.size(mask_name=map_load_mask_name), cut_resolution), True)

            # ============================================================================>
            # Update limit (cut resolution becomes top limit in next shell)
            # ============================================================================>
            high_shell_limit = cut_resolution

            # ============================================================================>
            #####
            # Truncate data, load and scale maps to reference dataset
            #####
            # ============================================================================>
            # Truncate the data to a particular resolution
            # ============================================================================>
            pandda.truncate_scaled_data(dataset_handlers = pandda.datasets.mask(mask_name=map_load_mask_name))
            # ============================================================================>
            # Load the reference map so that we can scale the individual maps to this
            # ============================================================================>
            ref_map_holder = pandda.load_reference_map( map_resolution = cut_resolution )
            # ============================================================================>
            # Load the required maps
            # ============================================================================>
            map_holder_list = pandda.load_and_morph_maps( dataset_handlers = pandda.datasets.mask(mask_name=map_load_mask_name),
                                                          ref_map_holder   = ref_map_holder,
                                                          map_resolution   = cut_resolution     )
            # ============================================================================>
            # Extract the statistical maps at this resolution (if requested)
            # ============================================================================>
            if pandda.args.method.recalculate_statistical_maps:
                # No Statistical maps - set to None so that they are created
                statistical_maps = None
            else:
                # Try to find a statistical map at this resolution
                statistical_maps = pandda.stat_maps.get(cut_resolution)
            # ============================================================================>
            # Create an object to hold all of the maps, and can be used to calculate the mean maps, etc...
            # ============================================================================>
            map_analyser = PanddaMapAnalyser(   dataset_maps     = map_holder_list,
                                                meta             = Meta({'resolution'   : cut_resolution,
                                                                         'grid_size'    : pandda.reference_grid().grid_size(),
                                                                         'grid_size_1d' : pandda.reference_grid().grid_size_1d()}),
                                                statistical_maps = statistical_maps,
                                                parent           = pandda )
            # ============================================================================>
            # Add the analysis mask to the map analyser        
            # ============================================================================>
            map_analyser.dataset_maps.all_masks().add_mask(mask_name=analysis_mask_name, mask=[False]*map_analyser.dataset_maps.size())
            for dh in pandda.datasets.mask(mask_name=analysis_mask_name):
                map_analyser.dataset_maps.all_masks().set_mask_value(mask_name=analysis_mask_name, entry_id=dh.tag, value=True)
            # ============================================================================>
            # Add the building mask to the map analyser
            # ============================================================================>
            map_analyser.dataset_maps.all_masks().add_mask(mask_name=building_mask_name, mask=[False]*map_analyser.dataset_maps.size())
            for dh in pandda.datasets.mask(mask_name=building_mask_name):
                map_analyser.dataset_maps.all_masks().set_mask_value(mask_name=building_mask_name, entry_id=dh.tag, value=True)

            # ============================================================================>
            #####
            # Calculate Statistical Maps (if required) and Calculate Dataset Uncertainties
            #####
            # ============================================================================>
            if pandda.args.method.recalculate_statistical_maps:
                pandda.log('===================================>>>', True)
                pandda.log('Building Map Distributions for {!s} Datasets at {!s}A'.format(pandda.datasets.size(mask_name=building_mask_name), cut_resolution), True)
            else:
                pandda.log('===================================>>>', True)
                pandda.log('Using Existing Map Distributions at {!s}A'.format(cut_resolution), True)
                pandda.log('This should be 0: {!s}'.format(pandda.datasets.size(mask_name=building_mask_name)))
                pandda.log('This should be 0: {!s}'.format(map_analyser.dataset_maps.size(mask_name=building_mask_name)))
            # ============================================================================>
            # Calculate the mean map
            # ============================================================================>
            if (not map_analyser.statistical_maps.mean_map) or pandda.args.method.recalculate_statistical_maps:
                mean_map = map_analyser.calculate_mean_map(masked_idxs=pandda.reference_grid().global_mask().outer_mask_indices(), mask_name=building_mask_name)
            # ============================================================================>
            # Calculate the uncertainty of all loaded maps (needs the mean map to have been calculated)
            # ============================================================================>
            # If pandda.args.method.recalculate_statistical_maps, then no building_mask datasets will have been added, so don't need to be selective in this function
            map_uncties = map_analyser.calculate_map_uncertainties(masked_idxs=pandda.reference_grid().global_mask().inner_mask_indices())
            # ============================================================================>
            # Plot uncertainties of maps
            # ============================================================================>
            try:
                from ascii_graph import Pyasciigraph
                g=Pyasciigraph()
                graph_data = [(mh.tag, round(mh.meta.map_uncertainty,3)) for mh in map_analyser.dataset_maps.all()]
                for l in g.graph(label='Sorted Map Uncertainties (Ascending Order)', data=graph_data, sort=1):
                    print(l)
            except ImportError:
                print('IMPORT ERROR (ascii_graph) - CANNOT GENERATE UNCERTAINTY GRAPH')
                pass
            # ============================================================================>
            # Calculate the statistics of the maps
            # ============================================================================>
            if (not map_analyser.statistical_maps.sadj_map) or pandda.args.method.recalculate_statistical_maps:
                map_analyser.calculate_statistical_maps(    masked_idxs=pandda.reference_grid().global_mask().outer_mask_indices(),
                                                            mask_name=building_mask_name,
                                                            cpus=pandda.args.settings.cpus)
            # TODO TODO TODO
            # TODO IMPLEMENT - Collect all of the map statistics from the different dataset_handlers
            # So that the data_collections can be populated independently of when the uncertainties etc were calculated
            #pandda.analyse_dataset_variability_3()
            # TODO TODO TODO

            # ============================================================================>
            # PICKLE THE DATASETS THAT HAVE JUST BEEN PROCESSED
            # ============================================================================>
            pandda.pickle_the_pandda(components=['datasets'], datasets=pandda.datasets.mask(mask_name=analysis_mask_name))
            pandda.update_pandda_size(tag='After Analysing Maps @ {!s}'.format(cut_resolution))
            # ============================================================================>
            # Extract and store the statistical map objects
            # ============================================================================>
            # Only need to add if we're calculating new statistical maps
            if pandda.args.method.recalculate_statistical_maps:
                # Check to see if already in there
                if cut_resolution in pandda.stat_maps.get_resolutions():
                    pandda.log('Overwriting existing statistical maps @ {!s}A'.format(cut_resolution))
                pandda.stat_maps.add(stat_map_list=map_analyser.statistical_maps, resolution=cut_resolution)
                pandda.pickle_the_pandda(components=['stat_maps'])
            # ============================================================================>
            # Write out Grid Point Distributions for interesting grid points (high modality, etc...)
            # ============================================================================>
            # TODO TODO TODO
            # TODO TODO TODO
            # ============================================================================>
            # Write Grid Point Distributions - Standard Function, just to provide an output
            # ============================================================================>
            try:
                from libtbx.math_utils import iceil
                grid_size = pandda.reference_grid().grid_size()
                num_points = min(10, min(grid_size))
                grid_points = zip(*[range(0, s, iceil(s/num_points)) for s in grid_size])
                pandda.write_grid_point_distributions(  grid_points     = grid_points,
                                                        map_analyser    = map_analyser,
                                                        output_filename = None          )
            except:
                print('UNIMPORTANT: FAILED TO WRITE AUTOMATIC DISTRIBUTION OF GRID POINTS')
                raise
            # ============================================================================>
            #####
            # Blob Search - Manual Settings
            #####
            # ============================================================================>
            # Minimum size of cluster
            min_cluster_volume   = pandda.params.blob_search.min_blob_volume
            min_cluster_z_peak   = pandda.params.blob_search.min_blob_z_peak
            # Z cutoff for maps
            z_cutoff             = pandda.params.blob_search.contour_level
            # Clustering Methods
            clustering_criterion = pandda.params.blob_search.clustering.criterion
            clustering_metric    = pandda.params.blob_search.clustering.metric
            clustering_method    = pandda.params.blob_search.clustering.linkage
            # Cutoff for separation of clusters (sqrt((2x)**2 + (2y)**2 + (2z)**2)) -- allows diagonal grid points to connect
            clustering_cutoff = 1.1 * numpy.math.sqrt(3) * pandda.reference_grid().grid_spacing()
            # ============================================================================>
            #####
            # DATA PROCESSING
            #####
            # Calculate the moments of the distributions at the grid points
            # Use the means and the stds to convert the maps to z-maps
            # Use the local mask to look for groups of significant z-values
            # ============================================================================>
            # XXX Change this to a separate object XXX
            pandda.print_clustering_settings(   z_cutoff             = z_cutoff,
                                                min_cluster_volume   = min_cluster_volume,
                                                min_cluster_z_peak   = min_cluster_z_peak,
                                                clustering_cutoff    = clustering_cutoff,
                                                clustering_criterion = clustering_criterion,
                                                clustering_metric    = clustering_metric,
                                                clustering_method    = clustering_method  )

            assert cut_resolution not in resolution_count
            resolution_count[cut_resolution] = []

            # ============================================================================>
            #####
            # Calculate Z-Maps
            #####
            # ============================================================================>
            # Time the processing of the dataset maps
            t_start = time.time()
            pandda.log('===================================>>>', True)
            pandda.log('Calculating Z-Maps for {!s} Datasets at {!s}A'.format(pandda.datasets.size(mask_name=analysis_mask_name), cut_resolution), True)

            # Iterate through and calculate z-maps
            for i_dh, d_handler in enumerate(pandda.datasets.mask(mask_name=analysis_mask_name)):

                # Record the which resolution this dataset was analysed at
                resolution_count[cut_resolution].append(d_handler.tag)
                    
                # Create a file that says what resolution this was processed at
                link_dir = os.path.join(pandda.output_handler.get_dir('resolutions'), '{!s}-{!s}'.format(d_handler.name, cut_resolution))
                if not os.path.exists(link_dir):
                    rel_symlink(orig=d_handler.output_handler.get_dir('root'), link=link_dir)

                # Make this a function and add more to it...
                text_file = d_handler.output_handler.get_file('dataset_info')
                with open(text_file, 'a') as fh:
                    # XXX WRITE SOME MORE INFORMATION OUT HERE
                    fh.write('PROCESSED AT {!s}A\n'.format(cut_resolution))
               
                # Flag that this dataset is being analysed 
                pandda.datasets.all_masks().set_mask_value(mask_name='analysed', entry_id=d_handler.tag, value=True)
                
                # Extract the map for this dataset    
                m_handler = map_analyser.dataset_maps.get(tag=d_handler.tag)
                
                ################################################
                # AUTOGENERATE SCRIPTS FOR VIEWING THE DATASET #
                ################################################

                pandda.write_pymol_scripts(d_handler=d_handler)
                
                #############################################
                # CALCULATE Z-MAPS AND LOOK FOR LARGE BLOBS #
                #############################################

                pandda.log('===================================>>>', True)
                pandda.log('Calculating Z-MAPs for Dataset {!s} ({!s}/{!s})'.format(d_handler.tag, i_dh+1, pandda.datasets.size(mask_name=analysis_mask_name)), True)

                ##################################
                # EXTRACT MAP VALUES             #
                ##################################

                assert m_handler.map is not None, 'NO MAP FOUND'
                # Write the sampled map
                if not os.path.exists(d_handler.output_handler.get_file('sampled_map')):
                    pandda.write_array_to_map(  output_file = d_handler.output_handler.get_file('sampled_map'),
                                                map_data    = m_handler.map)
                # Write distribution of the map values
                if not os.path.exists(d_handler.output_handler.get_file('s_map_png')):
                    pandda.write_map_value_distribution(map_vals     = m_handler.map.select(pandda.reference_grid().global_mask().outer_mask_indices()),
                                                        output_file  = d_handler.output_handler.get_file('s_map_png'))

                ##################################
                # CALCULATE MEAN-DIFF MAPS       #
                ##################################
                d_map = m_handler.map - map_analyser.statistical_maps.mean_map
                # Write the map
                if not os.path.exists(d_handler.output_handler.get_file('mean_diff_map')):
                    pandda.write_array_to_map(  output_file = d_handler.output_handler.get_file('mean_diff_map'),
                                                map_data    = d_map)
                # Write distribution of the map values
                if not os.path.exists(d_handler.output_handler.get_file('d_mean_map_png')):
                    pandda.write_map_value_distribution(map_vals     = d_map.select(pandda.reference_grid().global_mask().outer_mask_indices()),
                                                        output_file  = d_handler.output_handler.get_file('d_mean_map_png'))

                ##################################
                # CALCULATE Z-MAPS               #
                ##################################

                ###################################################################
                # NAIVE Z-MAP - NOT USING UNCERTAINTY ESTIMATION OR ADJUSTED STDS #
                ###################################################################

                z_map_naive = map_analyser.calculate_z_map(tag=d_handler.tag, method='naive')
                # Write z map
                if pandda.args.settings.developer.write_all_z_map_types and (not os.path.exists(d_handler.output_handler.get_file('z_map_naive'))):
                    pandda.write_array_to_map(  output_file = d_handler.output_handler.get_file('z_map_naive'),
                                                map_data    = z_map_naive)
                # Write distribution of the map values
                if not os.path.exists(d_handler.output_handler.get_file('z_map_naive_png')):
                    pandda.write_map_value_distribution(map_vals     = z_map_naive.select(pandda.reference_grid().global_mask().outer_mask_indices()),
                                                        output_file  = d_handler.output_handler.get_file('z_map_naive_png'),
                                                        plot_normal  = True)

                ##################################
                # Normalise this map to N(0,1)   #
                ##################################
                z_map_naive_masked = [z_map_naive[i] for i in pandda.reference_grid().global_mask().outer_mask_indices()]
                z_map_naive_normalised = (z_map_naive - numpy.mean(z_map_naive_masked)) / numpy.std(z_map_naive_masked)
                # Write z map
                if pandda.args.settings.developer.write_all_z_map_types and (not os.path.exists(d_handler.output_handler.get_file('z_map_naive_normalised'))):
                    pandda.write_array_to_map(  output_file = d_handler.output_handler.get_file('z_map_naive_normalised'),
                                                map_data    = z_map_naive_normalised)
                # Write distribution of the map values
                if not os.path.exists(d_handler.output_handler.get_file('z_map_naive_normalised_png')):
                    pandda.write_map_value_distribution(map_vals     = z_map_naive_normalised.select(pandda.reference_grid().global_mask().outer_mask_indices()),
                                                        output_file  = d_handler.output_handler.get_file('z_map_naive_normalised_png'),
                                                        plot_normal  = True)

                ##################################
                # ADJUSTED+UNCERTAINTY Z-MAP     #
                ##################################
                z_map_corrected = map_analyser.calculate_z_map(tag=d_handler.tag, method='adjusted+uncertainty')
                # Write z map
                if pandda.args.settings.developer.write_all_z_map_types and (not os.path.exists(d_handler.output_handler.get_file('z_map_corrected'))):
                    pandda.write_array_to_map(  output_file = d_handler.output_handler.get_file('z_map_corrected'),
                                                map_data    = z_map_corrected)
                # Write distribution of the map values
                if not os.path.exists(d_handler.output_handler.get_file('z_map_corrected_png')):
                    pandda.write_map_value_distribution(map_vals     = z_map_corrected.select(pandda.reference_grid().global_mask().outer_mask_indices()),
                                                        output_file  = d_handler.output_handler.get_file('z_map_corrected_png'),
                                                        plot_normal  = True)

                # ============================================================================>
                # Normalise this map to N(0,1)
                # ============================================================================>
                z_map_corrected_masked = [z_map_corrected[i] for i in pandda.reference_grid().global_mask().outer_mask_indices()]
                z_map_corrected_normalised = (z_map_corrected - numpy.mean(z_map_corrected_masked)) / numpy.std(z_map_corrected_masked)
                # Write z map (always want to be writing this map)
                if not os.path.exists(d_handler.output_handler.get_file('z_map_corrected_normalised')):
                    pandda.write_array_to_map(  output_file = d_handler.output_handler.get_file('z_map_corrected_normalised'),
                                                map_data    = z_map_corrected_normalised)
                # Write distribution of the map values
                if not os.path.exists(d_handler.output_handler.get_file('z_map_corrected_normalised_png')):
                    pandda.write_map_value_distribution(map_vals     = z_map_corrected_normalised.select(pandda.reference_grid().global_mask().outer_mask_indices()),
                                                        output_file  = d_handler.output_handler.get_file('z_map_corrected_normalised_png'),
                                                        plot_normal  = True)
                # Plot sorted map values against a normal distribution to see how normal the z-map is
                if not os.path.exists(d_handler.output_handler.get_file('z_map_qq_plot_png')):
                    pandda.write_qq_plot_against_normal(map_vals     = z_map_corrected_normalised.select(pandda.reference_grid().global_mask().outer_mask_indices()),
                                                        output_file  = d_handler.output_handler.get_file('z_map_qq_plot_png'))

                # ============================================================================>
                # ANALYSE Z-MAP FOR STATISTICAL VALIDITY
                # ============================================================================>
                # Calculate and store statistics of z-maps
                z_map_stats = basic_statistics(flex.double([z_map_corrected[i] for i in pandda.reference_grid().global_mask().outer_mask_indices()]))
                m_handler.z_map_stats = {}
                m_handler.z_map_stats['z_map_mean'] = z_map_stats.mean
                m_handler.z_map_stats['z_map_std']  = z_map_stats.bias_corrected_standard_deviation
                m_handler.z_map_stats['z_map_skew'] = z_map_stats.skew
                m_handler.z_map_stats['z_map_kurt'] = z_map_stats.kurtosis
                # Store the z-statistics for this dataset                
                pandda.map_observations.set_data_value(data_name='z_map_mean', entry_id=m_handler.tag, value=m_handler.z_map_stats['z_map_mean'])
                pandda.map_observations.set_data_value(data_name='z_map_std',  entry_id=m_handler.tag, value=m_handler.z_map_stats['z_map_std'])
                pandda.map_observations.set_data_value(data_name='z_map_skew', entry_id=m_handler.tag, value=m_handler.z_map_stats['z_map_skew'])
                pandda.map_observations.set_data_value(data_name='z_map_kurt', entry_id=m_handler.tag, value=m_handler.z_map_stats['z_map_kurt'])
                # Store other meta about this z-map
                pandda.map_observations.set_data_value(data_name='raw_masked_map_mean', entry_id=d_handler.tag, value=m_handler.meta.raw_masked_map_mean)
                pandda.map_observations.set_data_value(data_name='raw_masked_map_rms',  entry_id=d_handler.tag, value=m_handler.meta.raw_masked_map_rms)
                pandda.map_observations.set_data_value(data_name='map_uncertainty',     entry_id=d_handler.tag, value=m_handler.meta.map_uncertainty)
                pandda.map_observations.set_data_value(data_name='analysed_resolution', entry_id=d_handler.tag, value=m_handler.meta.resolution)

                # ============================================================================>
                # XXX XXX XXX XXX XXX XXX XXX XXX XXX XXX XXX XXX
                # XXX  WHICH MAP TO DO THE BLOB SEARCHING ON  XXX
                # XXX XXX XXX XXX XXX XXX XXX XXX XXX XXX XXX XXX
                # ============================================================================>
                
                z_map = z_map_corrected_normalised

                # ============================================================================>
                # STORE THE Z MAP        
                # ============================================================================>

                z_map_holder = MapHolder(   num         = m_handler.num,
                                            tag         = m_handler.tag,
                                            map         = z_map,
                                            # Change these for the 'fake' grid unit_cell and a P1 space_group
                                            unit_cell   = m_handler.unit_cell,
                                            space_group = m_handler.space_group,
                                            meta        = Meta({'type'          : 'z-map',
                                                                'resolution'    : cut_resolution}),
                                            parent      = m_handler  )
                # Link the sampled map handler to the z map holder
                m_handler.child = z_map_holder
                # Link the dataset handler to the child map holder
                d_handler.child = m_handler

                # ============================================================================>
                #####
                # LOOK FOR CLUSTERS OF Z-SCORES  
                #####
                # ============================================================================>

                num_clusters, raw_hit_clusters = pandda.cluster_high_z_values(
                                                            d_handler            = d_handler,
                                                            z_map                = z_map,
                                                            z_cutoff             = z_cutoff,
#                                                                point_mask           = combined_outer_mask,
                                                            point_mask           = combined_total_mask,
                                                            clustering_cutoff    = clustering_cutoff,
                                                            clustering_criterion = clustering_criterion,
                                                            clustering_metric    = clustering_metric,
                                                            clustering_method    = clustering_method
                                                         )

                if num_clusters == 0:
                    # Nothing more to do with this dataset at the moment
                    continue
                elif num_clusters == -1:
                    # This dataset is too noisy to analyse - flag!
                    pandda.datasets.all_masks().set_mask_value(mask_name='noisy zmap', entry_id=d_handler.tag, value=True)
                    # Link datasets to the noisy results directory
                    noisy_dir = os.path.join(pandda.output_handler.get_dir('noisy_datasets'), d_handler.name)
                    if not os.path.exists(noisy_dir):
                        rel_symlink(orig=d_handler.output_handler.get_dir('root'), link=noisy_dir)
                    # Nothing more to do with this dataset at the moment
                    continue

                # ============================================================================>
                #####
                # FILTER CLUSTERS OF Z-SCORES  
                #####
                # ============================================================================>

                # Filter the clusters by size and peak height
                num_clusters, filtered_hit_clusters = pandda.filter_z_clusters_1(
                                                            hit_clusters       = raw_hit_clusters,
                                                            min_cluster_volume = min_cluster_volume,
                                                            min_cluster_z_peak = min_cluster_z_peak     )

                if num_clusters == 0:
                    pandda.log('===> No Clusters found - Minimum cluster peak/size not reached.', True)
                    # Nothing more to do with this dataset at the moment
                    continue

                # ============================================================================>
                # Filter the clusters by distance from protein
                # ============================================================================>
                num_clusters, filtered_hit_clusters = pandda.filter_z_clusters_2(
                                                            hit_clusters     = filtered_hit_clusters,
                                                            grid_spacing     = pandda.reference_grid().grid_spacing(), 
                                                            grid_origin_cart = pandda.reference_dataset().origin_shift(),
                                                            ref_structure    = pandda.reference_dataset().new_structure().hierarchy     )

                if num_clusters == 0:
                    pandda.log('===> No Clusters found - Clusters too far from protein.', True)
                    # Nothing more to do with this dataset at the moment
                    continue
                
                # ============================================================================>
                # GROUP NEARBY CLUSTERS TOGETHER
                # ============================================================================>
                num_clusters, filtered_hit_clusters = pandda.group_clusters( hit_clusters = filtered_hit_clusters,
                                                                             grid_spacing = pandda.reference_grid().grid_spacing()   )


                # ============================================================================>
                # Filter the clusters by symmetry equivalence
                # ============================================================================>
                num_clusters, filtered_hit_clusters = pandda.filter_z_clusters_3(
                                                            hit_clusters     = filtered_hit_clusters,
                                                            grid_spacing     = pandda.reference_grid().grid_spacing(), 
                                                            grid_origin_cart = pandda.reference_dataset().origin_shift(),
                                                            ref_unit_cell    = pandda.reference_dataset().unit_cell,
                                                            ref_sym_ops      = pandda.crystal_contact_generators,
                                                            ref_structure    = pandda.reference_dataset().new_structure().hierarchy     )
                
                pandda.log('===> {!s} Cluster(s) found.'.format(num_clusters), True)
                assert num_clusters > 0, 'NUMBER OF CLUSTERS AFTER FILTERING == 0!'
                
                # Create a link to the interesting directories in the initial results directory
                hit_dir = os.path.join(pandda.output_handler.get_dir('interesting_datasets'), d_handler.name)
                if not os.path.exists(hit_dir):
                    rel_symlink(orig=d_handler.output_handler.get_dir('root'), link=hit_dir)
                   
                # Flag the dataset as interesting 
                pandda.datasets.all_masks().set_mask_value(mask_name='interesting', entry_id=d_handler.tag, value=True)

                # ============================================================================>
                # PRINT MAP OF THE CLUSTERS  #
                # ============================================================================>
                highz_points = []; [highz_points.extend(zip(*v)[0]) for v in filtered_hit_clusters.values()]
                highz_map_array = numpy.zeros(pandda.reference_grid().grid_size_1d(), dtype=int)
                highz_map_array.put(map(pandda.reference_grid().grid_indexer(), highz_points), [1]*len(highz_points))
                pandda.write_array_to_map(d_handler.output_handler.get_file('high_z_mask'), flex.double(highz_map_array.tolist()))
                
                # ============================================================================>
                # Create cluster object from the clustered points
                # ============================================================================>
                d_handler.hit_clusters = cluster_data(filtered_hit_clusters)

                # ============================================================================>
                # Estimate the occupancy of the detected features 
                # ============================================================================>
                # Extract sites for this cluster and estimate the occupancy of the event
                for event_idx, event_sites in enumerate(d_handler.hit_clusters.get_points()):
                    pandda.log('===================================>>>', True)
                    # Get the label for the cluster
                    event_key = d_handler.hit_clusters.get_keys()[event_idx]
                    # Expand the sites to include the surrounding region (Only take 2nd return arguement of 'max' sites)
                    expanded_sites = get_grid_points_within_index_cutoff_of_grid_sites(event_sites, max_grid_dist=3)[1]
                    pandda.log('=> Event sites ({!s} points) expanded to {!s} points'.format(len(event_sites), len(expanded_sites)))
                    # Use the outer mask to provide a reference point for the correlation to the mean map
                    reference_sites = pandda.reference_grid().global_mask().outer_mask()
                    # Calculate the correlation with the mean map as different amounts of the mean map are subtracted
                    event_occs, event_corrs, global_corrs, corr_discrep = calculate_occupancy_correlations(
                        ref_map          = map_analyser.statistical_maps.mean_map,
                        query_map        = m_handler.map,
                        feature_region   = expanded_sites,
                        reference_region = reference_sites,
                        min_occ          = pandda.params.occupancy_estimation.min_occ,
                        max_occ          = pandda.params.occupancy_estimation.max_occ,
                        occ_increment    = pandda.params.occupancy_estimation.occ_increment,
                        verbose          = False)
                    
                    # Extract the maximum discrepancy as an estimation of the event occupancy
                    max_corr_discrep = max(corr_discrep)
                    event_occ_est = event_occs[corr_discrep.index(max_corr_discrep)]
                    pandda.log('=> Event occupancy estimated as {!s}'.format(event_occ_est))

                    # ============================================================================>
                    # Calculate occupancy maps at this occupancy
                    # ============================================================================>
                    event_occ_map = calculate_occupancy_subtracted_map(
                        ref_map      = map_analyser.statistical_maps.mean_map, 
                        query_map    = m_handler.map,
                        subtract_occ = 1.0-event_occ_est)
                    # Write out this array
                    pandda.write_array_to_map(d_handler.output_handler.get_file('occupancy_map').format(event_key, event_occ_est), event_occ_map)

                    # ============================================================================>
                    # Write out the graph of the occupancy correlations
                    # ============================================================================>
                    if pandda.args.output.plot_graphs:
                        import matplotlib
                        matplotlib.interactive(0)
                        from matplotlib import pyplot
                        # 2 subplots sharing x-axis
                        fig, (axis_1_1, axis_2_1) = pyplot.subplots(2, sharex=True)
                        # 1st Plot
                        line_1_1, = axis_1_1.plot(event_occs, global_corrs, 'g--', label='GLOBAL')
                        line_1_2, = axis_1_1.plot(event_occs, event_corrs, 'k--', label='EVENT')
                        axis_1_1.set_xlabel('EVENT OCCUPANCY')
                        axis_1_1.set_ylabel('MEAN-MAP CORRELATIONS', color='k')
                        axis_1_1.set_ylim((-1, 1))
                        # 2nd Plot
                        line_2_1, = axis_2_1.plot(event_occs, corr_discrep, 'b-', label='DISCREP')
                        axis_2_1.set_ylabel('CORRELATION DIFFERENCE', color='b')
                        # Plot line at the maximum
                        line_2_2, = axis_2_1.plot([event_occ_est,event_occ_est],[-1,1], 'k-')
                        text_2_1 = axis_2_1.text(0.02+event_occ_est, 0.0, str(event_occ_est))
                        # Joint legend
                        axis_1_1.legend(handles=[line_1_1, line_1_2, line_2_1], loc=4)
                        # Title
                        axis_1_1.set_title('MAP QUALITY V LIGAND OCCUPANCY')
                        # Remove spacing between subplots
                        fig.subplots_adjust(hspace=0)
                        pyplot.setp([a.get_xticklabels() for a in fig.axes[:-1]], visible=False)
                        pyplot.setp([axis_2_1.get_xticklabels()], visible=True)
                        # Apply tight layout to prevent overlaps
                        #pyplot.tight_layout()
                        # Save
                        pyplot.savefig(d_handler.output_handler.get_file('occ_corr_png').format(event_key))
                        pyplot.close(fig)

# XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX
# TODO REIMPLEMENT THIS IF IT CAN BE DONE SO THAT IT LOOKS NICE
#
#                # Image Blobs in the datasets using ccp4mg
#                if d_handler.hit_clusters:
#
#                    pandda.log('===================================>>>', True)
#                    pandda.log('Imaging blobs in Dataset {!s}'.format(d_handler.tag), True)
#
#                    sorted_blob_indices = d_handler.hit_clusters.sort(sorting_function=max)
#                    blob_num = len(sorted_blob_indices)
#
#                    for blob_rank, blob_grid_peak in enumerate(d_handler.hit_clusters.get_centroids(indices=sorted_blob_indices)):
#
#                        # Only produce a certain number of images
#                        if blob_rank == pandda.params.blob_search.blobs_to_image:
#                            break
#
#                        status_bar(n=blob_rank, n_max=blob_num)
#
#                        blob_cart_peak = [g*pandda.reference_grid().grid_spacing() for g in blob_grid_peak]
#
#                        # Make images of the blob
#                        pandda.image_blob(  script    = d_handler.output_handler.get_file('ccp4mg_script'),
#                                            image     = d_handler.output_handler.get_file('ccp4mg_png'),
#                                            d_handler = d_handler,
#                                            point_no  = blob_rank+1,
#                                            point     = blob_cart_peak,
#                                            towards   = centre_of_mass
#                                        )
#
#                    status_bar(n=blob_num, n_max=blob_num)
# XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX

            # XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX
            # TODO REIMPLEMENT THIS IF WE CAN MAKE IT RUN EVEN VAGUELY QUICKLY
            #
            #    ##################################
            #    # POST-PROCESS Z-MAPS            #
            #    ##################################
            #
            #    mod_z_map, resamp_mod_z_map = pandda.process_z_map(z_map=z_map)
            #
            #    # Write map
            #    pandda.write_array_to_map(  output_file  = d_handler.get_mtz_filename().replace('.mtz','.processed.zvalues.ccp4'),
            #                                map_data     = flex.double(mod_z_map))
            #    # Write down-sampled map
            #    pandda.write_array_to_map(  output_file  = d_handler.get_mtz_filename().replace('.mtz','.resamp.processed.zvalues.ccp4'),
            #                                map_data     = flex.double(resamp_mod_z_map),
            #                                grid_size    = pandda.reference_grid().resampled_grid_size(),
            #                                grid_spacing = pandda.reference_grid().resampled_grid_spacing())
            # XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX

            # ============================================================================>
            
            # ============================================================================>
            #####
            # PICKLE THE DATASETS THAT HAVE JUST BEEN PROCESSED
            #####
            # ============================================================================>

            pandda.pickle_the_pandda(components=['datasets'], datasets=pandda.datasets.mask(mask_name=analysis_mask_name))
            pandda.update_pandda_size(tag='After Finding Clusters @ {!s}A'.format(cut_resolution))

            # ============================================================================>
            #####
            # Write the summaries
            #####
            # ============================================================================>

            pandda.write_map_analyser_summary(map_analyser=map_analyser, analysis_mask_name=analysis_mask_name)
        
            t_end = time.time()
            pandda.log('===================================>>>', True)
            pandda.log('{!s}A Map Processing Time: {!s}'.format(cut_resolution, time.strftime("%H hours:%M minutes:%S seconds", time.gmtime(t_end - t_start))), True)
            pandda.log('@{!s}A:\t {!s}/{!s} Datasets Analysed'.format(cut_resolution, pandda.datasets.size(mask_name=analysis_mask_name), pandda.datasets.size(mask_name='rejected - total', invert=True)))
            pandda.log('Total:\t {!s}/{!s} Datasets Analysed'.format(pandda.datasets.size(mask_name='analysed'), pandda.datasets.size(mask_name='rejected - total', invert=True)))
            
            # ============================================================================>
            #####
            # DELETE THE MAP ANALYSER TO FREE UP MEMORY
            #####
            # ============================================================================>
            del map_analyser
            gc.collect() 

        # ============================================================================>
        #####
        # END PROCESSING LOOP - Pickle everything again
        #####
        # ============================================================================>

        t_analysis_end = time.time()
        pandda.log('===================================>>>', True)
        pandda.log('Total Analysis Time: {!s}'.format(time.strftime("%H hours:%M minutes:%S seconds", time.gmtime(t_analysis_end - t_analysis_start))), True)

        pandda.pickle_the_pandda(all=True)
        pandda.update_pandda_size(tag='After Processing')

        # ============================================================================>
        #####
        # ANALYSIS
        #####
        # Analyse the processed data
        # TODO Create a `hit processor` class
        # ============================================================================>

        cluster_total, cluster_num, all_dataset_clusters = pandda.collate_all_clusters()

        # Print some slightly less important information
        pandda.log('===================================>>>', False)
        for d_tag, cluster_count in cluster_num:
            pandda.log('Dataset {!s}: {!s} Clusters'.format(d_tag, cluster_count), False)
        # Print a summary of the number of identified clusters
        pandda.log('===================================>>>', True)
        pandda.log('Total Datasets with Clusters: {!s}'.format(len(cluster_num)), True)
        pandda.log('Total Clusters: {!s}'.format(cluster_total), True)

        combined_cluster = pandda.process_z_value_clusters()

        # Write a combined file of all of the result hits
        pandda.write_ranked_cluster_csv(
                                cluster        = combined_cluster,
                                sorted_indices = combined_cluster.sort(sorting_data='values', sorting_function=max, decreasing=True),
                                outfile        = pandda.output_handler.get_file('blob_summaries')
                                )

        # Cluster results by site
        # TODO Add site_definition object to pickles so that we can retain site information between runs
        
        # ============================================================================>
        #####
        # SUMMARIES ------------------------------>>>
        #####
        # ============================================================================>

        pandda.write_analysis_summary()
        pandda.write_summary_csvs()
        pandda.write_summary_graphs()

#        if pandda.args.settings.developer.validate_output:
#            test_pandda_results(pandda)

        try:
            from ascii_graph import Pyasciigraph
            g=Pyasciigraph()
            graph_data = [(res, len(resolution_count[res])) for res in sorted(resolution_count)]
            for l in g.graph(label='Datasets Processed at Each Resolution', data=graph_data, sort=0):
                print(l)
        except ImportError:
            print('IMPORT ERROR (ascii_graph) - CANNOT GENERATE MAP ANALYSIS GRAPH')
            pass

        pandda.log('Datasets Processed: {!s}'.format(sum(map(len,resolution_count.values()))))
        pandda.log('Datasets Loaded {!s}'.format(pandda.datasets.size(mask_name='rejected - total', invert=True)))

        pandda.exit()

    except KeyboardInterrupt:
        raise
    except AssertionError:
        raise
#    except Exception as err:
#        return (pandda, err)

    return pandda, None

def run_custom_analyses(pandda):
    """If a list of hits is available, test to see whether the pandda identified them"""

    try:

        # ============================================================================>
        # ======================================>
        # Manual Analyses - These only need processing once
        # ======================================>
        # ============================================================================>
        analyses_dir = pandda.output_handler.get_dir('analyses')
        # ============================================================================>

        if 0:
            pandda.log('===================================>>>')
            pandda.log('Calculating Deviations of C-alphas between structures')

            rms = lambda vals: numpy.sqrt(numpy.mean(numpy.abs(vals)**2))
            norm = lambda vals: numpy.sqrt(numpy.sum(numpy.abs(vals)**2))

            # Pull all c-alpha sites for each structure
            all_sites = numpy.array([d.transform_points_to_reference(d.get_calpha_sites()) for d in pandda.datasets.all()])
            # Calculate the mean x,y,z for each c-alpha
            mean_sites = numpy.mean(all_sites, axis=0)
            # Differences from the mean
            diff_sites = all_sites - mean_sites
            # Euclidean norms of the distances moved
            diff_norms = numpy.apply_along_axis(norm, axis=2, arr=diff_sites)

            with open(os.path.join(analyses_dir,'calpha_variation.csv'), 'w') as fh:
                for row in diff_norms:
                    out_list = row.round(3).tolist()
                    out_line = ', '.join(map(str,out_list)) + '\n'
                    fh.write(out_line)

            pandda.log('Largest deviation from the mean site: {!s}'.format(diff_norms.max()))
            pandda.log('Average deviation from the mean site: {!s}'.format(diff_norms.mean()))

        # ============================================================================>

        if 0:
            pandda.log('===================================>>>')
            pandda.log('Clustering the Refined Structures')

            distance_matrix = []
            for d1 in pandda.datasets.all():
               distance_matrix.append([d1.transform_points_to_reference(d1.get_calpha_sites()).rms_difference(d2.transform_points_to_reference(d2.get_calpha_sites())) for d2 in pandda.datasets.all()])

            distance_matrix = numpy.array(distance_matrix)

            with open(os.path.join(analyses_dir,'calpha_distance_matrix.csv'), 'w') as fh:
                for row in distance_matrix:
                    out_list = row.round(3).tolist()
                    out_line = ', '.join(map(str,out_list)) + '\n'
                    fh.write(out_line)

        # ============================================================================>

    except:
        pandda.log('FAILURE DURING CUSTOM ANALYSES', True)
        raise

    return 0

# ============================================================================>
#
#   COMMAND LINE RUN
#
# ============================================================================>

if __name__ == '__main__':

    welcome(os.getlogin())

    pandda, err = pandda_main(args=None)

    if err:
        raise err
