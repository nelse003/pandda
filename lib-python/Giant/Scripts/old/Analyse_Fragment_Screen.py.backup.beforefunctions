import os, sys, glob

import iotbx.pdb as pdb_reader
import iotbx.map_tools as map_tools

import numpy
from scitbx.math import superpose
from scitbx.array_family import flex

from cctbx import maptbx
from iotbx.reflection_file_utils import extract_miller_array_from_file
from libtbx.math_utils import ifloor, iceil

from Giant.Xray.Miller.Utils import apply_symmetry_to_miller_array, check_miller_arrays_compatible_for_scaling
from Giant.Xray.Miller.Utils import scale_miller_array_to_reference, scale_amplitude_arrays_via_intensities
from Giant.Xray.Maps.Utils import get_fft_map_from_f_obs_and_structure, generate_p1_box_from_size, write_1d_array_as_p1_map
from Giant.Xray.Maps.Grid import calculate_sampling_distance, get_bounding_box_for_structure, calculate_grid_size, create_cartesian_grid
from Giant.Xray.Structure.Select import get_calpha_sites
from Giant.Stats.Moments import skew, kurtosis
from Giant.Stats.Normalise import normalise_array_to_z_scores

def status_bar(n, n_max):
    if (n+1==n_max):
        print '\r>>', 100, '%'
    elif (n%1000==0):
        print '\r>>', int(round(100.0*(n+1)/n_max,0)), '%',
    sys.stdout.flush()

# ============================================================================>

#####
# Output Files
#####

# ============================================================================>

output_dir = '/home/npearce/Analyses/FragmentSoaks/BAZ2BA-Feb13-Soak'

output_points = os.path.join(output_dir, 'sample_points.csv')
output_map_values = os.path.join(output_dir, 'map_values.csv')
output_map_point_summary = os.path.join(output_dir, 'map_point_summary.csv')
output_z_values = os.path.join(output_dir, 'z_values.csv')
output_z_values_summary = os.path.join(output_dir, 'z_values_summary.csv')
output_modified_z_values_summary = os.path.join(output_dir, 'mod_z_values_summary.csv')
if os.path.exists(output_points):
    os.remove(output_points)
if os.path.exists(output_map_values):
    os.remove(output_map_values)
if os.path.exists(output_map_point_summary):
    os.remove(output_map_point_summary)
if os.path.exists(output_z_values):
    os.remove(output_z_values)
if os.path.exists(output_z_values_summary):
    os.remove(output_z_values_summary)

# ============================================================================>

#####
# Global Variables
#####

# ============================================================================>

map_type = ['Fobs','2mFo-DFc'][1]
res_factor = 0.33

# ============================================================================>

#####
# Read Reference Data
#####

# ============================================================================>

input_dir = '/home/npearce/Analyses/FragmentSoaks/BAZ2BA-Feb13-Soak'

ref_pdb = os.path.join(input_dir,'reference.pdb')
ref_mtz = os.path.join(input_dir,'reference.mtz')
assert os.path.exists(ref_pdb)
assert os.path.exists(ref_mtz)

ref_input = pdb_reader.input(source_info=None,lines=open(ref_pdb,'r').read())
ref_struc = ref_input.xray_structure_simple()

ref_calpha_sites = get_calpha_sites(input_obj=ref_input, structure_obj=ref_struc)
print 'Num C-Alpha Sites: ', ref_calpha_sites.size()

# Get the observed data for the reference structure
ref_miller = extract_miller_array_from_file(ref_mtz, 'F,SIGF', log=open(os.devnull,'a'))
ref_miller.show_summary()

# Get the map data for the reference structure
ref_map = get_fft_map_from_f_obs_and_structure(f_obs=ref_miller, xray_structure=ref_struc, map_type=map_type)
# TODO revisit/remove this! - Apply uniform scaling across the datasets?
ref_map.apply_sigma_scaling()
ref_unit_cell = ref_map.unit_cell()
ref_space_group = ref_map.space_group()

# Create map handler for the reference structure
ref_basic_map = maptbx.basic_map(
        maptbx.basic_map_unit_cell_flag(),
        ref_map.real_map(),
        ref_map.real_map().focus(),
        ref_unit_cell.orthogonalization_matrix(),
        maptbx.out_of_bounds_clamp(0).as_handle(),
        ref_unit_cell)

print 'Size of Map: ', ref_map.n_real(), ref_map.n_grid_points()

print '===================================>>>'

# ============================================================================>

#####
# Create and map sites to the unit cell
#####

# ============================================================================>

# Set the target resolution for the other datasets # XXX Change this to be dynamic or otherwise defined in the next iterations of the script XXX
ref_res = ref_miller.d_min()
print 'Reference Resolution', ref_res

# TODO Set buffer size on the bounding box
ref_struc_extent = get_bounding_box_for_structure(ref_struc)
print 'Min/Max Sites', ref_struc_extent

# Calculate the sampling distance between points - TODO set this to the value used by Tickle
grid_spacing = calculate_sampling_distance(ref_res, res_factor)
print 'Grid Spacing', grid_spacing

# TODO Remove this when the map writing is able to deal with not having the origin
include_origin_in_grid = True

# Create a grid (cartesian) of points for sampling
if include_origin_in_grid:
    box_cart_size, grid_size, sample_points_cart_ref = create_cartesian_grid(min_carts=(0,0,0), max_carts=ref_struc_extent[1], grid_spacing=grid_spacing)
else:
    box_cart_size, grid_size, sample_points_cart_ref = create_cartesian_grid(min_carts=ref_struc_extent[0], max_carts=ref_struc_extent[1], grid_spacing=grid_spacing)
print 'Size of Sampling Box', box_cart_size
print 'Size of Sample Grid', grid_size

# Calculate the distance between the grid points
grid_point_volume = grid_spacing**3
print 'Grid Point Volume', grid_point_volume

# Pull the reference structure map values at the points
sample_map_vals_ref = ref_basic_map.get_cart_values(sample_points_cart_ref)

# Number of grid points
num_grid_points = grid_size[0]*grid_size[1]*grid_size[2]
assert num_grid_points == len(sample_map_vals_ref)
print 'Number of Grid Points:', num_grid_points

print 'First Point: ', sample_points_cart_ref[0]
print 'Last Point: ', sample_points_cart_ref[-1]

# Write the output points
with open(output_points, 'w') as points_file:
    print 'Writing Grid Sample Points File...'
    # Write (x,y,z) tuples on each line
    for i in xrange(num_grid_points):
        status_bar(n=i, n_max=num_grid_points)
        output_line = [str(round(x,3)) for x in sample_points_cart_ref[i]]
        points_file.write(', '.join(output_line)+'\n')

# Write output map
print 'Writing Sample Reference Map!'
ref_sampled_map_file = ref_mtz.replace('.mtz','.sampled.ccp4')
write_1d_array_as_p1_map(file_name=ref_sampled_map_file, map_data=sample_map_vals_ref, grid_size=grid_size, grid_spacing=grid_spacing)

# ============================================================================>

#####
# Create list of data
#####

# ============================================================================>

raw_pdbs = sorted(glob.glob(os.path.join(input_dir,'ProcessedFragmentSoak/BAZ2BA-*/1-apo/apo-BAZ2BA-*-refmac.pdb')))
raw_file_pairs = []

for pdb in raw_pdbs:
    mtz = pdb.replace('.pdb','.mtz')
    if os.path.exists(mtz):
        raw_file_pairs.append((pdb,mtz))

#raw_file_pairs = raw_file_pairs[0:3]

if not raw_file_pairs: raise SystemExit('No Files Found!')

output_transformations = []
output_inv_transformations = []

output_maps = []
output_z_maps = []

num_raw_datasets = len(raw_file_pairs)
file_pairs = []

print 'Number of Datasets: ', num_raw_datasets

# ============================================================================>

#####
# Scale and Process All Data
#####

# ============================================================================>

for d_num, (pdb, mtz) in enumerate(raw_file_pairs):

    # Separator
    print '===================================>>>'

    print 'Dataset', d_num+1

    print pdb
    print mtz

    # Read New Data
    new_miller = extract_miller_array_from_file(mtz, 'F,SIGF', log=open(os.devnull,'a'))

    # Scale New Data
    new_miller_sym = apply_symmetry_to_miller_array(chg_miller=new_miller, ref_miller=ref_miller)
    new_miller_sym_scale = scale_amplitude_arrays_via_intensities(chg_arr_a=new_miller_sym, ref_arr_a=ref_miller, n_bins=20)
    new_miller_scale = apply_symmetry_to_miller_array(chg_miller=new_miller_sym_scale, ref_miller=new_miller)
#    new_miller_scale.show_summary()

    new_ref_correlation = new_miller_sym_scale.correlation(ref_miller).coefficient()
    print 'Scaled-Ref Correlation: ', new_ref_correlation
    new_new_correlation = new_miller_scale.correlation(new_miller).coefficient()
    print 'Scaled-Orig Correlation:', new_new_correlation

    if new_new_correlation < 0.9:
        print 'Removing Dataset!'
        continue

    # Read PDB File
    new_input = pdb_reader.input(source_info=None,lines=open(pdb,'r').read())
    new_struc = new_input.xray_structure_simple()

    # Convert To Map
    new_map = get_fft_map_from_f_obs_and_structure(f_obs=new_miller_scale, xray_structure=new_struc, map_type=map_type, d_min=ref_res)
    new_map.apply_sigma_scaling()
    new_unit_cell = new_map.unit_cell()
    new_space_group = new_map.space_group()

    print 'Map Resolution: ', new_map.d_min()
    print 'Size of Map: ', new_map.n_real(), new_map.n_grid_points()

    # Align Maps
    new_calpha_sites = get_calpha_sites(input_obj=new_input, structure_obj=new_struc)
    print 'Num C-Alpha Sites: ', new_calpha_sites.size()

    try:
        # Sample at the sites in cart_sampling_sites
        alignment_fit = superpose.least_squares_fit(reference_sites=ref_calpha_sites, other_sites=new_calpha_sites)
    except RuntimeError:
        continue

#    print 'R:\t', '\n\t'.join([' '.join(map(str,l)) for l in alignment_fit.r.as_list_of_lists()])
#    print 'T:\t', '\n\t'.join([' '.join(map(str,l)) for l in alignment_fit.t.as_list_of_lists()])

    # New -> Ref mappings
    new_to_ref_rt = alignment_fit.rt()
    # Ref -> New mappings
    ref_to_new_rt = alignment_fit.rt().inverse()

    # Transform the sites to sample (ref -> new transform)
    sample_points_cart_new = ref_to_new_rt*sample_points_cart_ref

    # Check that the right matrices are being used
    print 'Initial C-alpha RMSD', new_calpha_sites.rms_difference(ref_calpha_sites)
    print 'Using ref_to_new to transform ref', new_calpha_sites.rms_difference(ref_to_new_rt*ref_calpha_sites)

    # Create map handler for the new structure
    new_basic_map = maptbx.basic_map(
        maptbx.basic_map_unit_cell_flag(),
        new_map.real_map(),
        new_map.real_map().focus(),
        new_unit_cell.orthogonalization_matrix(),
        maptbx.out_of_bounds_clamp(0).as_handle(),
        new_unit_cell)

    sample_map_vals_new = new_basic_map.get_cart_values(sample_points_cart_new)

    # Write output map
    print 'Writing Sampled Map!'
    new_sampled_map_file = mtz.replace('.mtz','.sampled.ccp4')
    write_1d_array_as_p1_map(file_name=new_sampled_map_file, map_data=sample_map_vals_new, grid_size=grid_size, grid_spacing=grid_spacing)

    # Save things
    output_maps.append(sample_map_vals_new)
    output_transformations.append(alignment_fit.rt())
    output_inv_transformations.append(alignment_fit.rt().inverse())
    file_pairs.append((pdb, mtz))

#    # Superpose data
#    new_map_superposed = maptbx.superpose_maps(
#        unit_cell_1        = new_map.unit_cell(),
#        unit_cell_2        = ref_map.unit_cell(),
#        map_data_1         = new_map.real_map_unpadded(),
#        n_real_2           = ref_map.n_real(),
#        rotation_matrix    = alignment_fit.r.elems,
#        translation_vector = alignment_fit.t.elems)
#
#    print 'Size of Map: ', new_map_superposed.last(), new_map_superposed.size()

print '===================================>>>'

# ============================================================================>

#####
# ANALYSIS
#####

# ============================================================================>

num_datasets = len(file_pairs)

# Create array of the output data
print 'Combining output maps into array...',; sys.stdout.flush()
all_data = numpy.array(output_maps)
print 'done'

# Correlations between all of the datasets
#all_data_coeffs = numpy.corrcoef(all_data)
#print 'Correlations less than 0.9:', sum(all_data_coeffs < 0.9)
#print 'Average Cross-Correlation:', ' '.join([str(round(x, 3)) for x in (all_data_coeffs.sum(0)-1)/(all_data_coeffs.shape[1]-1)])

# ============================================================================>

# Write out the z-values for each map individually relative to the other maps
with open(output_map_values, 'a') as map_values_file:

    print 'Writing ALL Map Values To File... ({!s} Datasets, {!s} Grid Points)'.format(num_datasets, num_grid_points)

    # Column Headings and Filenames
    output_line = ['Dataset {!s}'.format(i+1) for i in xrange(num_datasets)]
    map_values_file.write(', '.join(output_line)+'\n')

    # Map values at each point for each of the datasets
    for i in xrange(num_grid_points):
        status_bar(n=i, n_max=num_grid_points)
        output_line = map(str,all_data[:,i].round(3))
        map_values_file.write(', '.join(output_line)+'\n')

# ============================================================================>

# Calculate the mean and standard deviation for each grid point
# 1st moment - Mean
sample_map_vals_mean = all_data.mean(axis=0)
assert len(sample_map_vals_mean) == num_grid_points
print 'Means:', numpy.array([(round(x, 3)) for x in sample_map_vals_mean][0:5]), '(First 5 of {!s})'.format(len(sample_map_vals_mean))
# 2nd moment - Std
sample_map_vals_stds = all_data.std(axis=0)
assert len(sample_map_vals_stds) == num_grid_points
print 'Stds: ', numpy.array([(round(x, 3)) for x in sample_map_vals_stds][0:5])
# 3rd moment - Skew
sample_map_vals_skew = numpy.array([skew(all_data[:,i]) for i in xrange(num_grid_points)])
assert len(sample_map_vals_skew) == num_grid_points
print 'Skew: ', numpy.array([(round(x, 3)) for x in sample_map_vals_skew][0:5])
# 4th moment - Kurtosis
sample_map_vals_kurt = numpy.array([kurtosis(all_data[:,i]) for i in xrange(num_grid_points)])
assert len(sample_map_vals_kurt) == num_grid_points
print 'Kurt: ', numpy.array([(round(x, 3)) for x in sample_map_vals_kurt][0:5])

# ============================================================================>

# Writing out maps for the means and the other moments
print 'Writing Moment Maps!'
mean_map_file = ref_mtz.replace('.mtz','.mean.ccp4')
write_1d_array_as_p1_map(file_name=mean_map_file, map_data=flex.double(sample_map_vals_mean), grid_size=grid_size, grid_spacing=grid_spacing)
print 'Mean Map Written.'
stds_map_file = ref_mtz.replace('.mtz','.stds.ccp4')
write_1d_array_as_p1_map(file_name=stds_map_file, map_data=flex.double(sample_map_vals_stds), grid_size=grid_size, grid_spacing=grid_spacing)
print 'Standard Deviation Map Written.'
skew_map_file = ref_mtz.replace('.mtz','.skew.ccp4')
write_1d_array_as_p1_map(file_name=skew_map_file, map_data=flex.double(sample_map_vals_skew), grid_size=grid_size, grid_spacing=grid_spacing)
print 'Skew Map Written.'
kurt_map_file = ref_mtz.replace('.mtz','.kurt.ccp4')
write_1d_array_as_p1_map(file_name=kurt_map_file, map_data=flex.double(sample_map_vals_kurt), grid_size=grid_size, grid_spacing=grid_spacing)
print 'Kurtosis Map Written.'

# ============================================================================>

# Writing out the means and the other moments for the map
with open(output_map_point_summary, 'a') as map_point_summary_file:
    # Write (mean,std,skew,kurtosis) tuples on each line
    print 'Writing Map Point Summary To File...'
    for i in xrange(num_grid_points):
        status_bar(n=i, n_max=num_grid_points)
        output_summary_line = [round(sample_map_vals_mean[i],3), round(sample_map_vals_stds[i],3),
                                round(sample_map_vals_skew[i],3), round(sample_map_vals_kurt[i],3)]
        map_point_summary_file.write(', '.join(map(str,output_summary_line))+'\n')

# ============================================================================>

# Convert the map values in all_data to Z-scores based on the means and stds for the grid points
for i_data, data in enumerate(all_data):

    print '===================================>>>'

    z_array = normalise_array_to_z_scores(input_array=data, element_means=sample_map_vals_mean, element_stds=sample_map_vals_stds)

    assert i+1 == num_grid_points

    pdb, mtz = file_pairs[i_data]

    z_mean = round(numpy.mean(z_array),3)
    z_std  = round(numpy.std(z_array),3)
    z_skew = round(skew(z_array),3)
    z_kurt = round(kurtosis(z_array),3)
    z_max  = round(z_array.max(),3)
    z_min  = round(z_array.min(),3)

    print 'Dataset: ', i_data+1
    print 'PDB:', pdb
    print 'MTZ:', mtz
    print 'Mean:   ', z_mean
    print 'StdDev: ', z_std
    print 'Skew:   ', z_skew
    print 'Kurt:   ', z_kurt
    print 'MAX:    ', z_max
    print 'MIN:    ', z_min

    # Append data to total array
    output_z_maps.append(z_array)

    # Write output map
    print 'Writing Z-value Maps!'
    new_z_map_file = mtz.replace('.mtz','.zvalues.ccp4')
    write_1d_array_as_p1_map(file_name=new_z_map_file, map_data=flex.double(z_array), grid_size=grid_size, grid_spacing=grid_spacing)

    # Write out data summary
    with open(output_z_values_summary, 'a') as z_values_summary_file:
        print 'Writing Z-value Summary To File...',; sys.stdout.flush()
        output_summary_line = [z_max, z_min, z_mean, z_std, z_skew, z_kurt]
        z_values_summary_file.write(', '.join(map(str,output_summary_line))+'\n')
        print 'done'

#    # Write out the data
#    with open(output_z_values, 'a') as z_values_file:
#        print 'Writing Z-values To File...',
#        output_line = [round(z,3) for z in z_array] + [pdb, mtz]
#        z_values_file.write(', '.join(map(str,output_line))+'\n')
#        print 'done'

# ============================================================================>

print '===================================>>>'

# Create array for all z-values
output_z_maps = numpy.array(output_z_maps)

# TODO Write function to write array to file (with argument headers=[...]) TODO

# Write out the z-values for each map individually relative to the other maps
with open(output_z_values, 'a') as z_values_file:

    print 'Writing ALL Z-values To File... ({!s} Datasets, {!s} Grid Points)'.format(num_datasets, num_grid_points)

    # Column Headings and Filenames
    output_line = ['Dataset {!s}'.format(i+1) for i in xrange(num_datasets)]
    z_values_file.write(', '.join(output_line)+'\n')

    # Z-values at each point for each of the datasets
    for i in xrange(num_grid_points):
        status_bar(n=i, n_max=num_grid_points)
        output_line = map(str,output_z_maps[:,i].round(3))
        z_values_file.write(', '.join(output_line)+'\n')

# ============================================================================>

# Find groups of Z-values around grid points - First Pass > Average them

from Giant.Xray.Maps.Grid import get_grid_points_within_distance_cutoff_of_origin, combine_grid_point_and_grid_vectors
from Giant.Stats.Tests import test_significance_of_group_of_z_values, convert_pvalue_to_zscore
from Giant.Stats.Utils import resample_ordered_list_of_values

# Create a mask of points to select grid points within a certain distance of the origin (Angstroms)
mask_grid_vectors = get_grid_points_within_distance_cutoff_of_origin(grid_spacing=grid_spacing, distance_cutoff=2)
# Given the size of the mask, we can't process points at the edge of the grid (without additional filtering - TODO?)
buffer_zone = max(max(mask_grid_vectors))
# Given the size of the mask, how often should we calculate the mask? - at the moment jump by the radius of the sphere
grid_jump = iceil(1*max(max(mask_grid_vectors)))
# Create a grid object to easily transform between 3d grid points and 1d map data
sample_grid = flex.grid(grid_size)

print 'Data Resolution: {!s}'.format(ref_res)
print 'Grid Spacing: {!s}'.format(grid_spacing)
print 'Grid Size:', grid_size
print len(mask_grid_vectors), 'points will be sampled around each grid point'
print 'Maximum grid values to be sampled around points:', max(mask_grid_vectors)
print 'Buffer zone around edge of grid:', buffer_zone
print 'Sampling 1 in {!s} grid points'.format(grid_jump)

output_modified_z_maps = []

axis_name = ['X-axis','Y-axis','Z-axis']

for i_data, data in enumerate(output_z_maps):

    print '===================================>>>'

    print 'Dataset', i_data+1
    print 'Post-Processing Z-value Maps!'

    # Post-processed z-value map data
    modified_z_data = []
    map_sample_points = []

    # Datapoint/Gridpoint counter
    grid_i = 0

    # Iterate through all grid points
    for gp_centre in flex.nested_loop(grid_size):

        # Many grid points - print process
        status_bar(n=grid_i, n_max=num_grid_points)

        # Check that the grid points are in the order we expect them to be
        assert sample_grid(gp_centre) == grid_i

        # Check to see if the point should be skipped for calculation speed
        if [1 for i_dim, coord in enumerate(gp_centre) if (coord%grid_jump != 0)]:
#            print 'Rejecting Grid Point (Sampling):', gp_centre, [axis_name[i_dim] for i_dim, coord in enumerate(gp_centre) if (coord%grid_jump != 0)]
            return_statistic = 0
        # Check to see if the grid point is in the buffer zone of points around the edge of the map, where the statistic can't be calculated
        elif [1 for i_dim, coord in enumerate(gp_centre) if (coord<buffer_zone or coord>(grid_size[i_dim]-1-buffer_zone))]:
#            print 'Rejecting Grid Point (Buffer):', gp_centre, [axis_name[i_dim] for i_dim, coord in enumerate(gp_centre) if (coord<buffer_zone or coord>(grid_size[i_dim]-1-buffer_zone))]
            return_statistic = 0
            # Even though it's in the buffer, keep this point for the down-sampled map
            map_sample_points.append(sample_grid(gp_centre))
        # Otherwise calculate the statistic at the point (with the mask)
        else:
#            print 'Accepting Grid Point:', gp_centre
            # Get the group of grid points around the point
            gp_masked = combine_grid_point_and_grid_vectors(start_point=gp_centre, grid_vectors=mask_grid_vectors)
            # Iterate through and extract the map values at this point
            map_vals = [data[sample_grid(gp)] for gp in gp_masked]
            # Record the statistic of the map_vals
            # XXX Look at significance of group of Z-scores XXX
            resamp_map_vals = resample_ordered_list_of_values(map_vals)
            pval = test_significance_of_group_of_z_values(resamp_map_vals)
#            print pval
            return_statistic = convert_pvalue_to_zscore(pval)

            # OLD SMOOTHING - DELETE
            # For the moment just take the mean
#            return_statistic = flex.mean(flex.abs(flex.double(map_vals)))
            # OLD SMOOTHING - DELETE

            # Note index for the down-sampled map
            map_sample_points.append(sample_grid(gp_centre))

        # Record statistic
        modified_z_data.append(return_statistic)

        # Increment grid counter
        grid_i += 1


    # Write statistics as map
    print 'Writing Modified Z-value Maps!'
    orig_mtz_file = file_pairs[i_data][1]
    modified_z_map_file = orig_mtz_file.replace('.mtz','.modifiedzvalues.ccp4')
    write_1d_array_as_p1_map(file_name=modified_z_map_file, map_data=flex.double(modified_z_data), grid_size=grid_size, grid_spacing=grid_spacing)

    # Down-sample the grid to the calculated points
    down_sampled_map = [modified_z_data[p] for p in map_sample_points]
    down_sampled_grid = tuple([int(1+(g-1)/grid_jump) for g in grid_size])

    assert down_sampled_grid[0]*down_sampled_grid[1]*down_sampled_grid[2] == len(down_sampled_map)

    print 'Down-Sampled Grid:', down_sampled_grid

    # Calculate and print statistics
    z_array = numpy.array(down_sampled_map)
    z_mean = round(numpy.mean(z_array),3)
    print 'Mean:   ', z_mean
    z_std  = round(numpy.std(z_array),3)
    print 'StdDev: ', z_std
    z_skew = round(skew(z_array),3)
    print 'Skew:   ', z_skew
    z_kurt = round(kurtosis(z_array),3)
    print 'Kurt:   ', z_kurt
    z_max  = round(z_array.max(),3)
    print 'MAX:    ', z_max
    z_min  = round(z_array.min(),3)
    print 'MIN:    ', z_min

    # Write out data summary
    with open(output_modified_z_values_summary, 'a') as mod_z_values_summary_file:
        print 'Writing Modified Z-value Summary To File...',; sys.stdout.flush()
        output_summary_line = [z_max, z_min, z_mean, z_std, z_skew, z_kurt]
        mod_z_values_summary_file.write(', '.join(map(str,output_summary_line))+'\n')
        print 'done'

    # Write down-sampled map
    print 'Writing Down-Sampled Modified Z-value Maps!'
    orig_mtz_file = file_pairs[i_data][1]
    down_sampled_map_file = orig_mtz_file.replace('.mtz','.downsamp.modifiedzvalues.ccp4')
    write_1d_array_as_p1_map(file_name=down_sampled_map_file, map_data=flex.double(down_sampled_map), grid_size=down_sampled_grid, grid_spacing=grid_jump*grid_spacing)



